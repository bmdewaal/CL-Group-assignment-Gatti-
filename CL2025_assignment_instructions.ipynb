{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcbFlt0uJpNc"
      },
      "source": [
        "In this assignment you will be asked to extend the work by Gatti et al by checking whether form-meaning mappings learned on a different yet related language to that considered in the original study still capture the perceived valence of pseudowords. To do this you will be asked to engage with several different resources and adapt the pipeline following the instructions. Along the way, you will be asked to answer a few questions.\n",
        "\n",
        "You need to submit the complete notebook in .ipynb format, with intermediate outputs visible. The notebook should be named as follows:\n",
        "\n",
        "CL2025_groupN_assignment.ipynb\n",
        "\n",
        "where N is the group number. Submissions in the wrong format or with names not adhering to the guidelines will not be evaluated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rkN7t4rqacE"
      },
      "source": [
        "Indicate group members' names, student numbers, and contributions below:\n",
        "- 1. \n",
        "- 2.\n",
        "- 3.\n",
        "- 4.\n",
        "- 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OQbS5Urfit8W",
        "outputId": "58df468b-6daa-4b77-b4c4-19dea3361b30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'psycho-embeddings' already exists and is not an empty directory.\n",
            "/Users/bramdewaal/Desktop/Uni/VSC/CL/Group Assignment/psycho-embeddings\n",
            "\u001b[33mDEPRECATION: Loading egg at /Users/bramdewaal/anaconda3/lib/python3.11/site-packages/colorcorrect-0.9.1-py3.11-macosx-11.1-arm64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: datasets in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from datasets) (18.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from datasets) (4.67.0)\n",
            "Requirement already satisfied: xxhash in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# the code has been tested using the psycho-embeddings library to extract representations from LLMs. You can also use other libraries,\n",
        "# as long as you make sure that you are producing the correct output.\n",
        "!git clone https://github.com/MilaNLProc/psycho-embeddings.git\n",
        "%cd psycho-embeddings\n",
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Loading egg at /Users/bramdewaal/anaconda3/lib/python3.11/site-packages/colorcorrect-0.9.1-py3.11-macosx-11.1-arm64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: nltk in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
            "Requirement already satisfied: click in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from nltk) (4.67.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[33mDEPRECATION: Loading egg at /Users/bramdewaal/anaconda3/lib/python3.11/site-packages/colorcorrect-0.9.1-py3.11-macosx-11.1-arm64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting fasttext\n",
            "  Using cached fasttext-0.9.3-cp311-cp311-macosx_15_0_universal2.whl\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from fasttext) (68.0.0)\n",
            "Requirement already satisfied: numpy in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.6\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[33mDEPRECATION: Loading egg at /Users/bramdewaal/anaconda3/lib/python3.11/site-packages/colorcorrect-0.9.1-py3.11-macosx-11.1-arm64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement psycho_embeddings (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for psycho_embeddings\u001b[0m\u001b[31m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "\u001b[33mDEPRECATION: Loading egg at /Users/bramdewaal/anaconda3/lib/python3.11/site-packages/colorcorrect-0.9.1-py3.11-macosx-11.1-arm64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting pyreadr\n",
            "  Using cached pyreadr-0.5.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from pyreadr) (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.23.2 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from pandas>=1.2.0->pyreadr) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from pandas>=1.2.0->pyreadr) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from pandas>=1.2.0->pyreadr) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from pandas>=1.2.0->pyreadr) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->pyreadr) (1.16.0)\n",
            "Using cached pyreadr-0.5.3-cp311-cp311-macosx_11_0_arm64.whl (309 kB)\n",
            "Installing collected packages: pyreadr\n",
            "Successfully installed pyreadr-0.5.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[33mDEPRECATION: Loading egg at /Users/bramdewaal/anaconda3/lib/python3.11/site-packages/colorcorrect-0.9.1-py3.11-macosx-11.1-arm64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: openpyxl in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (3.0.10)\n",
            "Requirement already satisfied: et_xmlfile in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from openpyxl) (1.1.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk\n",
        "%pip install fasttext\n",
        "%pip install psycho_embeddings \n",
        "\n",
        "# Needed to import Rdata file \n",
        "%pip install pyreadr\n",
        "\n",
        "# Needed to read Brysbaert valence excel sheet\n",
        "%pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ofb0L_c0AW0W"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bramdewaal/anaconda3/lib/python3.11/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
            "  warnings.warn(\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80d4117c29634a1ca370ef024af26482",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bramdewaal/anaconda3/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "/Users/bramdewaal/anaconda3/lib/python3.11/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/Users/bramdewaal/anaconda3/lib/python3.11/site-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# the solution to the assignment has been obtained using these packages.\n",
        "# you're free to use other packages though: consider this as an indication, not a prescription.\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import fasttext as ft\n",
        "import pickle as pkl\n",
        "import fasttext.util\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from transformers import AutoTokenizer\n",
        "from psycho_embeddings import ContextualizedEmbedder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra3phcRhKuuo"
      },
      "source": [
        "**Task 1** (*10 points available, see breakdown per task below*)\n",
        "\n",
        "You should replicate the main design in the paper *Valence without meaning* by Gatti and colleagues (2024), using estimates collected for Dutch word valence to train linear regression models and apply them to predict the valence of English pseudowords from Gatti and colleagues.\n",
        "\n",
        "In detail, to train your regression models, you should use the dataset by Speed and Brysbaert (2024) containing crowd-sourced valence ratings (use the metadata to identify the relevant columns) collected for approximately 24,000 Dutch words. See the paper *Ratings of valence, arousal, happiness, anger, fear, sadness, disgust, and surprise for 24,000 Dutch words* by Speed and Brysbaert (2024).\n",
        "\n",
        "You should train a letter unigram model and a bigram model. Each model should be trained on Dutch words only.\n",
        "\n",
        "Pay attention to one issue though: pseudowords created for English may be valid words in Dutch: therefore, you should first filter the list of pseudowords against a large store of Dutch words. To do so, use the words in the Dutch prevalence lexicon available in this OSF repository: https://osf.io/9zymw/. Essentially, you need to exclude any pseudoword that happens to be a word for which a prevalence estimate is available, whatever the prevalence is.\n",
        "\n",
        "Each code block indicates how many points are available and how they are attributed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Hrd4EhHlAcmi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gatti et al. pseudoword valence dataset:\n",
            "             Valence  predicted_val  predicted_valL  predicted_valL_BI  \\\n",
            "rownames                                                                 \n",
            "aardvark        6.26       6.392012        4.920180           6.410768   \n",
            "abalone         5.30       4.756492        5.284912           5.115389   \n",
            "abandon         2.84       4.260055        5.001226           5.479860   \n",
            "abandonment     2.63       4.196807        5.022504           5.334364   \n",
            "abbey           5.85       6.123953        5.147159           5.162931   \n",
            "\n",
            "             predicted_valDIM  predicted_valL_DIM  predicted_valBI  \\\n",
            "rownames                                                             \n",
            "aardvark             5.772722            5.774341         6.410768   \n",
            "abalone              4.728264            4.858120         5.115389   \n",
            "abandon              3.978241            3.987623         5.479860   \n",
            "abandonment          3.833330            3.828077         5.334364   \n",
            "abbey                6.064834            6.094675         5.162931   \n",
            "\n",
            "             predicted_valBI_DIM  \n",
            "rownames                          \n",
            "aardvark                6.392012  \n",
            "abalone                 4.756492  \n",
            "abandon                 4.260055  \n",
            "abandonment             4.196807  \n",
            "abbey                   6.123953  \n",
            "\n",
            "Speed & Brysbaert Dutch valence dataset:\n",
            "      word  n.obs  irt.prevalence  z.irt.prevalence  prevalence  z.prevalence\n",
            "0  T-shirt    324        0.986622          2.215053    0.978395      1.689888\n",
            "1    aagje    303        0.907405          1.324941    0.877888      1.075808\n",
            "2     aagt    324        0.169817         -0.954888    0.188272     -0.827920\n",
            "3      aai    335        0.993290          2.472451    0.988060      1.794794\n",
            "4  aaibaar    333        0.996284          2.676802    0.990991      1.830889\n"
          ]
        }
      ],
      "source": [
        "# read in the pseudowords from Gatti and colleagues, \n",
        "# as well as the valence ratings for 24,000 Dutch words from Speed and Brysbaert (2024)\n",
        "# show the first 5 lines of each dataset.\n",
        "# 1 point for identifying the correct files and correctly loading their content\n",
        "\n",
        "import pyreadr\n",
        "import pandas as pd\n",
        "\n",
        "# Using pyreadr to import the Rdata gatti dataset\n",
        "gatti_result = pyreadr.read_r(\"/Users/bramdewaal/Desktop/Uni/VSC/CL/Group Assignment/data/data_pseudovalence.Rdata\")\n",
        "gatti_df = list(gatti_result.values())[0]\n",
        "\n",
        "print(\"Gatti et al. pseudoword valence dataset:\")\n",
        "print(gatti_df.head())\n",
        "\n",
        "\n",
        "# Importing Speed & Brysbaert dataset\n",
        "speed_df = pd.read_csv(\"/Users/bramdewaal/Desktop/Uni/VSC/CL/Group Assignment/data/prevalence_netherlands.csv\", sep=\"\\t\")\n",
        "\n",
        "\n",
        "print(\"\\nSpeed & Brysbaert Dutch valence dataset:\")\n",
        "print(speed_df.head())\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['word', 'n.obs', 'irt.prevalence', 'z.irt.prevalence', 'prevalence',\n",
            "       'z.prevalence'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(speed_df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "As4XV-LQPbyH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pseudowords that were filtered out:\n",
            "['abandon', 'abdomen', 'abject', 'abracadabra', 'abrupt', 'absence', 'absent', 'abstract', 'absurd', 'abundant', 'accent', 'accept', 'accident', 'account', 'accountant', 'ace', 'acid', 'acne', 'acquit', 'acre', 'act', 'activist', 'actor', 'ad', 'adder', 'addict', 'adept', 'adolescent', 'adrenaline', 'adult', 'advocate', 'aerobics', 'affect', 'affidavit', 'affront', 'aftershave', 'agenda', 'agent', 'aids', 'air', 'airbag', 'airstrip', 'alarm', 'albino', 'album', 'alcohol', 'alert', 'alfalfa', 'algebra', 'alias', 'alibi', 'allegro', 'alligator', 'allure', 'alpine', 'alter', 'altimeter', 'alumnus', 'amaretto', 'amateur', 'amber', 'ambrosia', 'ambulance', 'ammonia', 'ammonium', 'amulet', 'amuse', 'amusement', 'anaconda', 'anagram', 'anarchist', 'angel', 'angina', 'angora', 'angst', 'anklet', 'annex', 'anti', 'antichrist', 'anus', 'aorta', 'apache', 'apex', 'apparent', 'appendage', 'appendicitis', 'appendix', 'appetizer', 'aquarium', 'arcade', 'architect', 'area', 'arena', 'argument', 'ark', 'arm', 'armada', 'aroma', 'arrangement', 'array', 'arrest', 'arrogant', 'artwork', 'ascendant', 'asparagus', 'aspect', 'assessment', 'assist', 'asterisk', 'astronaut', 'atlas', 'atrium', 'attest', 'attitude', 'audio', 'audit', 'auditor', 'auditorium', 'aura', 'auto', 'avenue', 'avocado', 'babe', 'baby', 'babysit', 'babysitter', 'bachelor', 'back', 'backbone', 'backgammon', 'background', 'backhand', 'bacon', 'bad', 'badge', 'bagel', 'baggy', 'ballad', 'ballast', 'ballerina', 'ballet', 'ballpoint', 'ballroom', 'bam', 'ban', 'band', 'bandage', 'bang', 'banjo', 'bank', 'bar', 'barbecue', 'bard', 'barium', 'bark', 'barman', 'barometer', 'baron', 'barracuda', 'barrage', 'barrel', 'barricade', 'bartender', 'base', 'baseball', 'baseline', 'basement', 'basis', 'basket', 'basketball', 'bat', 'batch', 'baton', 'batter', 'battle', 'bazaar', 'bazooka', 'beat', 'beatnik', 'beauty', 'bebop', 'bed', 'bedding', 'bedpan', 'beef', 'beer', 'begin', 'beginner', 'beige', 'belligerent', 'belt', 'beryllium', 'bestseller', 'bias', 'bib', 'biceps', 'bidder', 'big', 'bigot', 'biker', 'bikini', 'billboard', 'bimbo', 'binder', 'binding', 'bingo', 'bio', 'birdie', 'biscuit', 'bistro', 'bit', 'bitch', 'bitter', 'blackjack', 'bladder', 'blank', 'blanket', 'blaster', 'blazer', 'blender', 'bleu', 'blind', 'blink', 'blizzard', 'blockbuster', 'blog', 'blond', 'blonde', 'blouse', 'blow', 'blowjob', 'blubber', 'bluegrass', 'blues', 'blunder', 'blunt', 'boa', 'board', 'bode', 'bodega', 'body', 'bodyguard', 'bogey', 'boiler', 'bonbon', 'bondage', 'bondsman', 'bonnet', 'bonsai', 'bonus', 'boogie', 'bookmaker', 'boom', 'boon', 'boost', 'booster', 'boot', 'bootleg', 'boots', 'bopper', 'border', 'boring', 'bottleneck', 'bouquet', 'bourbon', 'bout', 'bowl', 'bowler', 'bowling', 'box', 'boxer', 'boy', 'bracelet', 'brand', 'brandy', 'brat', 'break', 'breed', 'bridge', 'brief', 'briefing', 'brigade', 'brink', 'broccoli', 'brochure', 'broker', 'bromide', 'brood', 'broom', 'brunch', 'brunette', 'buddy', 'budget', 'buffer', 'buffet', 'bug', 'buggy', 'building', 'bulk', 'bulldozer', 'bullet', 'bulletin', 'bullshit', 'bumper', 'bun', 'bungalow', 'bunker', 'bunny', 'bureau', 'burger', 'burrito', 'bus', 'business', 'butler', 'butterfly', 'button', 'buzzer', 'bypass', 'cabaret', 'cabernet', 'cache', 'cactus', 'caddie', 'cadet', 'cake', 'cakewalk', 'calcium', 'calculator', 'calculus', 'camcorder', 'camel', 'cameo', 'camera', 'cameraman', 'camouflage', 'camp', 'camper', 'camping', 'campus', 'cannabis', 'canon', 'canvas', 'canyon', 'cap', 'cappuccino', 'capsule', 'captain', 'caramel', 'caravan', 'carbon', 'cardigan', 'cargo', 'carpool', 'carrier', 'cartoon', 'cartridge', 'cascade', 'case', 'cash', 'casino', 'cassette', 'cast', 'casual', 'catamaran', 'catcher', 'catchy', 'catwalk', 'cavalier', 'celebrity', 'cellist', 'cello', 'celluloid', 'cement', 'censor', 'census', 'cent', 'center', 'centerfold', 'century', 'cervix', 'chalet', 'champagne', 'chance', 'chant', 'chaos', 'chaperon', 'charade', 'charge', 'charisma', 'charlatan', 'charter', 'chassis', 'chat', 'chateau', 'chauffeur', 'chauvinist', 'check', 'checklist', 'checkpoint', 'cheerleader', 'cheeseburger', 'chef', 'chemo', 'cherub', 'chic', 'chick', 'chiffon', 'chili', 'chimp', 'chinchilla', 'chip', 'chipper', 'chiropractor', 'chloride', 'chloroform', 'choke', 'cholera', 'cholesterol', 'chopper', 'chorus', 'christen', 'chronic', 'chutney', 'cider', 'cinema', 'circuit', 'circus', 'citadel', 'citrus', 'city', 'claim', 'clairvoyant', 'clan', 'classic', 'clean', 'cleaner', 'click', 'climax', 'clinch', 'clinic', 'clip', 'clipper', 'clique', 'clitoris', 'close', 'closet', 'clown', 'club', 'cluster', 'coach', 'coaster', 'coat', 'coating', 'coax', 'cobra', 'cockpit', 'cocktail', 'code', 'cognac', 'coherent', 'coke', 'cola', 'collage', 'collector', 'college', 'collie', 'column', 'columnist', 'coma', 'combine', 'combo', 'comeback', 'comedy', 'comfort', 'commandant', 'commando', 'commentator', 'commerce', 'commercial', 'commodity', 'commodore', 'commune', 'communist', 'community', 'compact', 'competent', 'complement', 'complex', 'compliment', 'component', 'composer', 'compost', 'compressor', 'computer', 'concept', 'concern', 'concert', 'concerto', 'concubine', 'conditioner', 'condor', 'conductor', 'conference', 'confetti', 'conflict', 'conform', 'conga', 'consensus', 'consent', 'consistent', 'console', 'constant', 'construct', 'consul', 'consult', 'consultant', 'contact', 'container', 'content', 'contest', 'context', 'continent', 'contingent', 'continue', 'contract', 'contrast', 'convent', 'convertible', 'cooker', 'cookie', 'cool', 'copyright', 'cordon', 'corduroy', 'corner', 'cornflakes', 'corps', 'correct', 'correspondent', 'corridor', 'corrupt', 'corsage', 'cortex', 'cortisone', 'cottage', 'counseling', 'counselor', 'countdown', 'counter', 'country', 'county', 'coup', 'coupe', 'coupon', 'courage', 'cover', 'coverage', 'cowboy', 'cowgirl', 'coyote', 'crack', 'cracker', 'cranberry', 'crank', 'crash', 'crawl', 'crayon', 'crazy', 'credit', 'crescendo', 'cretin', 'crew', 'cricket', 'crime', 'crisis', 'croissant', 'croquet', 'cross', 'crucifix', 'cruise', 'cruiser', 'crypt', 'cue', 'cuisine', 'cult', 'culture', 'cup', 'cupcake', 'curator', 'curling', 'curriculum', 'curry', 'curve', 'custard', 'cutter', 'cyanide', 'cyberspace', 'dam', 'dame', 'damp', 'damper', 'dandy', 'dank', 'darkroom', 'dart', 'dashboard', 'database', 'date', 'deadline', 'deal', 'dealer', 'debacle', 'debutante', 'decade', 'decadent', 'decent', 'decimeter', 'deck', 'decoder', 'decor', 'decorum', 'deed', 'deejay', 'default', 'defect', 'defibrillator', 'deficit', 'delinquent', 'delirium', 'delta', 'demo', 'demon', 'den', 'denim', 'dentist', 'deodorant', 'depot', 'descendant', 'design', 'designer', 'desk', 'desperado', 'dessert', 'detail', 'detective', 'detector', 'detergent', 'detonator', 'detriment', 'deuce', 'deviant', 'device', 'diabetes', 'diagnose', 'diagram', 'dialect', 'diameter', 'dictator', 'die', 'diesel', 'different', 'diffuse', 'dildo', 'dilemma', 'diligence', 'diligent', 'diner', 'dinghy', 'dingo', 'diocese', 'dioxide', 'dip', 'diploma', 'direct', 'directory', 'discipline', 'disco', 'discount', 'discreet', 'disk', 'dispenser', 'display', 'district', 'diva', 'dizzy', 'do', 'doctor', 'doctrine', 'document', 'dodo', 'dog', 'dollar', 'dominant', 'dominion', 'domino', 'donkey', 'donor', 'doom', 'door', 'dopamine', 'dope', 'dossier', 'dot', 'douche', 'dove', 'downer', 'download', 'drab', 'dragon', 'drain', 'drainage', 'drama', 'draw', 'drift', 'drifter', 'drink', 'drinker', 'drive', 'driver', 'drop', 'drug', 'drugs', 'drum', 'drummer', 'duel', 'duet', 'duffel', 'dummy', 'dump', 'dunk', 'duo', 'dupe', 'duplex', 'duster', 'dynamo', 'echelon', 'echo', 'ecstasy', 'editor', 'effect', 'efficiency', 'egghead', 'ego', 'elegant', 'element', 'elevator', 'elf', 'elite', 'elixir', 'elk', 'eloquent', 'email', 'embargo', 'embryo', 'emerald', 'eminent', 'empire', 'employee', 'enamel', 'enchilada', 'end', 'engagement', 'engineering', 'enigma', 'ensemble', 'enter', 'entertainer', 'entertainment', 'entourage', 'entree', 'entrepreneur', 'episode', 'equalizer', 'equator', 'equivalent', 'era', 'ergo', 'erotica', 'escape', 'escort', 'espresso', 'essay', 'essence', 'ether', 'etiquette', 'eucalyptus', 'euro', 'evident', 'ex', 'exact', 'excellent', 'excelsior', 'excrement', 'exit', 'exodus', 'exorcist', 'experiment', 'expert', 'expertise', 'expo', 'export', 'exposure', 'extra', 'extract', 'extravagant', 'eyeliner', 'factor', 'fade', 'fair', 'fairway', 'fake', 'falafel', 'fan', 'fancy', 'fanfare', 'fantasy', 'farce', 'farm', 'farmer', 'fascist', 'fashionable', 'fat', 'faun', 'fauna', 'fax', 'feature', 'feces', 'fee', 'feedback', 'feeder', 'feeling', 'fellow', 'feminist', 'ferry', 'festival', 'fiancee', 'fiasco', 'fiber', 'fiction', 'fielder', 'file', 'filet', 'film', 'filmmaker', 'filter', 'finale', 'finalist', 'fine', 'finesse', 'finish', 'firewall', 'firmament', 'fit', 'fitness', 'fjord', 'flagrant', 'flair', 'flamboyant', 'flamingo', 'flan', 'flank', 'flap', 'flash', 'flashback', 'flashy', 'flat', 'fleece', 'flex', 'flip', 'flipper', 'flirt', 'flop', 'floppy', 'florist', 'flow', 'fluorescent', 'flux', 'fly', 'flyer', 'focus', 'folder', 'folk', 'folklore', 'fond', 'fondue', 'font', 'forceps', 'form', 'format', 'forte', 'forum', 'fox', 'foxtrot', 'foyer', 'fragment', 'frame', 'franc', 'franchise', 'freak', 'freelance', 'freelancer', 'freestyle', 'frequent', 'fret', 'frisbee', 'fruit', 'fuchsia', 'fun', 'fundraising', 'fungus', 'funk', 'funky', 'fusion', 'futon', 'future', 'gadget', 'gag', 'gaga', 'gal', 'gala', 'gallon', 'game', 'gamma', 'gander', 'gang', 'gangster', 'gap', 'gaping', 'garage', 'gas', 'gasoline', 'gate', 'gateway', 'gay', 'gazelle', 'geek', 'geisha', 'gel', 'gelding', 'gender', 'generator', 'genie', 'genius', 'genocide', 'genre', 'gentleman', 'gerbil', 'gift', 'gig', 'gigolo', 'gimmick', 'gin', 'ginseng', 'giraffe', 'gist', 'glad', 'gladiator', 'glamour', 'glee', 'glimmer', 'glitter', 'globe', 'glossy', 'glucose', 'go', 'goal', 'goalie', 'god', 'godfather', 'gold', 'golden', 'golf', 'golfer', 'gondola', 'goodwill', 'gorilla', 'gospel', 'gossip', 'gothic', 'goulash', 'gourmet', 'graffiti', 'graft', 'gram', 'grandeur', 'grapefruit', 'gravel', 'green', 'grief', 'grieve', 'grill', 'grim', 'grind', 'gringo', 'grip', 'grit', 'grizzly', 'groggy', 'groovy', 'groupie', 'guacamole', 'guano', 'guerrilla', 'guillotine', 'gulp', 'gum', 'gust', 'gut', 'gym', 'gymnasium', 'gymnast', 'habitat', 'hacker', 'haiku', 'hairspray', 'halftime', 'hall', 'halo', 'halt', 'halter', 'ham', 'hamburger', 'hamster', 'hand', 'handicap', 'hang', 'hangar', 'hanger', 'happen', 'happy', 'hard', 'hardcore', 'harden', 'hardware', 'hare', 'harem', 'hark', 'harmonica', 'harp', 'hater', 'have', 'haven', 'hazard', 'headline', 'headliner', 'headset', 'healer', 'healing', 'heat', 'heavy', 'heel', 'helium', 'helix', 'helper', 'hem', 'hen', 'hepatitis', 'hernia', 'herpes', 'hertz', 'hetero', 'hickory', 'high', 'hinder', 'hint', 'hip', 'hippie', 'hit', 'hobby', 'hobo', 'hockey', 'hoe', 'hole', 'hologram', 'holster', 'home', 'homo', 'honk', 'hooligan', 'hoop', 'hop', 'hope', 'horde', 'horizon', 'horror', 'hospice', 'host', 'hostel', 'hostess', 'hot', 'hotdog', 'hotel', 'hotline', 'house', 'hovercraft', 'hub', 'hulk', 'humbug', 'hummer', 'humor', 'husky', 'hut', 'hydrant', 'hyena', 'hymen', 'hype', 'hyper', 'hypothalamus', 'ibuprofen', 'icing', 'icon', 'idealist', 'ignorant', 'image', 'immigrant', 'imminent', 'impact', 'impediment', 'imperfect', 'imperialist', 'impertinent', 'import', 'important', 'impotent', 'imprint', 'impromptu', 'incentive', 'incest', 'inch', 'incident', 'incoherent', 'incompetent', 'inconsistent', 'incorrect', 'incubator', 'indecent', 'indicator', 'indifferent', 'indirect', 'indiscreet', 'indoor', 'indulgent', 'infant', 'inferno', 'influenza', 'info', 'informant', 'inherent', 'inning', 'innocent', 'input', 'insect', 'insecticide', 'insider', 'insolent', 'instant', 'instinct', 'instrument', 'insult', 'intact', 'intake', 'intellect', 'intelligent', 'intercom', 'interest', 'interface', 'interim', 'intern', 'internet', 'interval', 'interview', 'interviewer', 'intolerant', 'intro', 'invite', 'ion', 'iris', 'irrelevant', 'issue', 'item', 'jacket', 'jackpot', 'jade', 'jaguar', 'jam', 'jamboree', 'jammer', 'jargon', 'jazz', 'jazzy', 'jeans', 'jeep', 'jet', 'jihad', 'jingle', 'jive', 'job', 'jockey', 'jogger', 'joint', 'joke', 'joker', 'journalist', 'jubilee', 'judo', 'jukebox', 'jumbo', 'jumper', 'jumpsuit', 'jungle', 'junior', 'junk', 'junkie', 'jury', 'kamikaze', 'kappa', 'karaoke', 'karate', 'karma', 'kebab', 'keel', 'keen', 'keep', 'keeper', 'keg', 'kelp', 'kennel', 'ketchup', 'keyboard', 'kick', 'kidnapper', 'kidnapping', 'killer', 'kilo', 'kilometer', 'kilt', 'kimono', 'kin', 'kind', 'kink', 'kinky', 'kiosk', 'kit', 'kitten', 'kiwi', 'knot', 'koala', 'kook', 'kumquat', 'lab', 'label', 'lacrosse', 'lactose', 'ladder', 'lady', 'ladylike', 'lag', 'lager', 'lamp', 'land', 'landing', 'lap', 'lapel', 'laptop', 'large', 'laryngitis', 'larynx', 'laser', 'lasso', 'latent', 'latex', 'latitude', 'latrine', 'lava', 'leader', 'league', 'lease', 'leg', 'lego', 'lens', 'lessen', 'letter', 'level', 'lever', 'liaison', 'libel', 'libido', 'lid', 'lifestyle', 'lift', 'ligament', 'limbo', 'limit', 'limo', 'limousine', 'lingerie', 'link', 'linoleum', 'lint', 'lip', 'lipstick', 'lira', 'list', 'listing', 'liter', 'lithium', 'live', 'loafer', 'lobby', 'lobbyist', 'locker', 'loco', 'loft', 'log', 'logo', 'look', 'loom', 'loon', 'loop', 'loot', 'lord', 'loser', 'lotion', 'lotto', 'lotus', 'lounge', 'lover', 'lunch', 'lunchbox', 'lust', 'lycra', 'lyrics', 'macaroni', 'mach', 'machete', 'machine', 'macho', 'macro', 'madam', 'maestro', 'magazine', 'magenta', 'magnesium', 'magnitude', 'magnolia', 'mahjong', 'mail', 'mailbox', 'mailing', 'mainframe', 'mainstream', 'major', 'maker', 'malaria', 'mall', 'malt', 'mama', 'man', 'management', 'manager', 'mango', 'manicure', 'manifest', 'mannequin', 'mantel', 'mantra', 'manuscript', 'map', 'marathon', 'mare', 'margarine', 'marine', 'marker', 'marketing', 'martini', 'mascara', 'masochist', 'massacre', 'massage', 'masseur', 'masseuse', 'mast', 'master', 'mat', 'matador', 'match', 'mate', 'matinee', 'matrix', 'matter', 'mausoleum', 'mauve', 'maximum', 'mediocre', 'medium', 'medley', 'meet', 'meeting', 'mega', 'melodrama', 'meltdown', 'memento', 'memo', 'memorabilia', 'memorandum', 'meningitis', 'menthol', 'mentor', 'menu', 'merengue', 'meringue', 'mescaline', 'mess', 'metallic', 'meter', 'metropolis', 'mezzanine', 'micro', 'microchip', 'microfilm', 'microwave', 'migraine', 'mild', 'militant', 'military', 'milkshake', 'millennium', 'milligram', 'millimeter', 'mime', 'mini', 'minibus', 'minimum', 'minister', 'minivan', 'mink', 'minor', 'miss', 'mist', 'mistletoe', 'mix', 'mixer', 'mixture', 'mobile', 'mode', 'model', 'modem', 'modern', 'modest', 'module', 'molecule', 'molest', 'mom', 'moment', 'momentum', 'monarch', 'monitor', 'mono', 'monopoly', 'monsieur', 'monster', 'montage', 'monument', 'moor', 'moot', 'mop', 'mores', 'motel', 'motor', 'motto', 'mousse', 'moustache', 'move', 'mozzarella', 'mud', 'muffin', 'mug', 'multimedia', 'multiple', 'multiplex', 'mum', 'museum', 'musical', 'musket', 'must', 'mustang', 'mutant', 'name', 'nanny', 'nanometer', 'nap', 'napalm', 'nasty', 'nautilus', 'navel', 'navigator', 'nectar', 'neon', 'nerd', 'nest', 'net', 'neutron', 'niche', 'nicotine', 'nimrod', 'ninja', 'nipper', 'nitwit', 'node', 'nonchalant', 'norm', 'notebook', 'novice', 'nuance', 'nucleus', 'nudist', 'nurse', 'nut', 'nylon', 'oasis', 'object', 'observant', 'occasion', 'occult', 'octopus', 'ode', 'offer', 'office', 'official', 'offset', 'offshore', 'okra', 'omega', 'omelet', 'omen', 'op', 'open', 'opener', 'opening', 'opera', 'operator', 'opium', 'opponent', 'optimist', 'optimum', 'order', 'oregano', 'organist', 'organizer', 'origami', 'ornament', 'orthodontist', 'orthodox', 'otter', 'ounce', 'outcast', 'outdoor', 'outfit', 'outlaw', 'outlook', 'output', 'outsider', 'oven', 'overall', 'overkill', 'overlap', 'oversized', 'oxide', 'pacemaker', 'pacifist', 'pack', 'pact', 'pad', 'page', 'pager', 'paintball', 'pair', 'pal', 'pallet', 'palm', 'pamper', 'pan', 'pancake', 'pancreas', 'panda', 'pandemonium', 'panel', 'pang', 'pantheon', 'pantry', 'panty', 'pap', 'papa', 'paper', 'paperback', 'paprika', 'par', 'parachute', 'parade', 'paradox', 'parallel', 'paranoia', 'parasol', 'pardon', 'park', 'parka', 'parking', 'part', 'participant', 'partner', 'party', 'pass', 'passage', 'password', 'pasta', 'pastor', 'pastrami', 'patch', 'patent', 'patience', 'patio', 'patriot', 'patron', 'pauper', 'peanuts', 'pedicure', 'pedigree', 'pee', 'peel', 'peer', 'pen', 'penalty', 'pendant', 'penis', 'penitent', 'penny', 'pension', 'pentagram', 'penthouse', 'pep', 'peptide', 'percent', 'percentage', 'perfect', 'perfectionist', 'performance', 'performer', 'perimeter', 'perk', 'perm', 'permanent', 'peroxide', 'persistent', 'pertinent', 'pessimist', 'pest', 'pester', 'pesto', 'pet', 'petroleum', 'petticoat', 'petunia', 'peyote', 'pi', 'pianist', 'piano', 'piccolo', 'picture', 'pier', 'pigskin', 'pin', 'ping', 'pink', 'pint', 'piranha', 'pisser', 'piston', 'pit', 'pita', 'pitcher', 'pizza', 'pizzeria', 'placebo', 'placement', 'placenta', 'plaid', 'plan', 'planetarium', 'plank', 'plankton', 'planner', 'plant', 'planter', 'plaque', 'plasma', 'plastic', 'plateau', 'platform', 'plating', 'playa', 'playback', 'playboy', 'player', 'plop', 'plot', 'plug', 'plutonium', 'pocket', 'podium', 'poet', 'pointer', 'poker', 'polio', 'polka', 'poll', 'pollen', 'polo', 'poltergeist', 'polyester', 'pomp', 'poncho', 'pond', 'ponder', 'pony', 'pool', 'pop', 'popcorn', 'pope', 'porno', 'port', 'portable', 'portal', 'portfolio', 'pose', 'post', 'poster', 'postman', 'pot', 'potent', 'potpourri', 'power', 'practical', 'prairie', 'precedent', 'predator', 'prefect', 'prefix', 'pregnant', 'prelude', 'premier', 'premium', 'present', 'president', 'prestige', 'pretext', 'pretzel', 'preview', 'prima', 'prime', 'primer', 'print', 'printer', 'prior', 'privacy', 'privilege', 'pro', 'procedure', 'processor', 'producer', 'product', 'profane', 'professional', 'professor', 'profiler', 'program', 'project', 'projector', 'promenade', 'prominent', 'promo', 'promotion', 'prompt', 'prop', 'propaganda', 'propeller', 'proper', 'prospect', 'prospectus', 'protector', 'protest', 'protestant', 'protocol', 'proton', 'prototype', 'prove', 'provider', 'proxy', 'prudent', 'prune', 'psalm', 'psoriasis', 'psyche', 'pub', 'publicist', 'puck', 'pudding', 'pull', 'pulp', 'pulsar', 'pump', 'punch', 'punk', 'punt', 'pup', 'pupil', 'puppy', 'pus', 'push', 'pusher', 'put', 'putt', 'putter', 'pylon', 'python', 'quad', 'quasi', 'query', 'queue', 'quiche', 'quilt', 'quintet', 'quiz', 'quota', 'quote', 'rabbi', 'race', 'racer', 'racist', 'racket', 'radar', 'radiant', 'radiator', 'radio', 'radium', 'radius', 'rag', 'rage', 'ragtime', 'raid', 'raider', 'rail', 'rake', 'rally', 'ram', 'ramp', 'ranch', 'rancher', 'random', 'range', 'rank', 'ranking', 'rap', 'rapper', 'rapport', 'rat', 'rating', 'ratio', 'ravage', 'raven', 'ravioli', 'reactor', 'reader', 'ready', 'realist', 'rebel', 'rebound', 'receiver', 'recent', 'receptionist', 'receptor', 'recital', 'record', 'recorder', 'recovery', 'recruiter', 'rectum', 'recycling', 'redundant', 'reef', 'reek', 'reel', 'referee', 'reflex', 'reform', 'refuge', 'regatta', 'reggae', 'regime', 'regiment', 'register', 'regulator', 'rein', 'relax', 'relaxed', 'release', 'relevant', 'remake', 'reminder', 'remover', 'repel', 'repertoire', 'replay', 'replica', 'report', 'reporter', 'research', 'reserve', 'reservoir', 'resort', 'resource', 'respect', 'response', 'rest', 'restaurant', 'retina', 'retro', 'return', 'reverence', 'review', 'revival', 'revolver', 'revue', 'rib', 'ride', 'riff', 'ring', 'rink', 'ripper', 'risotto', 'rite', 'roadblock', 'roadie', 'rob', 'robber', 'robe', 'robot', 'rock', 'rocker', 'rodeo', 'roe', 'roller', 'roman', 'romance', 'romp', 'roof', 'rook', 'room', 'rooms', 'rooster', 'root', 'rot', 'rotor', 'rotten', 'rouge', 'roulette', 'route', 'routine', 'royalty', 'rubber', 'rug', 'rugby', 'ruin', 'ruling', 'rum', 'rumba', 'rumble', 'run', 'runner', 'rush', 'rust', 'rut', 'sabbatical', 'sabotage', 'saboteur', 'sacrament', 'sadist', 'safari', 'safe', 'saga', 'sage', 'sake', 'salami', 'sale', 'saline', 'salmonella', 'salon', 'saloon', 'salsa', 'samba', 'sample', 'sanatorium', 'sandwich', 'sap', 'sardine', 'sashimi', 'sassafras', 'satire', 'sauna', 'sax', 'scalp', 'scalpel', 'scampi', 'scan', 'scanner', 'scenario', 'scene', 'scepter', 'schilling', 'schnitzel', 'school', 'scone', 'scoop', 'scooter', 'scope', 'score', 'scorer', 'scotch', 'scout', 'scrabble', 'scrambler', 'screening', 'scrimmage', 'script', 'scrotum', 'scrub', 'sec', 'sector', 'security', 'sedan', 'sediment', 'segment', 'select', 'selenium', 'semester', 'seminar', 'senator', 'senior', 'sensor', 'sentiment', 'sequel', 'serenade', 'sergeant', 'serpent', 'serum', 'serve', 'server', 'service', 'set', 'sexy', 'shabby', 'shag', 'shaker', 'shampoo', 'shanty', 'shawl', 'sheet', 'shell', 'shelter', 'sheriff', 'sherry', 'shift', 'shilling', 'shimmy', 'shirt', 'shit', 'shock', 'shocking', 'shop', 'shopper', 'short', 'shorts', 'shot', 'shotgun', 'shovel', 'show', 'showroom', 'shrapnel', 'shunt', 'shutter', 'shuttle', 'sidecar', 'sidekick', 'sightseeing', 'sigma', 'significant', 'silicone', 'silo', 'simulator', 'single', 'sinister', 'sinus', 'sip', 'sister', 'sitcom', 'site', 'ska', 'skateboard', 'skater', 'skeet', 'sketch', 'ski', 'skin', 'skinny', 'skunk', 'skyline', 'slab', 'slag', 'slang', 'slap', 'slash', 'sleep', 'sleet', 'slice', 'slim', 'slip', 'slipper', 'slobber', 'slogan', 'slop', 'slot', 'slum', 'slump', 'slurp', 'smack', 'smart', 'smarten', 'smash', 'smiley', 'smog', 'smooth', 'snack', 'snap', 'snapper', 'snapshot', 'sneaker', 'sneer', 'snip', 'snob', 'snorkel', 'snot', 'snowboard', 'snowboarder', 'soap', 'sober', 'socialist', 'society', 'soda', 'sofa', 'soft', 'soften', 'software', 'soiree', 'solarium', 'solo', 'solvent', 'sonar', 'song', 'sonnet', 'sorry', 'soul', 'sound', 'soundtrack', 'souvenir', 'spa', 'spade', 'spaghetti', 'spam', 'span', 'spar', 'speaker', 'special', 'specialist', 'species', 'specimen', 'spectator', 'spectrum', 'speech', 'speed', 'speedway', 'spelling', 'spike', 'spin', 'spinner', 'spinster', 'spirit', 'spiritual', 'spit', 'spleen', 'splinter', 'split', 'sponsor', 'spook', 'sport', 'spot', 'spotlight', 'spotter', 'spray', 'spread', 'spring', 'sprinkler', 'sprint', 'squadron', 'square', 'squash', 'squaw', 'stadium', 'stag', 'stage', 'stalker', 'stamp', 'stand', 'star', 'start', 'starter', 'state', 'statement', 'station', 'status', 'steak', 'steel', 'stem', 'step', 'steps', 'stereo', 'stereotype', 'stern', 'steward', 'stewardess', 'stick', 'sticker', 'stigma', 'stimulus', 'stinker', 'stock', 'stomp', 'stool', 'stoop', 'stop', 'stopper', 'stopwatch', 'store', 'storm', 'stout', 'straddle', 'strand', 'strapless', 'stress', 'stretch', 'stretcher', 'strike', 'string', 'strip', 'stripper', 'striptease', 'student', 'studio', 'stuff', 'stunt', 'stuntman', 'stutter', 'styling', 'stylist', 'suave', 'sub', 'subject', 'succubus', 'succulent', 'suite', 'sullen', 'sultan', 'sumo', 'super', 'superior', 'superman', 'supermodel', 'supernova', 'superstar', 'supervisor', 'supplement', 'support', 'supporter', 'surf', 'surfer', 'surplus', 'surprise', 'surveillance', 'survival', 'sushi', 'suspect', 'suspense', 'swami', 'swap', 'swastika', 'sweater', 'sweatshirt', 'sweetie', 'swing', 'switch', 'syllabus', 'symposium', 'synthesizer', 'tab', 'tabasco', 'tablet', 'tabloid', 'tackle', 'taco', 'tact', 'tag', 'talent', 'talisman', 'talk', 'talon', 'tampon', 'tang', 'tango', 'tank', 'tanker', 'tap', 'tape', 'tapioca', 'taps', 'tarantula', 'target', 'tarmac', 'tarot', 'taupe', 'taxi', 'team', 'teamwork', 'techno', 'teen', 'teenager', 'telegram', 'teller', 'temperament', 'template', 'tempo', 'ten', 'tenant', 'tender', 'tennis', 'tenor', 'tent', 'tequila', 'term', 'terminal', 'terrorist', 'test', 'testament', 'tetanus', 'theater', 'thermometer', 'thermos', 'thesis', 'thriller', 'tiara', 'tic', 'ticket', 'tier', 'time', 'timer', 'timing', 'tin', 'tip', 'tipsy', 'tissue', 'titan', 'titanium', 'toast', 'toaster', 'toe', 'toffee', 'tofu', 'toga', 'toilet', 'token', 'tolerant', 'tomahawk', 'ton', 'toner', 'tonic', 'tool', 'toot', 'top', 'topic', 'topless', 'torment', 'tornado', 'torpedo', 'torso', 'toss', 'tot', 'totem', 'tour', 'tourniquet', 'trachea', 'track', 'tractor', 'trail', 'trailer', 'train', 'trainee', 'trainer', 'training', 'tram', 'tramp', 'trampoline', 'trance', 'transcendent', 'transcript', 'transfer', 'transistor', 'transit', 'transport', 'trap', 'trapeze', 'trapper', 'trash', 'trauma', 'traverse', 'tree', 'tremor', 'trend', 'trendy', 'triage', 'trial', 'tribal', 'tribune', 'tricky', 'trimester', 'trio', 'trip', 'trivia', 'trolley', 'trombone', 'troop', 'truck', 'trucker', 'trust', 'trustee', 'tsunami', 'tuba', 'tube', 'tumor', 'tune', 'tunnel', 'turban', 'turbine', 'turbo', 'turbulent', 'turf', 'turquoise', 'tutu', 'tv', 'tweed', 'tweet', 'twist', 'twister', 'tycoon', 'type', 'typist', 'ultimatum', 'ultraviolet', 'umpire', 'undercover', 'underdog', 'underground', 'understatement', 'unfair', 'uniform', 'unit', 'unplugged', 'update', 'upgrade', 'upper', 'uppercut', 'uranium', 'urgent', 'urine', 'urn', 'uterus', 'utility', 'vacant', 'vaccine', 'vagina', 'vale', 'valium', 'valve', 'vamp', 'van', 'varsity', 'vast', 'vat', 'vector', 'veer', 'veil', 'velvet', 'vendetta', 'vent', 'ventilator', 'ventriloquist', 'venture', 'veranda', 'verdict', 'vermouth', 'vest', 'vet', 'veto', 'vibrator', 'video', 'videotape', 'view', 'viewer', 'vigilant', 'vigilante', 'villa', 'vintage', 'vinyl', 'violent', 'violet', 'virus', 'visa', 'voicemail', 'volley', 'voltage', 'volume', 'voodoo', 'voucher', 'voyeur', 'wad', 'wagon', 'wake', 'walrus', 'wand', 'want', 'warlord', 'warm', 'warrant', 'water', 'waterbed', 'waterproof', 'wave', 'web', 'website', 'wed', 'wedding', 'wee', 'weed', 'week', 'weekend', 'welfare', 'west', 'western', 'wet', 'whiplash', 'whirlpool', 'wig', 'wild', 'wildebeest', 'winch', 'wind', 'winner', 'winter', 'wireless', 'wit', 'wok', 'wolf', 'womanizer', 'wonder', 'workaholic', 'workshop', 'worm', 'wringer', 'wrong', 'yank', 'yard', 'yell', 'yen', 'yoga', 'yuppie', 'zebra', 'zest', 'zigzag', 'zit', 'zombie', 'zone', 'zoo', 'zoom']\n"
          ]
        }
      ],
      "source": [
        "# filter out pseudowords that happen to be valid Dutch words (mind case folding!)\n",
        "# show the set of pseudowords filtered out.\n",
        "# 1 point for applying the correct filtering\n",
        "\n",
        "\n",
        "# Gatti pseudowords (row names)\n",
        "gatti_words = gatti_df.index.str.lower()\n",
        "\n",
        "# Dutch real words from Speed & Brysbaert prevalence lexicon\n",
        "dutch_words = set(speed_df['word'].str.lower())\n",
        "\n",
        "# Converting to set & filtering out overlapping words (pseudowords that are valid Dutch words)\n",
        "filtered_out = sorted(set(gatti_words).intersection(dutch_words))\n",
        "\n",
        "# Filtering Gatti pseudowords that are in the real Dutch words set\n",
        "gatti_filtered_df = gatti_df[~gatti_df.index.str.lower().isin(dutch_words)]\n",
        "\n",
        "\n",
        "print(\"Pseudowords that were filtered out:\")\n",
        "print(filtered_out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Version 1 - not sure if its wrong?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zvza9pb6MFgC"
      },
      "outputs": [],
      "source": [
        "# # encode Dutch words and pseudowords from Gatti et al as uni- and bi-gram vectors\n",
        "# # show the uni-gram and bi-gram encoding of the pseudoword ampgrair\n",
        "# # 2 points for correctly encoding the target strings as uni- and bi-gram vectors\n",
        "\n",
        "# from collections import Counter\n",
        "# import numpy as np\n",
        "\n",
        "# # Step 1: Create a list of Dutch words from the valence dataset\n",
        "# valence_df = pd.read_excel(\"/Users/bramdewaal/Desktop/Uni/VSC/CL/Group Assignment/data/BrysbaertValence.xlsx\")\n",
        "\n",
        "# # Show the first few rows\n",
        "# print(valence_df.head())\n",
        "\n",
        "# # Show all column names\n",
        "# print(valence_df.columns)\n",
        "\n",
        "# # Clean: Keep only rows marked as known words (RemoveUnknown == 1)\n",
        "# valence_df = valence_df[valence_df[\"RemoveUnknown\"] == 1]\n",
        "\n",
        "# # Keep only the columns we care about\n",
        "# valence_df = valence_df[[\"Word\", \"Valence\"]]\n",
        "\n",
        "# # Normalize: lowercase the words\n",
        "# valence_df[\"Word\"] = valence_df[\"Word\"].str.lower()\n",
        "\n",
        "# # Final check\n",
        "# print(valence_df.head())\n",
        "\n",
        "# dutch_words = valence_df['Word'].dropna().astype(str).str.lower().tolist()\n",
        "\n",
        "# # Step 2: Extract all unigrams and bigrams in the corpus\n",
        "# def get_ngrams(word: str, n: int):\n",
        "#     return [word[i:i+n] for i in range(len(word) - n + 1)]\n",
        "\n",
        "# # Collect all unigrams and bigrams from the corpus\n",
        "# unigrams = set()\n",
        "# bigrams = set()\n",
        "# for word in dutch_words:\n",
        "#     unigrams.update(get_ngrams(word, 1))\n",
        "#     bigrams.update(get_ngrams(word, 2))\n",
        "\n",
        "# # Create sorted vocabularies (to lock feature order)\n",
        "# unigram_vocab = sorted(unigrams)\n",
        "# bigram_vocab = sorted(bigrams)\n",
        "\n",
        "# # Map each n-gram to a vector index\n",
        "# unigram_index = {gram: i for i, gram in enumerate(unigram_vocab)}\n",
        "# bigram_index = {gram: i for i, gram in enumerate(bigram_vocab)}\n",
        "\n",
        "# # Step 3: Define encoding functions\n",
        "# def encode_word_ngrams(word: str, index_map: dict, n: int) -> np.ndarray:\n",
        "#     \"\"\"Encodes a word as a sparse n-gram vector using a given n-gram index map.\"\"\"\n",
        "#     vec = np.zeros(len(index_map))\n",
        "#     ngrams = get_ngrams(word.lower(), n)\n",
        "#     counts = Counter(ngrams)\n",
        "#     for gram, count in counts.items():\n",
        "#         if gram in index_map:\n",
        "#             vec[index_map[gram]] = count\n",
        "#     return vec\n",
        "\n",
        "\n",
        "\n",
        "# # Encode the pseudoword \"ampgrair\"\n",
        "# word = \"ampgrair\"\n",
        "\n",
        "# uni_vec = encode_word_ngrams(word, unigram_index, 1)\n",
        "# bi_vec = encode_word_ngrams(word, bigram_index, 2)\n",
        "\n",
        "# # Preview the non-zero parts\n",
        "# def print_vector(vec, vocab):\n",
        "#     found = False\n",
        "#     for i, v in enumerate(vec):\n",
        "#         if v != 0:\n",
        "#             print(f\"{vocab[i]}: {int(v)}\")\n",
        "#     if not found:\n",
        "#         print(\"(No known n-grams found in vocabulary)\")\n",
        "\n",
        "# print(\"Unigram vector for 'ampgrair':\")\n",
        "# print_vector(uni_vec, unigram_vocab)\n",
        "\n",
        "# print(\"\\nBigram vector for 'ampgrair':\")\n",
        "# print_vector(bi_vec, bigram_vocab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Version 2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram vector for 'ampgrair':\n",
            "a: 2\n",
            "g: 1\n",
            "i: 1\n",
            "m: 1\n",
            "p: 1\n",
            "r: 2\n",
            "\n",
            "Bigram vector for 'ampgrair':\n",
            "ai: 1\n",
            "am: 1\n",
            "gr: 1\n",
            "ir: 1\n",
            "mp: 1\n",
            "pg: 1\n",
            "ra: 1\n"
          ]
        }
      ],
      "source": [
        "# # encode Dutch words and pseudowords from Gatti et al as uni- and bi-gram vectors\n",
        "# # show the uni-gram and bi-gram encoding of the pseudoword ampgrair\n",
        "# # 2 points for correctly encoding the target strings as uni- and bi-gram vectors\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Function to get character n-grams\n",
        "def get_ngrams(word: str, n: int):\n",
        "    return [word[i:i+n] for i in range(len(word) - n + 1)]\n",
        "\n",
        "valence_df = pd.read_excel(\"/Users/bramdewaal/Desktop/Uni/VSC/CL/Group Assignment/data/BrysbaertValence.xlsx\")\n",
        "# Collect all words for vocabulary (Dutch + pseudowords)\n",
        "dutch_words = valence_df[\"Word\"].astype(str).str.lower().tolist()\n",
        "pseudowords = gatti_filtered_df.index.str.lower().tolist()\n",
        "all_words = dutch_words + pseudowords\n",
        "\n",
        "# Build unigram and bigram vocabularies\n",
        "unigrams = sorted(set(c for w in all_words for c in get_ngrams(w, 1)))\n",
        "bigrams = sorted(set(b for w in all_words for b in get_ngrams(w, 2)))\n",
        "\n",
        "# Index maps\n",
        "unigram_index = {gram: i for i, gram in enumerate(unigrams)}\n",
        "bigram_index = {gram: i for i, gram in enumerate(bigrams)}\n",
        "\n",
        "# N-gram encoding function\n",
        "def encode_word_ngrams(word: str, index_map: dict, n: int) -> np.ndarray:\n",
        "    vec = np.zeros(len(index_map))\n",
        "    ngrams = get_ngrams(word.lower(), n)\n",
        "    counts = Counter(ngrams)\n",
        "    for gram, count in counts.items():\n",
        "        if gram in index_map:\n",
        "            vec[index_map[gram]] = count\n",
        "    return vec\n",
        "\n",
        "# Helper to print non-zero entries\n",
        "def print_vector(vec, vocab):\n",
        "    found = False\n",
        "    for i, v in enumerate(vec):\n",
        "        if v != 0:\n",
        "            print(f\"{vocab[i]}: {int(v)}\")\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(\"(No known n-grams found in vocabulary)\")\n",
        "\n",
        "# Encoding 'ampgrair'\n",
        "word = \"ampgrair\"\n",
        "print(\"Unigram vector for 'ampgrair':\")\n",
        "uni_vec = encode_word_ngrams(word, unigram_index, 1)\n",
        "print_vector(uni_vec, unigrams)\n",
        "\n",
        "print(\"\\nBigram vector for 'ampgrair':\")\n",
        "bi_vec = encode_word_ngrams(word, bigram_index, 2)\n",
        "print_vector(bi_vec, bigrams)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# !git add data/BrysbaertValence.xlsx data/data_pseudovalence.RData data/prevalence_netherlands.csv\n",
        "# !git commit -m \"Add data files\"\n",
        "# !git push origin main\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-w9-ySEuPp1Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uni-gram model Mean Squared Error: 0.42390078742333853\n",
            "Bi-gram model Mean Squared Error: 2.7532474695153363e+22\n"
          ]
        }
      ],
      "source": [
        "# use word valence estimates from Speed and Brysbaert (2024) to train\n",
        "# - a uni-gram model\n",
        "# - a bi-gram model\n",
        "# 2 points for correctly trained models\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "\n",
        "valence_df = pd.read_excel(\"/Users/bramdewaal/Desktop/Uni/VSC/CL/Group Assignment/data/BrysbaertValence.xlsx\")\n",
        "\n",
        "# Create the uni-gram and bi-gram features for each word using the previously defined encoding function\n",
        "# Note: adjust the column names if necessary. Here we assume the words are in the column \"Word\"\n",
        "valence_df[\"unigram_features\"] = valence_df[\"Word\"].apply(lambda w: encode_word_ngrams(w, unigram_index, 1))\n",
        "valence_df[\"bigram_features\"] = valence_df[\"Word\"].apply(lambda w: encode_word_ngrams(w, bigram_index, 2))\n",
        "\n",
        "# Convert the features into numpy arrays for modeling\n",
        "X_uni = np.vstack(valence_df[\"unigram_features\"].values)\n",
        "X_bi = np.vstack(valence_df[\"bigram_features\"].values)\n",
        "y = valence_df[\"Valence\"].values  # Adjust if the column name is different\n",
        "\n",
        "# Split the data into training and testing sets (using the same random_state for reproducibility)\n",
        "X_uni_train, X_uni_test, y_train, y_test = train_test_split(X_uni, y, test_size=0.2, random_state=42)\n",
        "X_bi_train, X_bi_test, _, _ = train_test_split(X_bi, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a uni-gram model using Linear Regression\n",
        "uni_model = LinearRegression()\n",
        "uni_model.fit(X_uni_train, y_train)\n",
        "\n",
        "# Train a bi-gram model using Linear Regression\n",
        "bi_model = LinearRegression()\n",
        "bi_model.fit(X_bi_train, y_train)\n",
        "\n",
        "# Evaluate both models on their respective test sets\n",
        "y_uni_pred = uni_model.predict(X_uni_test)\n",
        "y_bi_pred = bi_model.predict(X_bi_test)\n",
        "\n",
        "uni_mse = mean_squared_error(y_test, y_uni_pred)\n",
        "bi_mse = mean_squared_error(y_test, y_bi_pred)\n",
        "\n",
        "print(\"Uni-gram model Mean Squared Error:\", uni_mse)\n",
        "print(\"Bi-gram model Mean Squared Error:\", bi_mse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6RG3aeRSPtLX"
      },
      "outputs": [],
      "source": [
        "# apply trained models to predict the valence of pseudowords from Gatti et al (2024).\n",
        "# Then apply the same models back onto the training set to see how well they predict the valence of words in Speed and Brysbaert (2024).\n",
        "# 2 points for correctly applied models\n",
        "\n",
        "\n",
        "'''First we encode the pseudowords so they work with the models'''\n",
        "pseudowords = gatti_filtered_df.index.str.lower().tolist()\n",
        "pseudoword_uni_features = np.vstack([encode_word_ngrams(w, unigram_index, 1) for w in pseudowords])\n",
        "pseudoword_bi_features = np.vstack([encode_word_ngrams(w, bigram_index, 2) for w in pseudowords])\n",
        "\n",
        "'''Then we apply the trained models on the uni and bigram features'''\n",
        "pw_valence_uni = uni_model.predict(pseudoword_uni_features)\n",
        "pw_valence_bi = bi_model.predict(pseudoword_bi_features)\n",
        "\n",
        "'''Applying models to training set -> predict valence of words in Speed and Brysbaert (2024)'''\n",
        "speed_valence_uni = uni_model.predict(X_uni_train)\n",
        "speed_valence_bi = bi_model.predict(X_bi_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9aDwbajtPxze"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training words - uni-gram Spearman correlation: 0.098\n",
            "Training words - bi-gram Spearman correlation: 0.327\n",
            "Pseudowords - uni-gram Spearman correlation: 0.049\n",
            "Pseudowords - bi-gram Spearman correlation: 0.049\n"
          ]
        }
      ],
      "source": [
        "# compute the Spearman correlation coefficients between true valence and predicted valence under both uni- and bi-gram models for\n",
        "# - words from Speed and Brysbaert (2024)\n",
        "# - pseudowords from Gatti and colleagues (2024)\n",
        "# show both correlation coefficients.\n",
        "# 2 points for the correct Spearman correlation coefficients (rounded to the third decimal place)\n",
        "\n",
        "import scipy.stats\n",
        "\n",
        "spearman_uni_train, p_val_uni_train = scipy.stats.spearmanr(y_train, speed_valence_uni)\n",
        "spearman_bi_train, p_val_bi_train = scipy.stats.spearmanr(y_train, speed_valence_bi)\n",
        "\n",
        "\n",
        "true_pseudoword_valence = gatti_filtered_df[\"Valence\"].values\n",
        "spearman_uni_pw, p_val_uni_pw = scipy.stats.spearmanr(true_pseudoword_valence, pw_valence_uni)\n",
        "spearman_bi_pw, p_val_bi_pw = scipy.stats.spearmanr(true_pseudoword_valence, pw_valence_bi)\n",
        "\n",
        "\n",
        "print(\"Training words - uni-gram Spearman correlation:\", round(spearman_uni_train, 3))\n",
        "print(\"Training words - bi-gram Spearman correlation:\", round(spearman_bi_train, 3))\n",
        "print(\"Pseudowords - uni-gram Spearman correlation:\", round(spearman_uni_pw, 3))\n",
        "print(\"Pseudowords - bi-gram Spearman correlation:\", round(spearman_bi_pw, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0P1rNg5QoDn"
      },
      "source": [
        "**Task 2** (*8 points available, see breakdown below*)\n",
        "\n",
        "Again following Gatti and colleagues, you should encode the target strings (pseudowords and Dutch words from Speed and Brysbaert) as fastText embeddings, train a multiple regression model on Dutch words and apply it to the pseudowords in Gatti et al. You should finally report the Spearman correlation coefficient between observed and predicted valence for both words and pseudowords.\n",
        "\n",
        "You should use the pre-trained fastText model for Dutch, available at this page: https://fasttext.cc/docs/en/crawl-vectors.html\n",
        "\n",
        "Finally, you should answer two questions about the fastText model (see below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mDEPRECATION: Loading egg at /Users/bramdewaal/anaconda3/lib/python3.11/site-packages/colorcorrect-0.9.1-py3.11-macosx-11.1-arm64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gensim in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (4.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from gensim) (5.2.1)\n",
            "Collecting FuzzyTM>=0.4.0 (from gensim)\n",
            "  Downloading FuzzyTM-2.0.9-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: pandas in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim) (2.1.4)\n",
            "Collecting pyfume (from FuzzyTM>=0.4.0->gensim)\n",
            "  Downloading pyFUME-0.3.4-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3)\n",
            "Collecting scipy>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.10.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (100 kB)\n",
            "Collecting numpy>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.24.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
            "Collecting simpful==2.12.0 (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
            "  Downloading simpful-2.12.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting fst-pso==1.8.1 (from pyfume->FuzzyTM>=0.4.0->gensim)\n",
            "  Downloading fst-pso-1.8.1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting pandas (from FuzzyTM>=0.4.0->gensim)\n",
            "  Downloading pandas-1.5.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
            "Collecting miniful (from fst-pso==1.8.1->pyfume->FuzzyTM>=0.4.0->gensim)\n",
            "  Downloading miniful-0.0.6.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/bramdewaal/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
            "Downloading FuzzyTM-2.0.9-py3-none-any.whl (31 kB)\n",
            "Downloading pyFUME-0.3.4-py3-none-any.whl (60 kB)\n",
            "Downloading numpy-1.24.4-cp311-cp311-macosx_11_0_arm64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp311-cp311-macosx_12_0_arm64.whl (28.7 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m28.7/28.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pandas-1.5.3-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading simpful-2.12.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: fst-pso, miniful\n",
            "  Building wheel for fst-pso (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for fst-pso: filename=fst_pso-1.8.1-py3-none-any.whl size=20430 sha256=073ebf0d341d5ca2a823750564da78fb980f48afd7dbf77c2b9ef4f54bb6cec1\n",
            "  Stored in directory: /Users/bramdewaal/Library/Caches/pip/wheels/69/f5/e5/18ad53fe1ed6b2af9fad05ec052e4acbac8e92441df44bad2e\n",
            "  Building wheel for miniful (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for miniful: filename=miniful-0.0.6-py3-none-any.whl size=3513 sha256=7f2aa7669c1a883296bceb772fdf822075c56a9395b7687bdf3f6b4b266ffbaf\n",
            "  Stored in directory: /Users/bramdewaal/Library/Caches/pip/wheels/9d/ff/2f/afe4cd56f47de147407705626517d68bea0f3b74eb1fb168e6\n",
            "Successfully built fst-pso miniful\n",
            "Installing collected packages: numpy, scipy, pandas, simpful, miniful, fst-pso, pyfume, FuzzyTM\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.12.0\n",
            "    Uninstalling scipy-1.12.0:\n",
            "      Successfully uninstalled scipy-1.12.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.1.4\n",
            "    Uninstalling pandas-2.1.4:\n",
            "      Successfully uninstalled pandas-2.1.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed FuzzyTM-2.0.9 fst-pso-1.8.1 miniful-0.0.6 numpy-1.24.4 pandas-1.5.3 pyfume-0.3.4 scipy-1.10.1 simpful-2.12.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "!gunzip \"/Users/bramdewaal/Desktop/Uni/VSC/CL/Group Assignment/cc.nl.300.bin.gz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "pDeWgWUNAckd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "[-0.02131224 -0.03914224  0.06773406  0.03044282 -0.04795513  0.0471774\n",
            " -0.00110865 -0.01323065  0.03282857 -0.04470051 -0.00056513  0.01193492\n",
            "  0.02667335  0.031089    0.0165347  -0.08833717 -0.06693563  0.11471064\n",
            "  0.03419662 -0.05047507  0.01103405  0.09136231  0.03186877  0.01567305\n",
            " -0.06836259 -0.04501139  0.00831977  0.02103915  0.0024384   0.03259584\n",
            "  0.02246954 -0.09433993 -0.0057069  -0.09292027  0.0376602   0.03086784\n",
            "  0.00895987  0.04552672  0.05240382 -0.06856237  0.01585365 -0.20638812\n",
            " -0.02153354 -0.0542951  -0.0481896   0.03524248 -0.00424928  0.01175312\n",
            " -0.06431995 -0.01566298  0.01789582  0.0571667   0.09400827  0.03475932\n",
            " -0.00063606 -0.0306391   0.00573056 -0.0460996  -0.06893699  0.02682721\n",
            "  0.04458159  0.01929477 -0.1449769   0.00843618  0.0695213  -0.00264898\n",
            "  0.01616256 -0.12111287 -0.00664816  0.06675841 -0.01989051 -0.00071482\n",
            " -0.09768243  0.02444565  0.04862483 -0.01850101 -0.04908647  0.0343486\n",
            " -0.01243571 -0.04144683  0.01281989 -0.09639438 -0.00234332 -0.06621715\n",
            "  0.01280874 -0.02833897 -0.04033132  0.05653621  0.03721082 -0.03457037\n",
            "  0.00494653  0.0493472  -0.05341899  0.04138985 -0.00767424  0.00332097\n",
            "  0.01620222  0.00193611 -0.03022063 -0.08323933  0.09519112  0.00432625\n",
            " -0.04004045 -0.10430649 -0.08619742 -0.01476227 -0.01296975  0.03701226\n",
            " -0.19539563 -0.01264681  0.02444809  0.06090596  0.03118393 -0.01617927\n",
            "  0.11142697 -0.02811106 -0.02202673  0.03177032  0.06785139  0.00723322\n",
            " -0.04196172  0.11863252 -0.03332871  0.00484113 -0.0260238  -0.01933361\n",
            "  0.00434351 -0.05805296  0.03562591 -0.00035244 -0.0733141   0.01399364\n",
            "  0.06282719 -0.05478631  0.04929556  0.01380771 -0.01182108 -0.01274281\n",
            "  0.00591893  0.08577131  0.16146314 -0.05213245 -0.00941433 -0.12907663\n",
            "  0.00549065 -0.02568225  0.01240781 -0.03030521  0.07943733 -0.12955438\n",
            "  0.06180012  0.02849957  0.10037531 -0.07096606  0.08906214 -0.01631669\n",
            " -0.03996637 -0.13516918  0.0075412   0.04107747 -0.13253435 -0.01194683\n",
            "  0.01952422 -0.09325323  0.00214289  0.03298137  0.06850658  0.01320536\n",
            "  0.10253057 -0.01521363  0.07894865  0.00132831 -0.05331015  0.01225939\n",
            "  0.0171942   0.07275511  0.04754978  0.06331414 -0.04697984  0.01021375\n",
            " -0.08750767 -0.00595224  0.07779851  0.0087485   0.06124276 -0.00997075\n",
            "  0.02871649 -0.01437629  0.06830463 -0.05777899 -0.02321553  0.06840885\n",
            " -0.03811003  0.0086154  -0.02268989  0.0687841  -0.05955248  0.05491076\n",
            "  0.06073437  0.0104506  -0.03804616  0.04561938 -0.02077121 -0.08074014\n",
            " -0.04657479 -0.1240291   0.01508617 -0.00326952  0.01245612 -0.02418405\n",
            " -0.02419944  0.04213547 -0.0444555   0.00318597  0.01980436 -0.12992169\n",
            "  0.0184258   0.04880042 -0.03580744 -0.01486654  0.0563505   0.02030133\n",
            " -0.03212663  0.02299836 -0.04717481  0.03250478 -0.12333264  0.07215789\n",
            "  0.01386749  0.00352205 -0.0278593  -0.00802406 -0.00181735  0.0066144\n",
            "  0.02930986  0.01474511 -0.119991   -0.05373005 -0.05973876 -0.02736309\n",
            " -0.06831048  0.0966069   0.01576634 -0.11055261 -0.04929718  0.02257616\n",
            " -0.05566834  0.05026255 -0.03521036 -0.03181789  0.05866977 -0.0420126\n",
            "  0.0198932   0.04597303 -0.0556966   0.04256352 -0.03841452  0.04606393\n",
            " -0.04256792 -0.01110659 -0.13300309 -0.05138557 -0.01779121 -0.08198467\n",
            "  0.01721934 -0.05848791  0.03478605 -0.0125112   0.14409226  0.0394686\n",
            " -0.10637125  0.0003878  -0.16385877  0.01813789 -0.00888261 -0.06602093\n",
            "  0.10463798 -0.05263285  0.02294655 -0.02493627 -0.21764112 -0.03733071\n",
            "  0.08180938  0.05808686  0.04342903  0.00590533 -0.24646062 -0.01221272\n",
            "  0.00810309 -0.04023027  0.02508576 -0.0360238   0.00302843 -0.01018841\n",
            "  0.01286944  0.04270816  0.09637007 -0.02963477  0.0119648   0.01005364]\n"
          ]
        }
      ],
      "source": [
        "# load the fastText model\n",
        "# 1 point for correctly loading the appropriate fastText model\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "ft_dutch_model = fasttext.load_model(\"/Users/bramdewaal/Desktop/Uni/VSC/CL/Group Assignment/cc.nl.300.bin\")\n",
        "\n",
        "\"Checking if 'huis' is in the model to see if the model loaded correctly\"\n",
        "print('huis' in ft_dutch_model)  \n",
        "\n",
        "\"Checking the vector representation for 'huis'\"\n",
        "print(ft_dutch_model['huis'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensionality of pre-trained Dutch fastText embeddings: 300\n"
          ]
        }
      ],
      "source": [
        "print(\"Dimensionality of pre-trained Dutch fastText embeddings:\", ft_dutch_model.get_dimension())\n",
        " # Information also available on fastText documentation website: https://fasttext.cc/docs/en/crawl-vectors.html\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUD0VUJeRhr3"
      },
      "source": [
        "**What is the dimensionality of the pre-trained Dutch fastText embeddings? (*1 point for the correct answer*)**\n",
        "\n",
        "- The dimensionality of the pre-trained Dutch fastText embeddings is 300\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgej4BPNRoUE"
      },
      "source": [
        "**What minimum and maximum n-gram size was specified for training this fastText model? (*1 point for the correct answer*)**\n",
        "\n",
        "- According to the documentation (https://fasttext.cc/docs/en/crawl-vectors.html) the n-gram size used during training was 5, so both the minimum and maximum n-gram size is 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "aW-XEksGR28U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 20 values for 'speelplaats':\n",
            "[ 0.0253247  -0.00634261  0.02746305 -0.04024595  0.04888906  0.00660965\n",
            " -0.04152017 -0.01824508 -0.00645641  0.00093806  0.0708492  -0.03291791\n",
            "  0.00263817 -0.02825846 -0.02188046 -0.03188037 -0.01846142 -0.02203094\n",
            " -0.01883078 -0.00259199]\n",
            "\n",
            "First 20 values for 'danchunk':\n",
            "[-0.00592199  0.00097547  0.05925412  0.00053251 -0.00386978 -0.02089076\n",
            " -0.02829577  0.00972911 -0.02510111 -0.11454885 -0.02695064  0.01551034\n",
            "  0.02384409  0.01009528  0.04545438  0.00997385 -0.00474529  0.02524533\n",
            "  0.02430548 -0.02851078]\n"
          ]
        }
      ],
      "source": [
        "# encode Dutch words and pseudowords as fastText embeddings\n",
        "# show the first 20 values of the embedding of the word 'speelplaats' and of the pseudoword 'danchunk'\n",
        "# 2 points for correctly encoding words and pseudowords with fastText\n",
        "import fasttext\n",
        "\n",
        "embedding_speelplaats = ft_dutch_model.get_word_vector(\"speelplaats\")\n",
        "embedding_danchunk = ft_dutch_model.get_word_vector(\"danchunk\")\n",
        "\n",
        "print(\"First 20 values for 'speelplaats':\")\n",
        "print(embedding_speelplaats[:20])\n",
        "\n",
        "print(\"\\nFirst 20 values for 'danchunk':\")\n",
        "print(embedding_danchunk[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ePBth7cSAJU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error: 0.20609871160922746\n"
          ]
        }
      ],
      "source": [
        "# train regression model on word valence\n",
        "# 1 point for correctly training the regression model\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "words = dutch_words\n",
        "embeddings = [ft_dutch_model.get_word_vector(word) for word in words]\n",
        "\n",
        "X = np.array(embeddings)\n",
        "\n",
        "\n",
        "valence_ratings = valence_df[\"Valence\"].tolist()\n",
        "y = np.array(valence_ratings)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "reg_model = LinearRegression()\n",
        "reg_model.fit(X_train, y_train)\n",
        "\n",
        "\"Checking MSE for the model to see how it performs\"\n",
        "y_pred = reg_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "REJnnM2mSHHK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted valence for pseudowords (Gatti et al. 2024):\n",
            "[3.3738945 3.1622114 2.4161425 ... 3.0609865 2.810243  2.997438 ]\n",
            "Mean Squared Error on training data (X_train): 0.19423214498309957\n",
            "Spearman correlation on training data (X_train): 0.722\n",
            "The root mean squared error is: 0.4404543109109048\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Checked the range of valence ratings in Gatti dataframe, they range between 1.26 and 8.53. This indicates that a RMSE of ~0.44\\ncan be seen as relatively good, as the difference predicted between actual valence is only ~0.44 units. \\nRMSE was used, because squaring the errors converts them to \"units squared,\" and taking the square root \\nbrings them back to the original units of the target variable.\\n'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# apply the trained model to predict the valence of pseudowords from Gatti et al (2024).\n",
        "# Then apply the same model back onto the training set to see how well it predicts the valence of words in Speed and Brysbaert (2024).\n",
        "# 1 point for correctly applied model\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "\"Pseudowords in list form\"\n",
        "pseudowords = gatti_filtered_df.index.str.lower().tolist()\n",
        "pseudoword_embeddings = np.array([ft_dutch_model.get_word_vector(word) for word in pseudowords])\n",
        "\n",
        "\"Predicting on the pseudowords from Gatti\"\n",
        "pseudoword_valence_pred = reg_model.predict(pseudoword_embeddings)\n",
        "\n",
        "print(\"Predicted valence for pseudowords (Gatti et al. 2024):\")\n",
        "print(pseudoword_valence_pred)\n",
        "\n",
        "\"Applying model on training set\"\n",
        "train_valence_pred = reg_model.predict(X_train)\n",
        "\n",
        "# Evaluate the predictions on the training set against the true values (y_train)\n",
        "mse_train = mean_squared_error(y_train, train_valence_pred)\n",
        "spearman_corr_train, _ = spearmanr(y_train, train_valence_pred)\n",
        "\n",
        "print(\"Mean Squared Error on training data (X_train):\", mse_train)\n",
        "print(\"Spearman correlation on training data (X_train):\", round(spearman_corr_train, 3))\n",
        "\n",
        "rmse_train_valence_pred = np.sqrt(0.194)  \n",
        "print(\"The root mean squared error is:\", rmse_train_valence_pred)\n",
        "\n",
        "'''Checked the range of valence ratings in Gatti dataframe, they range between 1.26 and 8.53. This indicates that a RMSE of ~0.44\n",
        "can be seen as relatively good, as the difference predicted between actual valence is only ~0.44 units. \n",
        "RMSE was used, because squaring the errors converts them to \"units squared,\" and taking the square root \n",
        "brings them back to the original units of the target variable.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIyyTyfHSKh5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Squared Error on training data (X_train): 0.19423214498309957\n",
            "Spearman correlation on training data (X_train): 0.722\n",
            "Spearman correlation for pseudowords (Gatti et al. 2024): 0.476\n"
          ]
        }
      ],
      "source": [
        "# compute the Spearman correlation coefficients between true valence and predicted valence for\n",
        "# - words from Speed and Brysbaert (2024)\n",
        "# - pseudowords from Gatti and colleagues (2024)\n",
        "# show the correlation coefficient.\n",
        "# 1 point for the correct Spearman correlation coefficients (rounded to the third decimal place)\n",
        "\n",
        "spearman_corr_train, _ = spearmanr(y_train, train_valence_pred)\n",
        "\n",
        "print(\"Mean Squared Error on training data (X_train):\", mse_train)\n",
        "print(\"Spearman correlation on training data (X_train):\", round(spearman_corr_train, 3))\n",
        "\n",
        "# Assuming your pseudoword DataFrame (gatti_filtered_df) contains a 'Valence' column with the true ratings:\n",
        "y_pseudowords = gatti_filtered_df[\"Valence\"].values\n",
        "\n",
        "# Compute Spearman correlation for pseudowords\n",
        "spearman_corr_pseudowords, _ = spearmanr(y_pseudowords, pseudoword_valence_pred)\n",
        "\n",
        "print(\"Spearman correlation for pseudowords (Gatti et al. 2024):\", round(spearman_corr_pseudowords, 3))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnqKJ5XOSTbM"
      },
      "source": [
        "**Task 3** (*6 points available, see breakdown below*)\n",
        "\n",
        "Now you are asked to extend the work by Gatti et al by also considering the representations learned by a transformer-based models, in detail *RobBERT v2* (https://huggingface.co/pdelobelle/robbert-v2-dutch-base). You should follow the same pipeline as for the previous models, encoding both Dutch words from Speed and Brysbaert (2024) and the pseudowords from Gatti et al using the embedding of each string at layer 0, before positional information is factored in. If a string consists of multiple tokens, average the embeddings of all tokens to produce the embedding of the whole string. Then train a multiple regression model on the valence of Dutch words, apply it to the pseudowords, and compute the Spearman correlation between observed and predicted ratings.\n",
        "\n",
        "Use the HuggingFace model card for RobBERT v2 to check how to access it.\n",
        "\n",
        "I recommend saving the embeddings to file once you have generated them and you know they are correct: embedding thousands of strings takes some time, and you don't want to have to do it again. For the same reason, develop your code by considering only a small fractions of the words and pseudowords, in order to quickly see if something is wrong. Only when you are positive it works, embed all strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "id": "Ppi-Zcp6i9Rn"
      },
      "outputs": [],
      "source": [
        "# load and instantiate the right model\n",
        "# 1 point for loading the right model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nGBaQgZqZzhw"
      },
      "outputs": [],
      "source": [
        "# encode the words and pseudowords using RobBERT v2. I've used the free GPU runtime on COLAB to speed things up,\n",
        "# but in this case you need to batch the words and pseudowords. You can use the function below to create batches\n",
        "# but you will have to pay attention at how you store embeddings.\n",
        "# show the first 20 values of the embedding of the word 'miauwen' and of the pseudoword 'lixthless'\n",
        "# 2 points for correctly encoding words and pseudowords\n",
        "\n",
        "def chunks(lst, n):\n",
        "\n",
        "    \"\"\"Chunks a list into equal chunks containing n elements. Returns a list of lists.\"\"\"\n",
        "\n",
        "    chunked = []\n",
        "    for i in range(0, len(lst), n):\n",
        "        chunked.append(lst[i:i + n])\n",
        "    return chunked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BFq3hHCDUPjL"
      },
      "outputs": [],
      "source": [
        "# train regression model on word valence estimates from Speed and Brysbaert (2024)\n",
        "# 1 point for correctly training the regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "evaU9NAxUSoW"
      },
      "outputs": [],
      "source": [
        "# apply the trained model to predict the valence of pseudowords from Gatti et al (2024).\n",
        "# Then apply the same model back onto the training set to see how well it predicts the valence of words in Speed and Brysbaert (2024).\n",
        "# 1 point for correctly applied model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "JVcuHS02UUPd"
      },
      "outputs": [],
      "source": [
        "# compute the Spearman correlation coefficients between true valence and predicted valence for\n",
        "# - words from Speed and Brysbaert (2024)\n",
        "# - pseudowords from Gatti and colleagues (2024)\n",
        "# show the correlation coefficient\n",
        "# 1 point for the correct Spearman correlation coefficients (rounded to the third decimal place)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfWX1QTfB172"
      },
      "source": [
        "**Task 4** (*16 points available, 4 for each question*)\n",
        "\n",
        "Answer the following questions.\n",
        "\n",
        "**4a.** Describe the performance of each featurization, comparing\n",
        "- the performance of a same model between the training and test set\n",
        "- the performance of different models on the training set\n",
        "- the performance of different models on the test set\n",
        "\n",
        "(*4 points available, max 150 words*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EONmoGe8CAyI"
      },
      "source": [
        "*type your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpkgvOqeB6jH"
      },
      "source": [
        "**4b.** Compare the correlations you found when training uni-gram, bi-gram, and fastText models on Dutch words and the correlations of similar models trained on English data as reported by Gatti and colleagues; summarize the most important similarities and differences.\n",
        "\n",
        "(*4 points available, max 150 words*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv7P2zvnCBiX"
      },
      "source": [
        "*type your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ2SxrYHy3Hx"
      },
      "source": [
        "**4c.** Do you think the performance of the fastText featurization would change if you were to use different n-grams? Would you make them smaller or larger? Justify your answer.\n",
        "\n",
        "(*4 points available, max 150 words*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M-lvw2qVjNH"
      },
      "source": [
        "*type your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_-zN3Vp2OBD"
      },
      "source": [
        "**4d.** Do you think that training the same models on uni-grams, bi-grams, fastText and transformer-based embeddings but using valence ratings for Finnish (a language which uses the same alphabet as English but is not a IndoEuropean language) words would yield a similar pattern of results? Justify your answer.\n",
        "\n",
        "(*4 points available, max 150 words*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20T-4kCdVppE"
      },
      "source": [
        "*type your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4ILTPziXptK"
      },
      "source": [
        "**Task 5** (*3 points available*)\n",
        "\n",
        "Compute the average Levenshtein Distance (aLD) between each pseudoword and the 20 words at the smallest edit distance from it. Consider the set of words you used to filter out pseudowords that happen to be valid Dutch words (the file is available in this OSF repository: https://osf.io/9zymw/) to retrieve the 20 words at the smallest edit distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OGks7N-JCjFu"
      },
      "outputs": [],
      "source": [
        "# compute the average Levenshtein distance from each pseudoword to the words used to filter out pseudowords.\n",
        "# Show the aLD estimate for the pseudowords 'nedukes', 'pewbin', and 'vibcines'\n",
        "# 3 points for correctly computing aLD for pseudowords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBdwMhHsYY0j"
      },
      "source": [
        "**Task 6** (*3 points available*)\n",
        "\n",
        "For each pseudoword, record the number of tokens in which RobBERT v2 encodes it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": true,
        "id": "FDOechQfmvqE"
      },
      "outputs": [],
      "source": [
        "# record the number of tokens in which RobBERT divides each pseudoword\n",
        "# show the number of tokens for the pseudowords 'yuxwas', 'skibfy', and 'errords'\n",
        "# 3 points for correctly mapping pseudowords to number of tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9LxipdMYqXN"
      },
      "source": [
        "**Task 7** (*5 points available, see breakdown below*)\n",
        "\n",
        "Compute the residuals of the predicted valence under the four regressors trained and applied in tasks 2 to 4. Then, correlate the residuals from all four models with aLD. Finally, correlate the residuals from the RobBERT v2 model with the number of tokens in which each pseudoword is split. Use the Pearson's correlation coefficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dc2p7UXSCi-q"
      },
      "outputs": [],
      "source": [
        "# compute the residuals from all four regression models fitted before\n",
        "# 1 point available for correctly computing residuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KkqJLI17C0Ml"
      },
      "outputs": [],
      "source": [
        "# compute the Pearson's correlation between residuals and average LD for all models,\n",
        "# as well as the correlation between RobBERT v2 residuals and the number of tokens in which each pseudoword\n",
        "#    is encoded by the RobBERT v2 model.\n",
        "# show all correlation coefficients\n",
        "# 4 points for the correct correlation coefficients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6owroLfAC4vf"
      },
      "source": [
        "**Task 8** What is the relation between the errors each model made and aLD? what about the number of tokens (limited to the RobBERT v2 model)?\n",
        "\n",
        "(*4 points available, max 150 words*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvaOAjqxuHgm"
      },
      "source": [
        "*testo in corsivo*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
