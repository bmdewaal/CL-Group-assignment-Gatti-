{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcbFlt0uJpNc"
   },
   "source": [
    "In this assignment you will be asked to extend the work by Gatti et al by checking whether form-meaning mappings learned on a different yet related language to that considered in the original study still capture the perceived valence of pseudowords. To do this you will be asked to engage with several different resources and adapt the pipeline following the instructions. Along the way, you will be asked to answer a few questions.\n",
    "\n",
    "You need to submit the complete notebook in .ipynb format, with intermediate outputs visible. The notebook should be named as follows:\n",
    "\n",
    "CL2025_groupN_assignment.ipynb\n",
    "\n",
    "where N is the group number. Submissions in the wrong format or with names not adhering to the guidelines will not be evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rkN7t4rqacE"
   },
   "source": [
    "Indicate group members' names, student numbers, and contributions below:\n",
    "- 1.\n",
    "- 2.\n",
    "- 3.\n",
    "- 4.\n",
    "- 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\polis\\Downloads\\CL 2nd time\\Assignment\\data\\psycho-embeddings-main\n",
      "Processing c:\\users\\polis\\downloads\\cl 2nd time\\assignment\\data\\psycho-embeddings-main\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: transformers>=4.30.2 in c:\\anaconda\\lib\\site-packages (from psycho-embeddings==0.1.0) (4.30.2)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in c:\\anaconda\\lib\\site-packages (from psycho-embeddings==0.1.0) (0.2.0)\n",
      "Requirement already satisfied: tqdm>=4.66.4 in c:\\anaconda\\lib\\site-packages (from psycho-embeddings==0.1.0) (4.66.4)\n",
      "Requirement already satisfied: datasets>=2.19.1 in c:\\anaconda\\lib\\site-packages (from psycho-embeddings==0.1.0) (3.1.0)\n",
      "Requirement already satisfied: torch in c:\\anaconda\\lib\\site-packages (from psycho-embeddings==0.1.0) (2.2.1+cpu)\n",
      "Requirement already satisfied: filelock in c:\\anaconda\\lib\\site-packages (from datasets>=2.19.1->psycho-embeddings==0.1.0) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\anaconda\\lib\\site-packages (from datasets>=2.19.1->psycho-embeddings==0.1.0) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\anaconda\\lib\\site-packages (from datasets>=2.19.1->psycho-embeddings==0.1.0) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\anaconda\\lib\\site-packages (from datasets>=2.19.1->psycho-embeddings==0.1.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\anaconda\\lib\\site-packages (from datasets>=2.19.1->psycho-embeddings==0.1.0) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\anaconda\\lib\\site-packages (from datasets>=2.19.1->psycho-embeddings==0.1.0) (2.32.2)\n",
      "Requirement already satisfied: xxhash in c:\\anaconda\\lib\\site-packages (from datasets>=2.19.1->psycho-embeddings==0.1.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\anaconda\\lib\\site-packages (from datasets>=2.19.1->psycho-embeddings==0.1.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\anaconda\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.19.1->psycho-embeddings==0.1.0) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\anaconda\\lib\\site-packages (from datasets>=2.19.1->psycho-embeddings==0.1.0) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\anaconda\\lib\\site-packages (from datasets>=2.19.1->psycho-embeddings==0.1.0) (0.30.1)\n",
      "Requirement already satisfied: packaging in c:\\anaconda\\lib\\site-packages (from datasets>=2.19.1->psycho-embeddings==0.1.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\anaconda\\lib\\site-packages (from datasets>=2.19.1->psycho-embeddings==0.1.0) (6.0.1)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from tqdm>=4.66.4->psycho-embeddings==0.1.0) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\anaconda\\lib\\site-packages (from transformers>=4.30.2->psycho-embeddings==0.1.0) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\anaconda\\lib\\site-packages (from transformers>=4.30.2->psycho-embeddings==0.1.0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\anaconda\\lib\\site-packages (from transformers>=4.30.2->psycho-embeddings==0.1.0) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\anaconda\\lib\\site-packages (from torch->psycho-embeddings==0.1.0) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\anaconda\\lib\\site-packages (from torch->psycho-embeddings==0.1.0) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\anaconda\\lib\\site-packages (from torch->psycho-embeddings==0.1.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda\\lib\\site-packages (from torch->psycho-embeddings==0.1.0) (3.1.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\anaconda\\lib\\site-packages (from aiohttp->datasets>=2.19.1->psycho-embeddings==0.1.0) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\anaconda\\lib\\site-packages (from aiohttp->datasets>=2.19.1->psycho-embeddings==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\anaconda\\lib\\site-packages (from aiohttp->datasets>=2.19.1->psycho-embeddings==0.1.0) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\anaconda\\lib\\site-packages (from aiohttp->datasets>=2.19.1->psycho-embeddings==0.1.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\anaconda\\lib\\site-packages (from aiohttp->datasets>=2.19.1->psycho-embeddings==0.1.0) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\anaconda\\lib\\site-packages (from aiohttp->datasets>=2.19.1->psycho-embeddings==0.1.0) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\anaconda\\lib\\site-packages (from aiohttp->datasets>=2.19.1->psycho-embeddings==0.1.0) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets>=2.19.1->psycho-embeddings==0.1.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets>=2.19.1->psycho-embeddings==0.1.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets>=2.19.1->psycho-embeddings==0.1.0) (1.26.17)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets>=2.19.1->psycho-embeddings==0.1.0) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda\\lib\\site-packages (from jinja2->torch->psycho-embeddings==0.1.0) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\anaconda\\lib\\site-packages (from pandas->datasets>=2.19.1->psycho-embeddings==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda\\lib\\site-packages (from pandas->datasets>=2.19.1->psycho-embeddings==0.1.0) (2023.3.post1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\anaconda\\lib\\site-packages (from sympy->torch->psycho-embeddings==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets>=2.19.1->psycho-embeddings==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\anaconda\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.19.1->psycho-embeddings==0.1.0) (0.2.0)\n",
      "Building wheels for collected packages: psycho-embeddings\n",
      "  Building wheel for psycho-embeddings (setup.py): started\n",
      "  Building wheel for psycho-embeddings (setup.py): finished with status 'done'\n",
      "  Created wheel for psycho-embeddings: filename=psycho_embeddings-0.1.0-py2.py3-none-any.whl size=6885 sha256=70714eae702ff53d0c105c57659ed1fc635ae06e67852f0c824b83d78b350e4d\n",
      "  Stored in directory: c:\\users\\polis\\appdata\\local\\pip\\cache\\wheels\\5b\\d0\\af\\6b84f61f5bbad06da20f6ef78d9a0d12fb7c143748e3b9d969\n",
      "Successfully built psycho-embeddings\n",
      "Installing collected packages: psycho-embeddings\n",
      "  Attempting uninstall: psycho-embeddings\n",
      "    Found existing installation: psycho-embeddings 0.1.0\n",
      "    Uninstalling psycho-embeddings-0.1.0:\n",
      "      Successfully uninstalled psycho-embeddings-0.1.0\n",
      "Successfully installed psycho-embeddings-0.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\anaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/MilaNLProc/psycho-embeddings.git\n",
    "\n",
    "#instead: \n",
    "\n",
    "%cd \"C:\\Users\\polis\\Downloads\\CL 2nd time\\Assignment\\data\\psycho-embeddings-main\"\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQbS5Urfit8W",
    "outputId": "77ad93d2-be24-4393-94fe-f29b5288464d"
   },
   "outputs": [],
   "source": [
    "# the code has been tested using the psycho-embeddings library to extract representations from LLMs. You can also use other libraries,\n",
    "# as long as you make sure that you are producing the correct output.\n",
    "#!git clone https://github.com/MilaNLProc/psycho-embeddings.git\n",
    "# %cd psycho-embeddings\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\polis\\Downloads\\CL 2nd time\\Assignment\n"
     ]
    }
   ],
   "source": [
    "#my cell\n",
    "%cd \"C:/Users/polis/Downloads/CL 2nd time/Assignment\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install numpy pandas tqdm transformers\n",
    "# !pip install fasttext-wheel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fasttext\n",
    "# model = fasttext.load_model(\"/Users/polis/Downloads/CL 2nd time/Assignment.bin\")  # Test with a real path if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "# print(torch.__version__)\n",
    "# print(torch.cuda.is_available())  # should return False for CPU-only version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"C:/Users/polis/Downloads/CL 2nd time/Assignment/psycho-embeddings\")\n",
    "\n",
    "# from psycho_embeddings import ContextualizedEmbedder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xformers\n",
      "  Downloading xformers-0.0.28.post1.tar.gz (7.8 MB)\n",
      "     ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.1/7.8 MB 4.3 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.9/7.8 MB 11.8 MB/s eta 0:00:01\n",
      "     ----- ---------------------------------- 1.0/7.8 MB 11.0 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 2.1/7.8 MB 12.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 2.1/7.8 MB 12.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 2.1/7.8 MB 12.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 2.1/7.8 MB 12.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 2.1/7.8 MB 12.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 2.1/7.8 MB 12.1 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 2.2/7.8 MB 4.7 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 3.1/7.8 MB 6.5 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 3.3/7.8 MB 5.7 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 4.2/7.8 MB 7.2 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 4.2/7.8 MB 7.2 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 4.2/7.8 MB 7.2 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 4.2/7.8 MB 7.2 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 4.2/7.8 MB 7.2 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 4.2/7.8 MB 7.2 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 4.2/7.8 MB 7.2 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 4.2/7.8 MB 7.2 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 4.2/7.8 MB 7.2 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 4.2/7.8 MB 7.2 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 4.2/7.8 MB 7.2 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 4.2/7.8 MB 7.2 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 4.2/7.8 MB 7.2 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 5.2/7.8 MB 4.2 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 5.2/7.8 MB 4.2 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 6.1/7.8 MB 4.6 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 6.3/7.8 MB 4.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 7.2/7.8 MB 5.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.8/7.8 MB 5.3 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\anaconda\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\polis\\\\AppData\\\\Local\\\\Temp\\\\pip-install-qcn5imsz\\\\xformers_a982ad0647564f5db89ed53b1a46f883\\\\third_party/flash-attention/csrc/composable_kernel/client_example/24_grouped_conv_activation/grouped_convnd_bwd_data_bilinear/grouped_conv_bwd_data_bilinear_residual_fp16.cpp'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install xformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.51.1-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\python3\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python3\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python3\\lib\\site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\python3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\python3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python3\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python3\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python3\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Downloading transformers-4.51.1-py3-none-any.whl (10.4 MB)\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 2.9/10.4 MB 18.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.6/10.4 MB 19.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.4/10.4 MB 18.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Using cached regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 15.4 MB/s eta 0:00:00\n",
      "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Installing collected packages: safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed fsspec-2025.3.2 huggingface-hub-0.30.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement psycho_embeddings (from versions: none)\n",
      "ERROR: No matching distribution found for psycho_embeddings\n"
     ]
    }
   ],
   "source": [
    "#!pip install psycho_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofb0L_c0AW0W",
    "outputId": "ec46754a-55a9-481c-8725-92c5dbae665a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the solution to the assignment has been obtained using these packages.\n",
    "# you're free to use other packages though: consider this as an indication, not a prescription.\n",
    "#import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fasttext as ft\n",
    "import pickle as pkl\n",
    "import fasttext.util\n",
    "#from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "#from psycho_embeddings import ContextualizedEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61Mo4yyLXnt6",
    "outputId": "cb4ae5ff-a8ed-41f0-a70f-7db5d81fc457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\anaconda\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\anaconda\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\anaconda\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\anaconda\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\anaconda\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\anaconda\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\anaconda\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\anaconda\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\anaconda\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#%pip install nltk #installed\n",
    "#%pip install fasttext #installed\n",
    "#%pip install psycho_embeddings #installed\n",
    "\n",
    "# Needed to import Rdata file\n",
    "#%pip install pyreadr installed from anaconda prompt\n",
    "\n",
    "# Needed to read Brysbaert valence excel sheet\n",
    "#%pip install openpyxl #installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fasttext as ft\n",
    "import pickle as pkl\n",
    "import fasttext.util\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "#from psycho_embeddings import ContextualizedEmbedder\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ra3phcRhKuuo"
   },
   "source": [
    "**Task 1** (*10 points available, see breakdown per task below*)\n",
    "\n",
    "You should replicate the main design in the paper *Valence without meaning* by Gatti and colleagues (2024), using estimates collected for Dutch word valence to train linear regression models and apply them to predict the valence of English pseudowords from Gatti and colleagues.\n",
    "\n",
    "In detail, to train your regression models, you should use the dataset by Speed and Brysbaert (2024) containing crowd-sourced valence ratings (use the metadata to identify the relevant columns) collected for approximately 24,000 Dutch words. See the paper *Ratings of valence, arousal, happiness, anger, fear, sadness, disgust, and surprise for 24,000 Dutch words* by Speed and Brysbaert (2024).\n",
    "\n",
    "You should train a letter unigram model and a bigram model. Each model should be trained on Dutch words only.\n",
    "\n",
    "Pay attention to one issue though: pseudowords created for English may be valid words in Dutch: therefore, you should first filter the list of pseudowords against a large store of Dutch words. To do so, use the words in the Dutch prevalence lexicon available in this OSF repository: https://osf.io/9zymw/. Essentially, you need to exclude any pseudoword that happens to be a word for which a prevalence estimate is available, whatever the prevalence is.\n",
    "\n",
    "Each code block indicates how many points are available and how they are attributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Hrd4EhHlAcmi"
   },
   "outputs": [],
   "source": [
    "# read in the pseudowords from Gatti and colleagues, as well as the valence ratings for 24,000 Dutch words from Speed and Brysbaert (2024)\n",
    "# show the first 5 lines of each dataset.\n",
    "# 1 point for identifying the correct files and correctly loading their content\n",
    "\n",
    "# read in the pseudowords from Gatti and colleagues,\n",
    "# as well as the valence ratings for 24,000 Dutch words from Speed and Brysbaert (2024)\n",
    "# show the first 5 lines of each dataset.\n",
    "# 1 point for identifying the correct files and correctly loading their content\n",
    "\n",
    "# import pyreadr\n",
    "# import pandas as pd\n",
    "\n",
    "# Using pyreadr to import the Rdata gatti dataset\n",
    "#gatti_result = pyreadr.read_r(\"/Users/bramdewaal/Desktop/Uni/VSC/CL/Group Assignment/data/data_pseudovalence.Rdata\")\n",
    "\n",
    "# gatti_df = list(gatti_result.values())[0]\n",
    "\n",
    "# print(\"Gatti et al. pseudoword valence dataset:\")\n",
    "# print(gatti_df.head())\n",
    "\n",
    "\n",
    "# # Importing Speed & Brysbaert dataset\n",
    "# speed_df = pd.read_csv(\"/Users/polis/Downloads/CL 2nd time/Assignment/data/prevalence_netherlands.csv\", sep=\"\\t\")\n",
    "\n",
    "\n",
    "# print(\"\\nSpeed & Brysbaert Dutch valence dataset:\")\n",
    "# print(speed_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polislava\n",
    "\n",
    "# read in the pseudowords from Gatti and colleagues, as well as the valence ratings for 24,000 Dutch words from Speed and Brysbaert (2024)\n",
    "# show the first 5 lines of each dataset.\n",
    "# 1 point for identifying the correct files and correctly loading their content\n",
    "valence_df = pd.read_excel(\"All_Valence.xlsx\")\n",
    "prevalence_df = pd.read_csv(\"prevalence_netherlands.csv\", sep=\"\\t\")\n",
    "gatti_df = pd.read_csv(\"gatti_new_converted_data_pseudovalenceRData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polislava\n",
    "gatti_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rownames               0\n",
       "Valence                0\n",
       "predicted_val          0\n",
       "predicted_valL         0\n",
       "predicted_valL_BI      0\n",
       "predicted_valDIM       0\n",
       "predicted_valL_DIM     0\n",
       "predicted_valBI        0\n",
       "predicted_valBI_DIM    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Polislava\n",
    "gatti_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Valence</th>\n",
       "      <th>N_Unknown</th>\n",
       "      <th>N_Valence</th>\n",
       "      <th>ProportionUnknown</th>\n",
       "      <th>RemoveUnknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>concordantie</td>\n",
       "      <td>3.222222</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nepotisme</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prefectuur</td>\n",
       "      <td>3.222222</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prevaleren</td>\n",
       "      <td>3.444444</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>affiliatie</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24032</th>\n",
       "      <td>zwijgzaam</td>\n",
       "      <td>2.631579</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24033</th>\n",
       "      <td>zwijn</td>\n",
       "      <td>2.210526</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24034</th>\n",
       "      <td>zwijnenstal</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24035</th>\n",
       "      <td>zwoegen</td>\n",
       "      <td>2.263158</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24036</th>\n",
       "      <td>zwoel</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24037 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Word   Valence  N_Unknown  N_Valence  ProportionUnknown  \\\n",
       "0      concordantie  3.222222         11          9           0.550000   \n",
       "1         nepotisme  2.111111         10          9           0.526316   \n",
       "2        prefectuur  3.222222         10          9           0.526316   \n",
       "3        prevaleren  3.444444         10          9           0.526316   \n",
       "4        affiliatie  3.100000          9         10           0.473684   \n",
       "...             ...       ...        ...        ...                ...   \n",
       "24032     zwijgzaam  2.631579          0         19           0.000000   \n",
       "24033         zwijn  2.210526          0         19           0.000000   \n",
       "24034   zwijnenstal  1.800000          0         20           0.000000   \n",
       "24035       zwoegen  2.263158          0         19           0.000000   \n",
       "24036         zwoel  3.684211          0         19           0.000000   \n",
       "\n",
       "       RemoveUnknown  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  1  \n",
       "3                  1  \n",
       "4                  0  \n",
       "...              ...  \n",
       "24032              0  \n",
       "24033              0  \n",
       "24034              0  \n",
       "24035              0  \n",
       "24036              0  \n",
       "\n",
       "[24037 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Polislava\n",
    "valence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>n.obs</th>\n",
       "      <th>irt.prevalence</th>\n",
       "      <th>z.irt.prevalence</th>\n",
       "      <th>prevalence</th>\n",
       "      <th>z.prevalence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T-shirt</td>\n",
       "      <td>324</td>\n",
       "      <td>0.986622</td>\n",
       "      <td>2.215053</td>\n",
       "      <td>0.978395</td>\n",
       "      <td>1.689888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aagje</td>\n",
       "      <td>303</td>\n",
       "      <td>0.907405</td>\n",
       "      <td>1.324941</td>\n",
       "      <td>0.877888</td>\n",
       "      <td>1.075808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aagt</td>\n",
       "      <td>324</td>\n",
       "      <td>0.169817</td>\n",
       "      <td>-0.954888</td>\n",
       "      <td>0.188272</td>\n",
       "      <td>-0.827920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aai</td>\n",
       "      <td>335</td>\n",
       "      <td>0.993290</td>\n",
       "      <td>2.472451</td>\n",
       "      <td>0.988060</td>\n",
       "      <td>1.794794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaibaar</td>\n",
       "      <td>333</td>\n",
       "      <td>0.996284</td>\n",
       "      <td>2.676802</td>\n",
       "      <td>0.990991</td>\n",
       "      <td>1.830889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54314</th>\n",
       "      <td>één</td>\n",
       "      <td>319</td>\n",
       "      <td>0.996049</td>\n",
       "      <td>2.656250</td>\n",
       "      <td>0.990596</td>\n",
       "      <td>1.825880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54315</th>\n",
       "      <td>éénzijdige</td>\n",
       "      <td>58</td>\n",
       "      <td>0.953770</td>\n",
       "      <td>1.682565</td>\n",
       "      <td>0.913793</td>\n",
       "      <td>1.243203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54316</th>\n",
       "      <td>öre</td>\n",
       "      <td>357</td>\n",
       "      <td>0.307535</td>\n",
       "      <td>-0.502851</td>\n",
       "      <td>0.324930</td>\n",
       "      <td>-0.429765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54317</th>\n",
       "      <td>überhaupt</td>\n",
       "      <td>345</td>\n",
       "      <td>0.979032</td>\n",
       "      <td>2.034147</td>\n",
       "      <td>0.971014</td>\n",
       "      <td>1.620744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54318</th>\n",
       "      <td>übermensch</td>\n",
       "      <td>355</td>\n",
       "      <td>0.930570</td>\n",
       "      <td>1.480052</td>\n",
       "      <td>0.904225</td>\n",
       "      <td>1.195295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54319 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  n.obs  irt.prevalence  z.irt.prevalence  prevalence  \\\n",
       "0         T-shirt    324        0.986622          2.215053    0.978395   \n",
       "1           aagje    303        0.907405          1.324941    0.877888   \n",
       "2            aagt    324        0.169817         -0.954888    0.188272   \n",
       "3             aai    335        0.993290          2.472451    0.988060   \n",
       "4         aaibaar    333        0.996284          2.676802    0.990991   \n",
       "...           ...    ...             ...               ...         ...   \n",
       "54314         één    319        0.996049          2.656250    0.990596   \n",
       "54315  éénzijdige     58        0.953770          1.682565    0.913793   \n",
       "54316         öre    357        0.307535         -0.502851    0.324930   \n",
       "54317   überhaupt    345        0.979032          2.034147    0.971014   \n",
       "54318  übermensch    355        0.930570          1.480052    0.904225   \n",
       "\n",
       "       z.prevalence  \n",
       "0          1.689888  \n",
       "1          1.075808  \n",
       "2         -0.827920  \n",
       "3          1.794794  \n",
       "4          1.830889  \n",
       "...             ...  \n",
       "54314      1.825880  \n",
       "54315      1.243203  \n",
       "54316     -0.429765  \n",
       "54317      1.620744  \n",
       "54318      1.195295  \n",
       "\n",
       "[54319 rows x 6 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Polislava\n",
    "prevalence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rownames</th>\n",
       "      <th>Valence</th>\n",
       "      <th>predicted_val</th>\n",
       "      <th>predicted_valL</th>\n",
       "      <th>predicted_valL_BI</th>\n",
       "      <th>predicted_valDIM</th>\n",
       "      <th>predicted_valL_DIM</th>\n",
       "      <th>predicted_valBI</th>\n",
       "      <th>predicted_valBI_DIM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.392012</td>\n",
       "      <td>4.920180</td>\n",
       "      <td>6.410768</td>\n",
       "      <td>5.772722</td>\n",
       "      <td>5.774341</td>\n",
       "      <td>6.410768</td>\n",
       "      <td>6.392012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abalone</td>\n",
       "      <td>5.30</td>\n",
       "      <td>4.756492</td>\n",
       "      <td>5.284912</td>\n",
       "      <td>5.115389</td>\n",
       "      <td>4.728264</td>\n",
       "      <td>4.858120</td>\n",
       "      <td>5.115389</td>\n",
       "      <td>4.756492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>2.84</td>\n",
       "      <td>4.260055</td>\n",
       "      <td>5.001226</td>\n",
       "      <td>5.479860</td>\n",
       "      <td>3.978241</td>\n",
       "      <td>3.987623</td>\n",
       "      <td>5.479860</td>\n",
       "      <td>4.260055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>2.63</td>\n",
       "      <td>4.196807</td>\n",
       "      <td>5.022504</td>\n",
       "      <td>5.334364</td>\n",
       "      <td>3.833330</td>\n",
       "      <td>3.828077</td>\n",
       "      <td>5.334364</td>\n",
       "      <td>4.196807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abbey</td>\n",
       "      <td>5.85</td>\n",
       "      <td>6.123953</td>\n",
       "      <td>5.147159</td>\n",
       "      <td>5.162931</td>\n",
       "      <td>6.064834</td>\n",
       "      <td>6.094675</td>\n",
       "      <td>5.162931</td>\n",
       "      <td>6.123953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13782</th>\n",
       "      <td>zone</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.870006</td>\n",
       "      <td>5.407978</td>\n",
       "      <td>5.119138</td>\n",
       "      <td>5.246048</td>\n",
       "      <td>5.302681</td>\n",
       "      <td>5.119138</td>\n",
       "      <td>4.870006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13783</th>\n",
       "      <td>zoning</td>\n",
       "      <td>4.65</td>\n",
       "      <td>4.141520</td>\n",
       "      <td>5.255865</td>\n",
       "      <td>5.085299</td>\n",
       "      <td>4.494910</td>\n",
       "      <td>4.556050</td>\n",
       "      <td>5.085299</td>\n",
       "      <td>4.141520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13784</th>\n",
       "      <td>zoo</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.374944</td>\n",
       "      <td>5.396494</td>\n",
       "      <td>5.551119</td>\n",
       "      <td>7.720490</td>\n",
       "      <td>7.674317</td>\n",
       "      <td>5.551119</td>\n",
       "      <td>7.374944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13785</th>\n",
       "      <td>zoom</td>\n",
       "      <td>5.86</td>\n",
       "      <td>6.155045</td>\n",
       "      <td>5.395601</td>\n",
       "      <td>5.338779</td>\n",
       "      <td>6.479694</td>\n",
       "      <td>6.570841</td>\n",
       "      <td>5.338779</td>\n",
       "      <td>6.155045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13786</th>\n",
       "      <td>zucchini</td>\n",
       "      <td>6.30</td>\n",
       "      <td>5.864520</td>\n",
       "      <td>5.100183</td>\n",
       "      <td>5.315438</td>\n",
       "      <td>5.695355</td>\n",
       "      <td>5.727546</td>\n",
       "      <td>5.315438</td>\n",
       "      <td>5.864520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13786 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          rownames  Valence  predicted_val  predicted_valL  predicted_valL_BI  \\\n",
       "0         aardvark     6.26       6.392012        4.920180           6.410768   \n",
       "1          abalone     5.30       4.756492        5.284912           5.115389   \n",
       "2          abandon     2.84       4.260055        5.001226           5.479860   \n",
       "3      abandonment     2.63       4.196807        5.022504           5.334364   \n",
       "4            abbey     5.85       6.123953        5.147159           5.162931   \n",
       "...            ...      ...            ...             ...                ...   \n",
       "13782         zone     4.75       4.870006        5.407978           5.119138   \n",
       "13783       zoning     4.65       4.141520        5.255865           5.085299   \n",
       "13784          zoo     7.00       7.374944        5.396494           5.551119   \n",
       "13785         zoom     5.86       6.155045        5.395601           5.338779   \n",
       "13786     zucchini     6.30       5.864520        5.100183           5.315438   \n",
       "\n",
       "       predicted_valDIM  predicted_valL_DIM  predicted_valBI  \\\n",
       "0              5.772722            5.774341         6.410768   \n",
       "1              4.728264            4.858120         5.115389   \n",
       "2              3.978241            3.987623         5.479860   \n",
       "3              3.833330            3.828077         5.334364   \n",
       "4              6.064834            6.094675         5.162931   \n",
       "...                 ...                 ...              ...   \n",
       "13782          5.246048            5.302681         5.119138   \n",
       "13783          4.494910            4.556050         5.085299   \n",
       "13784          7.720490            7.674317         5.551119   \n",
       "13785          6.479694            6.570841         5.338779   \n",
       "13786          5.695355            5.727546         5.315438   \n",
       "\n",
       "       predicted_valBI_DIM  \n",
       "0                 6.392012  \n",
       "1                 4.756492  \n",
       "2                 4.260055  \n",
       "3                 4.196807  \n",
       "4                 6.123953  \n",
       "...                    ...  \n",
       "13782             4.870006  \n",
       "13783             4.141520  \n",
       "13784             7.374944  \n",
       "13785             6.155045  \n",
       "13786             5.864520  \n",
       "\n",
       "[13786 rows x 9 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Polislava\n",
    "gatti_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 lines of Dutch valence ratings:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Valence</th>\n",
       "      <th>N_Unknown</th>\n",
       "      <th>N_Valence</th>\n",
       "      <th>ProportionUnknown</th>\n",
       "      <th>RemoveUnknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>concordantie</td>\n",
       "      <td>3.222222</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nepotisme</td>\n",
       "      <td>2.111111</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>prefectuur</td>\n",
       "      <td>3.222222</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prevaleren</td>\n",
       "      <td>3.444444</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>affiliatie</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word   Valence  N_Unknown  N_Valence  ProportionUnknown  \\\n",
       "0  concordantie  3.222222         11          9           0.550000   \n",
       "1     nepotisme  2.111111         10          9           0.526316   \n",
       "2    prefectuur  3.222222         10          9           0.526316   \n",
       "3    prevaleren  3.444444         10          9           0.526316   \n",
       "4    affiliatie  3.100000          9         10           0.473684   \n",
       "\n",
       "   RemoveUnknown  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Polislava\n",
    "print(\"First 5 lines of Dutch valence ratings:\")\n",
    "valence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 lines of Dutch prevalence lexicon:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>n.obs</th>\n",
       "      <th>irt.prevalence</th>\n",
       "      <th>z.irt.prevalence</th>\n",
       "      <th>prevalence</th>\n",
       "      <th>z.prevalence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T-shirt</td>\n",
       "      <td>324</td>\n",
       "      <td>0.986622</td>\n",
       "      <td>2.215053</td>\n",
       "      <td>0.978395</td>\n",
       "      <td>1.689888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aagje</td>\n",
       "      <td>303</td>\n",
       "      <td>0.907405</td>\n",
       "      <td>1.324941</td>\n",
       "      <td>0.877888</td>\n",
       "      <td>1.075808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aagt</td>\n",
       "      <td>324</td>\n",
       "      <td>0.169817</td>\n",
       "      <td>-0.954888</td>\n",
       "      <td>0.188272</td>\n",
       "      <td>-0.827920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aai</td>\n",
       "      <td>335</td>\n",
       "      <td>0.993290</td>\n",
       "      <td>2.472451</td>\n",
       "      <td>0.988060</td>\n",
       "      <td>1.794794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaibaar</td>\n",
       "      <td>333</td>\n",
       "      <td>0.996284</td>\n",
       "      <td>2.676802</td>\n",
       "      <td>0.990991</td>\n",
       "      <td>1.830889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  n.obs  irt.prevalence  z.irt.prevalence  prevalence  z.prevalence\n",
       "0  T-shirt    324        0.986622          2.215053    0.978395      1.689888\n",
       "1    aagje    303        0.907405          1.324941    0.877888      1.075808\n",
       "2     aagt    324        0.169817         -0.954888    0.188272     -0.827920\n",
       "3      aai    335        0.993290          2.472451    0.988060      1.794794\n",
       "4  aaibaar    333        0.996284          2.676802    0.990991      1.830889"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Polislava\n",
    "print(\"\\nFirst 5 lines of Dutch prevalence lexicon:\")\n",
    "prevalence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 lines of Gatti pseudowords:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rownames</th>\n",
       "      <th>Valence</th>\n",
       "      <th>predicted_val</th>\n",
       "      <th>predicted_valL</th>\n",
       "      <th>predicted_valL_BI</th>\n",
       "      <th>predicted_valDIM</th>\n",
       "      <th>predicted_valL_DIM</th>\n",
       "      <th>predicted_valBI</th>\n",
       "      <th>predicted_valBI_DIM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aardvark</td>\n",
       "      <td>6.26</td>\n",
       "      <td>6.392012</td>\n",
       "      <td>4.920180</td>\n",
       "      <td>6.410768</td>\n",
       "      <td>5.772722</td>\n",
       "      <td>5.774341</td>\n",
       "      <td>6.410768</td>\n",
       "      <td>6.392012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abalone</td>\n",
       "      <td>5.30</td>\n",
       "      <td>4.756492</td>\n",
       "      <td>5.284912</td>\n",
       "      <td>5.115389</td>\n",
       "      <td>4.728264</td>\n",
       "      <td>4.858120</td>\n",
       "      <td>5.115389</td>\n",
       "      <td>4.756492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandon</td>\n",
       "      <td>2.84</td>\n",
       "      <td>4.260055</td>\n",
       "      <td>5.001226</td>\n",
       "      <td>5.479860</td>\n",
       "      <td>3.978241</td>\n",
       "      <td>3.987623</td>\n",
       "      <td>5.479860</td>\n",
       "      <td>4.260055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>2.63</td>\n",
       "      <td>4.196807</td>\n",
       "      <td>5.022504</td>\n",
       "      <td>5.334364</td>\n",
       "      <td>3.833330</td>\n",
       "      <td>3.828077</td>\n",
       "      <td>5.334364</td>\n",
       "      <td>4.196807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abbey</td>\n",
       "      <td>5.85</td>\n",
       "      <td>6.123953</td>\n",
       "      <td>5.147159</td>\n",
       "      <td>5.162931</td>\n",
       "      <td>6.064834</td>\n",
       "      <td>6.094675</td>\n",
       "      <td>5.162931</td>\n",
       "      <td>6.123953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rownames  Valence  predicted_val  predicted_valL  predicted_valL_BI  \\\n",
       "0     aardvark     6.26       6.392012        4.920180           6.410768   \n",
       "1      abalone     5.30       4.756492        5.284912           5.115389   \n",
       "2      abandon     2.84       4.260055        5.001226           5.479860   \n",
       "3  abandonment     2.63       4.196807        5.022504           5.334364   \n",
       "4        abbey     5.85       6.123953        5.147159           5.162931   \n",
       "\n",
       "   predicted_valDIM  predicted_valL_DIM  predicted_valBI  predicted_valBI_DIM  \n",
       "0          5.772722            5.774341         6.410768             6.392012  \n",
       "1          4.728264            4.858120         5.115389             4.756492  \n",
       "2          3.978241            3.987623         5.479860             4.260055  \n",
       "3          3.833330            3.828077         5.334364             4.196807  \n",
       "4          6.064834            6.094675         5.162931             6.123953  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Polislava\n",
    "print(\"\\nFirst 5 lines of Gatti pseudowords:\")\n",
    "gatti_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "As4XV-LQPbyH"
   },
   "outputs": [],
   "source": [
    "# filter out pseudowords that happen to be valid Dutch words (mind case folding!)\n",
    "# show the set of pseudowords filtered out.\n",
    "# 1 point for applying the correct filtering\n",
    "# filter out pseudowords that happen to be valid Dutch words (mind case folding!)\n",
    "# show the set of pseudowords filtered out.\n",
    "# 1 point for applying the correct filtering\n",
    "\n",
    "\n",
    "# # Gatti pseudowords (row names)\n",
    "# gatti_words = gatti_df.index.str.lower()\n",
    "\n",
    "# # Dutch real words from Speed & Brysbaert prevalence lexicon\n",
    "# dutch_words = set(speed_df['word'].str.lower())\n",
    "\n",
    "# # Converting to set & filtering out overlapping words (pseudowords that are valid Dutch words)\n",
    "# filtered_out = sorted(set(gatti_words).intersection(dutch_words))\n",
    "\n",
    "# # Filtering Gatti pseudowords that are in the real Dutch words set\n",
    "# gatti_filtered_df = gatti_df[~gatti_df.index.str.lower().isin(dutch_words)]\n",
    "\n",
    "\n",
    "# print(\"Pseudowords that were filtered out:\")\n",
    "# print(filtered_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polislava\n",
    "\n",
    "valence_df['Word'] = valence_df['Word'].str.lower()\n",
    "prevalence_df['word'] = prevalence_df['word'].str.lower()\n",
    "gatti_df['rownames'] = gatti_df['rownames'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polislava\n",
    "dutch_word_set = set(prevalence_df['word'])\n",
    "filtered_out = set(gatti_df['rownames']) & dutch_word_set\n",
    "gatti_filtered = gatti_df[~gatti_df['rownames'].isin(dutch_word_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pseudowords filtered out (found in Dutch):\n",
      "{'ninja', 'fondue', 'veer', 'melodrama', 'context', 'ferry', 'fielder', 'pedicure', 'commodore', 'ultimatum', 'minibus', 'hostel', 'nonchalant', 'tuba', 'spirit', 'ecstasy', 'pasta', 'nimrod', 'keel', 'bladder', 'impotent', 'snack', 'tarantula', 'trust', 'specimen', 'stadium', 'blond', 'loser', 'draw', 'serenade', 'concerto', 'oregano', 'milligram', 'tackle', 'privilege', 'honk', 'bulk', 'goodwill', 'cocktail', 'nitwit', 'beef', 'hotdog', 'briefing', 'valve', 'maximum', 'flat', 'knot', 'manicure', 'blackjack', 'agent', 'communist', 'tweet', 'cruise', 'indecent', 'protest', 'trash', 'plutonium', 'dodo', 'baton', 'anaconda', 'look', 'spar', 'expert', 'marketing', 'beginner', 'lab', 'pair', 'tune', 'string', 'turbulent', 'armada', 'ramp', 'die', 'kebab', 'run', 'convertible', 'mezzanine', 'mozzarella', 'appendicitis', 'varsity', 'psoriasis', 'input', 'medium', 'homo', 'mailbox', 'flan', 'hardware', 'alumnus', 'roman', 'county', 'detector', 'grip', 'decadent', 'equalizer', 'broker', 'important', 'prestige', 'allure', 'recital', 'feature', 'tabasco', 'compact', 'switch', 'hark', 'lord', 'supervisor', 'affidavit', 'extravagant', 'garage', 'editor', 'punt', 'plating', 'lamp', 'display', 'boon', 'babysitter', 'pier', 'segment', 'polio', 'reflex', 'pest', 'crawl', 'motto', 'diffuse', 'dragon', 'student', 'fauna', 'bumper', 'racist', 'popcorn', 'gig', 'synthesizer', 'minimum', 'promo', 'bat', 'gum', 'parking', 'soul', 'enamel', 'soda', 'ballerina', 'acre', 'moustache', 'depot', 'lotion', 'specialist', 'boogie', 'steps', 'egghead', 'bulldozer', 'ruin', 'tenant', 'sheet', 'cookie', 'drummer', 'research', 'trend', 'voyeur', 'uterus', 'delta', 'amateur', 'camouflage', 'jacket', 'sabbatical', 'perk', 'turquoise', 'essay', 'grapefruit', 'rob', 'keen', 'headset', 'sandwich', 'engagement', 'query', 'jet', 'primer', 'feedback', 'slop', 'semester', 'chaos', 'chloroform', 'tap', 'fashionable', 'sadist', 'drainage', 'website', 'happen', 'compliment', 'competent', 'deficit', 'peyote', 'parasol', 'flamboyant', 'genie', 'consent', 'trucker', 'generator', 'royalty', 'biceps', 'cache', 'pee', 'panel', 'predator', 'hologram', 'ambrosia', 'job', 'filet', 'scrub', 'nucleus', 'flashy', 'magazine', 'enter', 'chef', 'poltergeist', 'dizzy', 'locker', 'caddie', 'supporter', 'fellow', 'beryllium', 'asterisk', 'blaster', 'film', 'reservoir', 'cartoon', 'milkshake', 'trail', 'dank', 'band', 'flap', 'glossy', 'intake', 'condor', 'exodus', 'blow', 'conditioner', 'optimist', 'mono', 'concubine', 'thermos', 'bondsman', 'planner', 'runner', 'influenza', 'scalp', 'holster', 'rook', 'emerald', 'mascara', 'show', 'computer', 'sneer', 'indicator', 'prompt', 'have', 'bracelet', 'shop', 'pocket', 'dentist', 'tremor', 'tempo', 'refuge', 'circus', 'roe', 'urn', 'scepter', 'nanny', 'ketchup', 'menu', 'underground', 'pap', 'sergeant', 'modern', 'temperament', 'joint', 'gondola', 'penny', 'toe', 'album', 'babe', 'agenda', 'totem', 'future', 'alligator', 'accident', 'box', 'nuance', 'origami', 'bookmaker', 'supermodel', 'titanium', 'cameo', 'deadline', 'regime', 'militant', 'kook', 'boost', 'croquet', 'experiment', 'intro', 'tennis', 'excrement', 'trachea', 'farmer', 'celluloid', 'heat', 'gravel', 'accept', 'cool', 'shutter', 'fat', 'chopper', 'beauty', 'undercover', 'alfalfa', 'abdomen', 'dingo', 'lease', 'ammonia', 'ram', 'minister', 'hope', 'jeans', 'gang', 'consul', 'west', 'transistor', 'minivan', 'sorry', 'sinister', 'harden', 'pendant', 'clip', 'sacrament', 'god', 'maker', 'apex', 'pass', 'consult', 'catamaran', 'musket', 'cameraman', 'rabbi', 'guillotine', 'fax', 'reek', 'plant', 'charisma', 'blink', 'clean', 'syllabus', 'racer', 'realist', 'open', 'plaque', 'password', 'actor', 'crank', 'clinic', 'continent', 'yank', 'pop', 'quad', 'bazooka', 'quilt', 'contract', 'close', 'slurp', 'taco', 'array', 'warrant', 'penthouse', 'raid', 'tube', 'exact', 'prune', 'imminent', 'proper', 'rag', 'mixer', 'affront', 'rank', 'rumble', 'gist', 'pit', 'placement', 'loot', 'urine', 'kitten', 'league', 'crazy', 'neutron', 'masseuse', 'incoherent', 'view', 'slab', 'tramp', 'deodorant', 'insect', 'tent', 'monopoly', 'attitude', 'buffer', 'scooter', 'race', 'traverse', 'advocate', 'door', 'tetanus', 'statement', 'back', 'hovercraft', 'lady', 'club', 'voltage', 'lessen', 'repel', 'decoder', 'countdown', 'kinky', 'ten', 'terminal', 'feeder', 'jazz', 'acne', 'zebra', 'hype', 'form', 'mall', 'monument', 'rest', 'curriculum', 'vermouth', 'intolerant', 'atrium', 'patience', 'mop', 'cross', 'slobber', 'alert', 'strand', 'hickory', 'bidder', 'velvet', 'feces', 'mainstream', 'hostess', 'straddle', 'update', 'corrupt', 'sassafras', 'pigskin', 'roadie', 'diligence', 'tango', 'fan', 'scan', 'centerfold', 'perfect', 'sector', 'snapshot', 'ratio', 'ballast', 'playa', 'surfer', 'default', 'blind', 'blizzard', 'recycling', 'prop', 'snowboarder', 'koala', 'flux', 'sleep', 'planter', 'correct', 'skeet', 'climax', 'fusion', 'tribal', 'stool', 'montage', 'route', 'bout', 'lipstick', 'guano', 'wand', 'zit', 'paintball', 'curling', 'gourmet', 'hot', 'office', 'norm', 'outfit', 'map', 'soiree', 'okra', 'premium', 'plank', 'discount', 'slipper', 'shorts', 'cast', 'belligerent', 'cannabis', 'transfer', 'salsa', 'soundtrack', 'persistent', 'lag', 'ego', 'water', 'commodity', 'spiritual', 'shot', 'super', 'employee', 'monsieur', 'skater', 'oasis', 'ska', 'tank', 'radium', 'utility', 'sullen', 'banjo', 'opera', 'mast', 'participant', 'concern', 'gimmick', 'pylon', 'attest', 'hetero', 'chaperon', 'miss', 'insider', 'trampoline', 'mousse', 'quota', 'management', 'peptide', 'magnolia', 'robe', 'taps', 'pi', 'masochist', 'protestant', 'duet', 'rooster', 'interval', 'joker', 'reactor', 'petunia', 'contingent', 'brochure', 'pretext', 'program', 'latent', 'golf', 'souvenir', 'prelude', 'indiscreet', 'preview', 'boring', 'satire', 'ergo', 'latitude', 'bulletin', 'sister', 'senator', 'nasty', 'chipper', 'credit', 'pastor', 'kidnapper', 'detail', 'plasma', 'rapper', 'database', 'moment', 'rut', 'commune', 'rotor', 'faun', 'official', 'boots', 'cholesterol', 'unplugged', 'drama', 'pancreas', 'recent', 'bikini', 'gossip', 'output', 'pita', 'time', 'sale', 'high', 'jargon', 'tolerant', 'performance', 'addict', 'sentiment', 'keg', 'step', 'antichrist', 'entrepreneur', 'nut', 'memento', 'solvent', 'cracker', 'sap', 'auditor', 'print', 'angora', 'maestro', 'cuisine', 'info', 'crypt', 'pitcher', 'paranoia', 'trip', 'captain', 'pretzel', 'floppy', 'patch', 'iris', 'corps', 'cent', 'finesse', 'diagnose', 'outlaw', 'hernia', 'platform', 'bottleneck', 'subject', 'bullet', 'type', 'loom', 'alcohol', 'liaison', 'receiver', 'interface', 'ligament', 'professional', 'astronaut', 'square', 'vent', 'domino', 'stick', 'reader', 'conflict', 'kick', 'gut', 'sticker', 'fond', 'essence', 'anklet', 'flow', 'gazelle', 'teller', 'abstract', 'buddy', 'festival', 'mink', 'oxide', 'aorta', 'franchise', 'cheerleader', 'flirt', 'stand', 'talent', 'cheeseburger', 'turbo', 'bingo', 'abandon', 'parka', 'psalm', 'adept', 'gulp', 'imperfect', 'taupe', 'ether', 'mores', 'trapper', 'coaster', 'ballroom', 'horde', 'video', 'fit', 'paradox', 'survival', 'mentor', 'bug', 'bogey', 'bandage', 'charter', 'expertise', 'tumor', 'telegram', 'museum', 'demon', 'compressor', 'scenario', 'rebel', 'warlord', 'anus', 'puck', 'significant', 'alias', 'aquarium', 'jihad', 'wireless', 'collie', 'component', 'pancake', 'plug', 'vagina', 'vigilant', 'grit', 'rink', 'microwave', 'single', 'flank', 'begin', 'pep', 'harp', 'delirium', 'rush', 'swami', 'complement', 'meringue', 'ragtime', 'amber', 'stewardess', 'habitat', 'uniform', 'spectator', 'disk', 'mime', 'overall', 'ravioli', 'penitent', 'massage', 'alibi', 'projector', 'pantheon', 'bootleg', 'blouse', 'shit', 'effect', 'special', 'outcast', 'sweatshirt', 'frequent', 'sardine', 'torso', 'deal', 'damp', 'fun', 'school', 'sneaker', 'center', 'embryo', 'angel', 'custard', 'finalist', 'impact', 'bark', 'mix', 'cowgirl', 'gender', 'tampon', 'profiler', 'commandant', 'monarch', 'pack', 'cargo', 'ticket', 'judo', 'falafel', 'receptor', 'putter', 'impertinent', 'sober', 'camel', 'kilo', 'sketch', 'downer', 'solarium', 'trolley', 'understatement', 'vendetta', 'prefix', 'trio', 'placenta', 'alarm', 'tissue', 'trainee', 'intact', 'bus', 'pregnant', 'cutter', 'regiment', 'vest', 'tornado', 'interviewer', 'plastic', 'skateboard', 'pulsar', 'babysit', 'dupe', 'reverence', 'marine', 'building', 'tsunami', 'abracadabra', 'surprise', 'pub', 'navigator', 'talisman', 'kit', 'bridge', 'crayon', 'fluorescent', 'backgammon', 'orthodox', 'stem', 'walrus', 'dossier', 'remake', 'go', 'cabaret', 'kilometer', 'fret', 'download', 'gasoline', 'linoleum', 'propaganda', 'echo', 'decent', 'stage', 'chemo', 'thermometer', 'demo', 'lounge', 'community', 'memorandum', 'radar', 'freestyle', 'pamper', 'routine', 'franc', 'cricket', 'freak', 'cider', 'curve', 'nudist', 'president', 'humbug', 'gateway', 'vacant', 'slum', 'pantry', 'amuse', 'musical', 'spray', 'octopus', 'steak', 'cleaner', 'duel', 'coupe', 'burger', 'protocol', 'pony', 'tender', 'pull', 'dealer', 'wave', 'lager', 'memorabilia', 'glamour', 'radius', 'smarten', 'portfolio', 'latrine', 'angst', 'finale', 'border', 'groggy', 'descendant', 'saline', 'muffin', 'ban', 'instant', 'charade', 'reggae', 'boiler', 'tomahawk', 'ark', 'denim', 'multimedia', 'shock', 'techno', 'whiplash', 'acquit', 'flop', 'country', 'protector', 'bitch', 'resort', 'surplus', 'barbecue', 'century', 'broom', 'soften', 'ballet', 'catcher', 'nerd', 'speech', 'brand', 'bit', 'bestseller', 'cretin', 'outdoor', 'mobile', 'stop', 'expo', 'bank', 'list', 'hint', 'humor', 'freelance', 'panty', 'rectum', 'ginseng', 'hacker', 'desperado', 'cherub', 'stout', 'ripper', 'splinter', 'clairvoyant', 'bonus', 'upgrade', 'solo', 'migraine', 'farm', 'inning', 'succulent', 'internet', 'aspect', 'replica', 'sample', 'sofa', 'duffel', 'tourniquet', 'funk', 'pester', 'pizzeria', 'peel', 'tour', 'episode', 'voicemail', 'fly', 'lego', 'sauna', 'trendy', 'opium', 'stereo', 'assist', 'choke', 'genius', 'pauper', 'act', 'stopwatch', 'omelet', 'date', 'mailing', 'debacle', 'campus', 'bode', 'coke', 'dip', 'park', 'contact', 'karaoke', 'macho', 'revolver', 'cement', 'ranch', 'product', 'soap', 'massacre', 'cello', 'alpine', 'multiple', 'dialect', 'lust', 'upper', 'stalker', 'barracuda', 'mug', 'racket', 'pollen', 'processor', 'talk', 'butler', 'crash', 'bard', 'procedure', 'crack', 'desk', 'closet', 'imprint', 'salon', 'romance', 'eucalyptus', 'smog', 'provider', 'dam', 'stretcher', 'nylon', 'recorder', 'succubus', 'debutante', 'hole', 'roadblock', 'mate', 'cover', 'profane', 'sediment', 'canvas', 'blazer', 'consensus', 'aids', 'sitcom', 'breed', 'security', 'tip', 'prima', 'firewall', 'stamp', 'incident', 'diocese', 'blunder', 'elixir', 'hobo', 'case', 'mare', 'mango', 'master', 'triage', 'jam', 'poet', 'grill', 'casino', 'chronic', 'titan', 'showroom', 'fuchsia', 'boxer', 'basis', 'species', 'pint', 'pact', 'pastrami', 'douche', 'issue', 'tweed', 'bourbon', 'chant', 'ultraviolet', 'redundant', 'cupcake', 'funky', 'forceps', 'cordon', 'doctrine', 'meningitis', 'kennel', 'tin', 'diploma', 'matinee', 'journalist', 'alter', 'prototype', 'polo', 'sage', 'stern', 'farce', 'pump', 'poker', 'tiara', 'proxy', 'whirlpool', 'trance', 'sidekick', 'jive', 'vintage', 'promotion', 'consistent', 'horizon', 'informant', 'basement', 'penis', 'slice', 'entertainer', 'rocker', 'boom', 'port', 'carbon', 'rug', 'wagon', 'darkroom', 'tricky', 'radiant', 'lithium', 'scene', 'pacemaker', 'deuce', 'headliner', 'cadet', 'crisis', 'schilling', 'counselor', 'binder', 'script', 'fundraising', 'chic', 'accountant', 'binding', 'bullshit', 'warm', 'design', 'sax', 'indirect', 'hit', 'idealist', 'zoo', 'commentator', 'gelding', 'smooth', 'remover', 'flashback', 'wig', 'response', 'power', 'snot', 'lacrosse', 'jungle', 'columnist', 'storm', 'mannequin', 'combo', 'playboy', 'flyer', 'urgent', 'curry', 'microchip', 'strike', 'mom', 'span', 'excelsior', 'name', 'magenta', 'playback', 'performer', 'do', 'logo', 'pang', 'modest', 'pink', 'snap', 'rubber', 'futon', 'stuntman', 'code', 'rating', 'pen', 'charge', 'bunny', 'bleu', 'affect', 'roulette', 'fitness', 'cakewalk', 'ode', 'radiator', 'practical', 'auditorium', 'kamikaze', 'coach', 'stock', 'register', 'state', 'tier', 'diabetes', 'end', 'defibrillator', 'metropolis', 'blonde', 'tipsy', 'gladiator', 'ventriloquist', 'cornflakes', 'globe', 'veranda', 'donor', 'safe', 'cyanide', 'web', 'brood', 'vat', 'winch', 'allegro', 'mum', 'intelligent', 'annex', 'rancher', 'chick', 'avocado', 'shawl', 'spread', 'recovery', 'drain', 'shirt', 'spleen', 'dame', 'bagel', 'chinchilla', 'tablet', 'hairspray', 'page', 'filmmaker', 'contrast', 'incentive', 'stag', 'diagram', 'correspondent', 'viewer', 'sweetie', 'abundant', 'graffiti', 'virus', 'letter', 'meltdown', 'mutant', 'shilling', 'canon', 'factor', 'postman', 'chalet', 'camping', 'module', 'laptop', 'hooligan', 'composer', 'bimbo', 'anagram', 'bed', 'extract', 'cyberspace', 'prior', 'genocide', 'impediment', 'gym', 'stuff', 'trimester', 'karate', 'district', 'speedway', 'auto', 'blog', 'counter', 'scorer', 'discreet', 'content', 'sigma', 'fjord', 'audio', 'puppy', 'beige', 'meter', 'courage', 'fiber', 'shovel', 'shuttle', 'exit', 'jogger', 'voodoo', 'grief', 'shotgun', 'hotline', 'decimeter', 'belt', 'boa', 'party', 'plaid', 'pandemonium', 'dollar', 'foxtrot', 'blunt', 'cash', 'petticoat', 'jamboree', 'tic', 'socialist', 'decorum', 'folder', 'shrapnel', 'molecule', 'driver', 'arrangement', 'violent', 'flex', 'tribune', 'champagne', 'soft', 'intellect', 'chauffeur', 'rail', 'hepatitis', 'checklist', 'scoop', 'tarmac', 'capsule', 'argument', 'wake', 'elf', 'pulp', 'badge', 'vast', 'squash', 'doom', 'scrambler', 'erotica', 'fox', 'intercom', 'limo', 'college', 'promenade', 'elevator', 'chance', 'crucifix', 'piranha', 'blues', 'caravan', 'sensor', 'regulator', 'lobby', 'barrage', 'clique', 'focus', 'revival', 'winter', 'workshop', 'carpool', 'oven', 'instinct', 'cavalier', 'horror', 'parachute', 'swastika', 'architect', 'grind', 'house', 'target', 'perfectionist', 'home', 'empire', 'land', 'chili', 'cult', 'swing', 'diligent', 'train', 'fungus', 'adult', 'server', 'area', 'link', 'unit', 'folk', 'camcorder', 'yoga', 'saboteur', 'veil', 'abject', 'level', 'spa', 'womanizer', 'chiffon', 'pose', 'reform', 'wolf', 'cruiser', 'glimmer', 'put', 'blockbuster', 'blowjob', 'jockey', 'receptionist', 'eloquent', 'dump', 'motor', 'matrix', 'genre', 'asparagus', 'serum', 'studio', 'van', 'toilet', 'census', 'trauma', 'printer', 'replay', 'gothic', 'hinder', 'sonar', 'speed', 'gorilla', 'backhand', 'jury', 'tape', 'deed', 'wed', 'cortex', 'chorus', 'pro', 'deck', 'arm', 'zoom', 'echelon', 'cabernet', 'software', 'toast', 'etiquette', 'reserve', 'haven', 'nectar', 'spike', 'bazaar', 'batch', 'meeting', 'organist', 'zone', 'slip', 'dope', 'robber', 'trapeze', 'adrenaline', 'turban', 'shag', 'kilt', 'partner', 'battle', 'lift', 'bodyguard', 'niche', 'referee', 'spelling', 'grandeur', 'training', 'intern', 'resource', 'folklore', 'graft', 'mach', 'excellent', 'unfair', 'grieve', 'teen', 'hip', 'game', 'stylist', 'gospel', 'raider', 'smash', 'retina', 'cluster', 'tool', 'uranium', 'halftime', 'constant', 'producer', 'transit', 'dinghy', 'larynx', 'patriot', 'talon', 'spam', 'kin', 'panda', 'sequel', 'sidecar', 'cooker', 'schnitzel', 'account', 'chip', 'roller', 'mustang', 'monitor', 'saga', 'paper', 'relevant', 'suspense', 'spring', 'roof', 'goulash', 'wee', 'drug', 'offer', 'scout', 'week', 'christen', 'stigma', 'gigolo', 'rooms', 'rein', 'permanent', 'move', 'quasi', 'salmonella', 'prominent', 'incubator', 'report', 'bartender', 'console', 'drugs', 'scope', 'glitter', 'timing', 'nautilus', 'organizer', 'plop', 'stomp', 'malt', 'memo', 'goalie', 'duplex', 'patron', 'penalty', 'filter', 'metallic', 'sleet', 'riff', 'premier', 'instrument', 'pus', 'large', 'stereotype', 'detergent', 'yard', 'copyright', 'rugby', 'toot', 'junior', 'adolescent', 'swap', 'flamingo', 'basket', 'consultant', 'smart', 'manager', 'hard', 'mescaline', 'momentum', 'kappa', 'dioxide', 'brief', 'shunt', 'styling', 'portable', 'rust', 'western', 'camper', 'cola', 'set', 'seminar', 'lid', 'strapless', 'camp', 'support', 'lifestyle', 'monster', 'trial', 'sub', 'background', 'airstrip', 'patent', 'comfort', 'charlatan', 'calculator', 'sake', 'topless', 'decade', 'pal', 'air', 'marker', 'sabotage', 'ventilator', 'pan', 'loafer', 'tunnel', 'drink', 'ravage', 'broccoli', 'orthodontist', 'nicotine', 'weekend', 'release', 'pomp', 'insolent', 'spin', 'yen', 'backbone', 'elite', 'snowboard', 'opener', 'coherent', 'inch', 'starter', 'scrimmage', 'winner', 'part', 'counseling', 'image', 'pad', 'python', 'export', 'veto', 'brat', 'contest', 'jumpsuit', 'claim', 'fiasco', 'meet', 'heel', 'violet', 'hand', 'raven', 'detonator', 'beer', 'sonnet', 'bouquet', 'keyboard', 'combine', 'hyena', 'birdie', 'brink', 'outlook', 'diva', 'relax', 'perimeter', 'spaghetti', 'clan', 'feeling', 'rebound', 'engineering', 'precedent', 'ponder', 'hoe', 'algebra', 'saloon', 'pardon', 'arrest', 'halter', 'efficiency', 'percent', 'citadel', 'import', 'avenue', 'lint', 'bachelor', 'lycra', 'timer', 'dictator', 'boy', 'bungalow', 'coma', 'bam', 'umpire', 'shabby', 'lover', 'wedding', 'yuppie', 'deejay', 'hamburger', 'superstar', 'gymnasium', 'hater', 'skinny', 'start', 'eminent', 'mainframe', 'bun', 'altimeter', 'checkpoint', 'ride', 'kind', 'worm', 'scone', 'scotch', 'rodeo', 'liter', 'spade', 'rapport', 'matter', 'voucher', 'label', 'escape', 'motel', 'piano', 'volley', 'tot', 'bypass', 'rite', 'cowboy', 'adder', 'guacamole', 'aftershave', 'bowling', 'drum', 'concert', 'equivalent', 'lotto', 'gaping', 'cassette', 'apache', 'click', 'limit', 'laser', 'martini', 'sheriff', 'killer', 'poncho', 'euro', 'icing', 'file', 'bar', 'ion', 'stunt', 'kimono', 'espresso', 'dominant', 'leader', 'lyrics', 'bureau', 'pianist', 'omen', 'dandy', 'slim', 'scalpel', 'stinker', 'stutter', 'potpourri', 'rat', 'shopper', 'scampi', 'podium', 'scanner', 'marathon', 'pot', 'godfather', 'sumo', 'wet', 'compost', 'bonsai', 'golden', 'frame', 'convent', 'want', 'cervix', 'jeep', 'twister', 'slag', 'hamster', 'base', 'airbag', 'respect', 'florist', 'brunette', 'interim', 'limousine', 'barricade', 'bluegrass', 'fiction', 'machine', 'tact', 'revue', 'top', 'song', 'site', 'golfer', 'elk', 'batter', 'anti', 'lingerie', 'jaguar', 'bedpan', 'trustee', 'icon', 'harem', 'baggy', 'zombie', 'buffet', 'par', 'poll', 'ambulance', 'terrorist', 'sport', 'symposium', 'hotel', 'malaria', 'dog', 'major', 'gate', 'sprinkler', 'lobbyist', 'device', 'interview', 'nurse', 'senior', 'board', 'merengue', 'gel', 'absent', 'proton', 'gaga', 'record', 'clinch', 'keeper', 'machete', 'cinema', 'elegant', 'skyline', 'commercial', 'blubber', 'blanket', 'safari', 'salami', 'dispenser', 'outsider', 'check', 'prefect', 'spotlight', 'mud', 'tag', 'psyche', 'inferno', 'recruiter', 'cue', 'token', 'casual', 'gander', 'kink', 'menthol', 'pager', 'otter', 'pallet', 'nanometer', 'shampoo', 'offset', 'atlas', 'wad', 'screening', 'toffee', 'junkie', 'ring', 'bigot', 'eyeliner', 'catchy', 'assessment', 'pisser', 'log', 'gala', 'sanatorium', 'rumba', 'welfare', 'barometer', 'snapper', 'blank', 'flash', 'helix', 'sponsor', 'palm', 'range', 'citrus', 'dummy', 'magnesium', 'amaretto', 'toner', 'gangster', 'sound', 'amusement', 'order', 'bonnet', 'equator', 'appetizer', 'gold', 'wild', 'frisbee', 'hockey', 'evident', 'serve', 'crew', 'match', 'imperialist', 'ace', 'shelter', 'junk', 'risotto', 'budget', 'hummer', 'brigade', 'baron', 'random', 'jingle', 'snorkel', 'tv', 'ballad', 'cartridge', 'vaccine', 'giraffe', 'cellist', 'madam', 'select', 'handicap', 'moor', 'pacifist', 'slogan', 'barman', 'corduroy', 'bowler', 'coup', 'ballpoint', 'bonbon', 'coating', 'fleece', 'loco', 'troop', 'fairway', 'pizza', 'split', 'amulet', 'review', 'acid', 'joke', 'mantel', 'pusher', 'putt', 'tractor', 'spook', 'baseline', 'paperback', 'tarot', 'libido', 'player', 'mail', 'helper', 'diesel', 'supplement', 'parallel', 'commerce', 'ham', 'stripper', 'chutney', 'torpedo', 'hymen', 'striptease', 'vamp', 'hobby', 'pentagram', 'tab', 'chat', 'break', 'kiwi', 'rotten', 'infant', 'cognac', 'spectrum', 'spot', 'toaster', 'pesto', 'groovy', 'stopper', 'waterproof', 'drinker', 'ounce', 'nap', 'chateau', 'tanker', 'ruling', 'calcium', 'suspect', 'cholera', 'ladylike', 'butterfly', 'baby', 'service', 'magnitude', 'cobra', 'status', 'fake', 'vinyl', 'tenor', 'node', 'billboard', 'ladder', 'gymnast', 'spit', 'thesis', 'society', 'reef', 'entertainment', 'root', 'diner', 'professor', 'impromptu', 'chauvinist', 'immigrant', 'coax', 'peanuts', 'barium', 'gap', 'vigilante', 'foyer', 'trombone', 'lunch', 'absurd', 'reminder', 'retro', 'gift', 'conductor', 'dopamine', 'plateau', 'workaholic', 'dart', 'pessimist', 'collage', 'lactose', 'glucose', 'polka', 'mega', 'construct', 'wrong', 'opponent', 'reel', 'goal', 'quiz', 'mini', 'container', 'wind', 'circuit', 'minor', 'leg', 'overkill', 'biker', 'sherry', 'email', 'truck', 'margarine', 'mausoleum', 'rake', 'bondage', 'lap', 'host', 'zest', 'punch', 'surveillance', 'pertinent', 'gin', 'listing', 'laryngitis', 'spinster', 'uppercut', 'fruit', 'headline', 'opening', 'kiosk', 'loft', 'business', 'short', 'tutu', 'plankton', 'bacon', 'notebook', 'overlap', 'steel', 'suite', 'surf', 'ski', 'pin', 'kidnapping', 'repertoire', 'must', 'carrier', 'herpes', 'embargo', 'invite', 'fee', 'spotter', 'petroleum', 'indulgent', 'lunchbox', 'inconsistent', 'superman', 'format', 'poster', 'millimeter', 'plan', 'mantra', 'cactus', 'slap', 'polyester', 'hydrant', 'teamwork', 'fair', 'kumquat', 'strip', 'ton', 'prove', 'bunker', 'ad', 'score', 'mild', 'romp', 'lip', 'confetti', 'serpent', 'mat', 'drifter', 'turbine', 'novice', 'insult', 'fade', 'dot', 'rouge', 'deviant', 'sexy', 'potent', 'hem', 'optimum', 'continue', 'wok', 'font', 'term', 'smack', 'microfilm', 'valium', 'teenager', 'duo', 'incompetent', 'sec', 'jackpot', 'heavy', 'model', 'tang', 'cranberry', 'theater', 'appendix', 'boot', 'hall', 'item', 'ibuprofen', 'post', 'plot', 'bebop', 'verdict', 'burrito', 'tequila', 'snob', 'hardcore', 'donkey', 'ping', 'dashboard', 'peroxide', 'vale', 'culture', 'tabloid', 'multiplex', 'dunk', 'punk', 'hippie', 'pointer', 'stretch', 'bopper', 'squaw', 'clipper', 'operator', 'collector', 'piston', 'damper', 'weed', 'bistro', 'manuscript', 'defect', 'den', 'disco', 'delinquent', 'hen', 'detriment', 'geek', 'dynamo', 'forum', 'directory', 'different', 'navel', 'glad', 'macaroni', 'aerobics', 'jumper', 'geisha', 'limbo', 'wildebeest', 'comeback', 'rally', 'cascade', 'firmament', 'occult', 'supernova', 'anarchist', 'buggy', 'bib', 'transcendent', 'ammonium', 'hanger', 'rib', 'present', 'hospice', 'mess', 'vibrator', 'stoop', 'direct', 'barrel', 'steward', 'station', 'pension', 'corsage', 'military', 'project', 'rock', 'coupon', 'selenium', 'mist', 'modem', 'grim', 'spinner', 'tonic', 'pope', 'shanty', 'helium', 'mama', 'micro', 'prospect', 'snip', 'gallon', 'queue', 'guerrilla', 'mistletoe', 'skunk', 'groupie', 'flipper', 'shift', 'aroma', 'flair', 'cup', 'medley', 'coyote', 'ranking', 'thriller', 'percentage', 'insecticide', 'croissant', 'document', 'appendage', 'cappuccino', 'room', 'apparent', 'censor', 'nest', 'mediocre', 'discipline', 'piccolo', 'neon', 'libel', 'irrelevant', 'tofu', 'camera', 'baseball', 'hertz', 'fantasy', 'chiropractor', 'villa', 'inherent', 'drab', 'fancy', 'relaxed', 'brunch', 'rage', 'doctor', 'brandy', 'bang', 'hare', 'waterbed', 'celebrity', 'suave', 'jade', 'molest', 'ascendant', 'exposure', 'crime', 'cottage', 'sweater', 'finish', 'grizzly', 'macro', 'gamma', 'wonder', 'catwalk', 'planetarium', 'nipper', 'ex', 'hang', 'lira', 'green', 'button', 'exorcist', 'curator', 'flagrant', 'squadron', 'audit', 'gram', 'slump', 'jumbo', 'pool', 'arena', 'sultan', 'body', 'quintet', 'bias', 'loon', 'rot', 'test', 'shell', 'napalm', 'parade', 'sedan', 'sprint', 'ensemble', 'conform', 'crescendo', 'complex', 'perm', 'placebo', 'glee', 'prudent', 'blender', 'commando', 'bodega', 'fine', 'prime', 'decor', 'net', 'cake', 'object', 'oversized', 'passage', 'return', 'extra', 'clown', 'cockpit', 'gal', 'ready', 'hulk', 'yell', 'silo', 'mode', 'tree', 'fanfare', 'taxi', 'landing', 'toss', 'smiley', 'vet', 'interest', 'push', 'paprika', 'toga', 'absence', 'cap', 'element', 'hop', 'hoop', 'indifferent', 'freelancer', 'lapel', 'gentleman', 'radio', 'city', 'chassis', 'transcript', 'occasion', 'angina', 'pet', 'accent', 'lens', 'topic', 'haiku', 'booster', 'publicist', 'gag', 'keep', 'pup', 'husky', 'trailer', 'gerbil', 'track', 'feminist', 'harmonica', 'sip', 'kelp', 'era', 'sightseeing', 'sinus', 'calculus', 'videotape', 'dildo', 'observant', 'hut', 'tycoon', 'escort', 'forte', 'slang', 'gust', 'rum', 'gas', 'vector', 'designer', 'jubilee', 'pudding', 'peer', 'reporter', 'prospectus', 'sashimi', 'mahjong', 'aura', 'entourage', 'masseur', 'privacy', 'superior', 'silicone', 'activist', 'template', 'entree', 'shaker', 'corridor', 'portal', 'coat', 'cortisone', 'porno', 'hypothalamus', 'clitoris', 'indoor', 'restaurant', 'chimp', 'tapioca', 'manifest', 'ignorant', 'drop', 'gringo', 'patio', 'star', 'slot', 'bromide', 'fiancee', 'arrogant', 'lever', 'prairie', 'twist', 'volume', 'hangar', 'bitter', 'omega', 'stress', 'hazard', 'concept', 'drift', 'drive', 'typist', 'jammer', 'dessert', 'tram', 'speaker', 'hyper', 'live', 'bedding', 'conference', 'moot', 'skin', 'pond', 'stimulus', 'turf', 'beat', 'jazzy', 'chloride', 'visa', 'store', 'fragment', 'dove', 'pedigree', 'team', 'lotus', 'healer', 'shimmy', 'regatta', 'coverage', 'biscuit', 'bio', 'detective', 'enchilada', 'dominion', 'mauve', 'pupil', 'picture', 'beatnik', 'dilemma', 'matador', 'trap', 'comedy', 'transport', 'offshore', 'loop', 'bad', 'innocent', 'rap', 'shocking', 'bowl', 'torment', 'gadget', 'slash', 'scrotum', 'arcade', 'halt', 'zigzag', 'lasso', 'healing', 'op', 'jukebox', 'buzzer', 'samba', 'lava', 'man', 'gay', 'latex', 'duster', 'incorrect', 'wit', 'diameter', 'papa', 'fascist', 'underdog', 'sushi', 'karma', 'big', 'incest', 'millennium', 'abrupt', 'testament', 'corner', 'trivia', 'classic', 'hub', 'cardigan', 'venture', 'flip', 'column', 'trainer', 'wringer', 'basketball', 'enigma', 'halo', 'simulator', 'scrabble', 'propeller', 'quote', 'ornament', 'happy', 'artwork', 'mixture', 'caramel', 'albino', 'canyon', 'conga', 'quiche', 'robot'}\n"
     ]
    }
   ],
   "source": [
    "#Polislava\n",
    "print(\"\\nPseudowords filtered out (found in Dutch):\")\n",
    "print(filtered_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "zvza9pb6MFgC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unigram encoding for 'ampgrair':\n",
      "[[2. 1. 1. 1. 1. 2.]]\n",
      "\n",
      "Bigram encoding for 'ampgrair':\n",
      "[[1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# encode Dutch words and pseudowords from Gatti et al as uni- and bi-gram vectors\n",
    "# show the uni-gram and bi-gram encoding of the pseudoword ampgrair\n",
    "# 2 points for correctly encoding the target strings as uni- and bi-gram vectors\n",
    "\n",
    "#Polislava\n",
    "def extract_ngrams_fixed(words, n, vocab=None):\n",
    "    \"\"\"Extract n-gram vectors using a fixed vocabulary (if provided).\"\"\"\n",
    "    if vocab is None:\n",
    "        # Build vocabulary from scratch\n",
    "        vocab_set = set()\n",
    "        for word in words:\n",
    "            grams = [word[i:i+n] for i in range(len(word)-n+1)]\n",
    "            vocab_set.update(grams)\n",
    "        vocab = sorted(vocab_set)\n",
    "\n",
    "    vocab_index = {gram: idx for idx, gram in enumerate(vocab)}\n",
    "    features = np.zeros((len(words), len(vocab)))\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        grams = [word[j:j+n] for j in range(len(word)-n+1)]\n",
    "        counts = Counter(grams)\n",
    "        for gram, freq in counts.items():\n",
    "            if gram in vocab_index:\n",
    "                features[i, vocab_index[gram]] = freq\n",
    "\n",
    "    return features, vocab\n",
    "\n",
    "\n",
    "\n",
    "# Show n-gram encodings for 'ampgrair'\n",
    "unigram_amp, uni_vocab = extract_ngrams_fixed(['ampgrair'], 1)\n",
    "bigram_amp, bi_vocab = extract_ngrams_fixed(['ampgrair'], 2)\n",
    "\n",
    "print(\"\\nUnigram encoding for 'ampgrair':\")\n",
    "print(unigram_amp)\n",
    "\n",
    "print(\"\\nBigram encoding for 'ampgrair':\")\n",
    "print(bigram_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "-w9-ySEuPp1Y"
   },
   "outputs": [],
   "source": [
    "# use word valence estimates from Speed and Brysbaert (2024) to train\n",
    "# - a uni-gram model\n",
    "# - a bi-gram model\n",
    "# 2 points for correctly trained models\n",
    "\n",
    "\n",
    "#Polislava\n",
    "def train_linear_model(X, y):\n",
    "    # Add intercept\n",
    "    X_aug = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "    weights = np.linalg.pinv(X_aug.T @ X_aug) @ X_aug.T @ y\n",
    "    return weights\n",
    "\n",
    "def predict(X, weights):\n",
    "    X_aug = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "    return X_aug @ weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polislava\n",
    "train_words = valence_df['Word'].tolist()\n",
    "y_train = valence_df['Valence'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polislava\n",
    "X_train_uni, _ = extract_ngrams_fixed(train_words, 1)\n",
    "X_train_bi, _ = extract_ngrams_fixed(train_words, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polislava\n",
    "weights_uni = train_linear_model(X_train_uni, y_train)\n",
    "weights_bi = train_linear_model(X_train_bi, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.67492058e-02, -2.27150461e-02,  3.79124681e-03, -2.58695636e-02,\n",
       "        3.62409735e-02, -2.62174000e-02, -2.70045279e-02, -2.50427772e-02,\n",
       "        1.11929621e-02,  8.18558432e-02, -4.78443004e-02, -2.13772643e-02,\n",
       "       -5.14790609e-03, -3.30303525e-02,  6.01828777e-04,  6.33011925e-03,\n",
       "       -1.20955605e-01, -3.31810551e-02, -4.68156165e-02, -1.05704987e-03,\n",
       "        2.18311652e-02,  3.05459140e-03, -5.48181769e-02,  6.11508021e-02,\n",
       "        7.48789342e-02, -5.24030800e-02,  8.99265049e-02,  2.91923430e-01,\n",
       "       -1.35696649e-01,  1.34324737e-01, -4.32508310e-01, -1.70229716e-01,\n",
       "        2.23327262e-01,  3.90887328e-01,  2.94997861e+00])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Polislava\n",
    "weights_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.16368955e-02, -8.51248287e-02,  3.92915549e-03,  2.23546262e-02,\n",
       "        2.70716773e-01, -3.68963691e-01, -1.10348818e-01,  5.08426260e-02,\n",
       "        2.91116304e-02,  5.36027686e-02, -7.17834303e-02, -1.09719214e-01,\n",
       "        6.73734737e-02, -5.78070788e-02,  1.18451248e-01, -1.05087179e-02,\n",
       "        2.46922249e-01,  1.77095273e-02, -1.79191533e-03, -1.09816567e-02,\n",
       "       -2.21415253e-02,  5.18985260e-02, -1.84206473e-01,  2.93657516e-01,\n",
       "       -1.49313615e-01, -3.08510083e-01, -2.35785411e-01, -3.64957662e-01,\n",
       "        3.86532367e-02, -9.97626843e-02, -4.58751624e-01,  2.71890026e-01,\n",
       "       -6.99847214e-02,  2.58136636e-01,  2.22026482e-02, -1.76409223e-01,\n",
       "       -3.93540983e-02, -9.00738741e-01,  2.15757118e-01, -2.05106454e-03,\n",
       "       -1.07701564e-01,  1.02757858e-01, -1.59357234e-02,  5.91322135e-01,\n",
       "       -1.19074229e+00,  1.96756130e-03,  2.17982028e-01,  2.86994192e-02,\n",
       "        6.17172145e-02,  8.29700531e-02,  2.38879797e-01,  1.42305648e-02,\n",
       "       -1.15669165e+00,  3.12827343e-01, -9.33830811e-02,  1.19409378e-01,\n",
       "        9.87427749e-03,  5.14616449e-02,  3.62864496e-01, -1.35757857e+00,\n",
       "        2.89818888e-02, -1.47908380e-02,  1.02642250e-01, -2.13961858e-02,\n",
       "       -8.25595633e-02,  1.18186531e-01, -8.65655582e-02, -1.46830907e-01,\n",
       "       -1.02542019e-01, -3.43247131e-02,  5.59472770e-02, -2.21313588e-02,\n",
       "        2.29770191e-01, -4.42917720e-02,  1.11946007e-01, -4.60210413e-01,\n",
       "        3.45056686e-02, -2.17275047e-01, -1.95439519e-01,  2.45779075e-02,\n",
       "        5.85107299e-02, -1.03294506e-02, -1.25666369e-01, -3.49212509e-01,\n",
       "        7.67182969e-01, -2.99132198e-01,  5.28390270e-02,  2.46271456e-01,\n",
       "        9.66031182e-02,  5.54395632e-02,  7.13333863e-02,  2.54658333e-02,\n",
       "        2.91772878e-02, -1.20044668e-01,  5.87228294e-02,  6.68344657e-02,\n",
       "        1.82693208e-01, -2.88310900e-02,  2.45608789e-02,  1.22048792e-01,\n",
       "        2.71338549e-02,  3.45511570e-01, -1.12126169e-01,  5.50287150e-02,\n",
       "        3.84729126e-02,  4.81743572e-02,  2.32576550e-02,  6.53940740e-02,\n",
       "        1.93374221e-01,  1.31233128e-01, -1.01357144e-01,  3.53120583e-01,\n",
       "       -3.93872335e-02,  1.61438858e-01,  4.34370407e-01,  4.20179316e-01,\n",
       "        1.06480566e-01, -1.04840031e-01,  2.37042454e-01,  2.52758213e-01,\n",
       "        2.78791926e-01, -1.28800577e-01, -2.33145495e-01,  2.63483990e-01,\n",
       "        3.19189108e-01,  9.76729242e-02, -4.22652853e-02,  1.17837578e-02,\n",
       "       -1.09446429e-01,  1.38274716e-01,  3.26360221e-01, -7.78115030e-02,\n",
       "        5.53474269e-02,  6.42324810e-02,  8.55339868e-02,  8.64706665e-02,\n",
       "        1.58388745e-01,  7.49975429e-02,  8.06560870e-01, -9.67559425e-02,\n",
       "        1.07547888e+00,  5.09720966e-03,  1.40572897e-01,  8.69880555e-02,\n",
       "        1.85298051e-01,  1.47497942e-01,  2.04852464e-02,  1.05935399e-01,\n",
       "       -2.66880312e-02,  8.49167304e-02,  8.43859136e-02, -2.81627110e-01,\n",
       "        9.14303960e-02,  1.67574633e-01, -6.20225294e-02,  2.91704125e-01,\n",
       "        4.01670013e-01,  4.97003977e-03,  7.52437271e-02,  3.77972510e-02,\n",
       "        1.28763499e-01,  2.77501312e-01, -1.55236989e-02,  1.05688783e-01,\n",
       "        3.54904272e-01,  9.59373272e-03,  7.87308067e-01, -4.82486900e-01,\n",
       "        4.09145834e-02,  5.98062785e-02,  4.76433634e-03,  7.68604323e-01,\n",
       "       -4.71458448e-01, -5.69217121e-02, -6.47943128e-01,  1.28363517e+00,\n",
       "       -5.00289598e-01,  5.38351233e-01,  2.10933818e-01,  1.51539229e-01,\n",
       "       -5.24873794e-01, -2.01160255e-01,  7.59071994e-01, -7.46931985e-02,\n",
       "       -6.43340797e-03, -1.41191367e+00,  7.19244971e-01, -3.99979619e-02,\n",
       "        2.49385878e-01, -4.50740066e-01,  2.24444842e-02, -4.24356510e-02,\n",
       "       -4.88885907e-02,  5.30140458e-02, -4.93557987e-02, -1.69947397e-01,\n",
       "       -1.67523076e-01, -7.84123855e-01, -3.82390907e-01, -1.46133761e-01,\n",
       "       -1.99840603e-01, -1.40649973e-01,  6.84044817e-02,  3.07980110e-02,\n",
       "        2.92569813e-01, -5.92179648e-02, -3.16810474e-01, -8.72237172e-03,\n",
       "       -7.13059607e-02,  1.87489699e-02, -6.18423115e-02,  1.11185461e-01,\n",
       "       -4.13722599e-03, -2.50340714e-02, -2.41567515e-01,  5.57625271e-01,\n",
       "        1.34841950e-02, -7.27355570e-02, -9.37505220e-02,  6.17655372e-02,\n",
       "        7.67901581e-02,  3.98539824e-01, -1.95413950e-01,  1.14302739e-01,\n",
       "        6.99514469e-01,  2.86411345e-01,  1.45102480e-01,  1.16377572e-01,\n",
       "        1.27221785e-01,  9.06935800e-02,  2.81640051e-01,  1.34504841e-01,\n",
       "        1.42070198e-01,  3.01837943e-01, -7.59498841e-02,  2.58984615e-01,\n",
       "        3.22065963e-01,  1.24533933e-01,  2.75853551e-02,  1.19535638e-02,\n",
       "        4.28643526e-02,  3.35772626e-01,  9.14546796e-02, -7.36404414e-02,\n",
       "       -4.98997375e-02, -1.15173640e-01,  1.24144790e-02,  7.07608672e-03,\n",
       "       -4.55787992e-02,  3.54856088e-02, -9.27003347e-02, -2.44220561e-02,\n",
       "       -1.58135285e-01,  1.67047213e-01,  4.56585825e-03, -4.67268344e-02,\n",
       "       -8.91328344e-02, -1.39338288e-02,  1.53437629e-01,  1.04236896e-01,\n",
       "       -1.85239171e-01,  4.78146966e-01, -1.41867787e-01,  1.25936530e+00,\n",
       "       -4.10072838e-03,  1.43982205e-01,  3.33593240e-02,  1.05050136e-01,\n",
       "       -1.29497064e-02,  9.92151564e-02, -1.23337141e-01,  1.41323677e-01,\n",
       "        1.07507941e-01,  3.72892113e-01, -1.20097771e-03, -5.50057016e-03,\n",
       "        1.93863287e-01,  4.94730797e-02,  1.12962849e-01,  2.01360525e-01,\n",
       "        3.12755949e-01,  1.14516111e-01,  6.92460426e-02,  5.33306832e-02,\n",
       "        2.07672867e-01,  8.93134937e-02,  3.36040626e-01,  5.02255493e-02,\n",
       "        1.93102420e-01,  2.70586452e-02, -9.58717291e-02, -8.49955128e-02,\n",
       "       -3.37855616e-01, -2.45552769e-02, -1.14475209e-01,  1.07193084e-01,\n",
       "        2.16287438e-01, -1.79380890e-01, -3.18016929e-01,  6.44215171e-03,\n",
       "       -2.66546974e-01,  6.14061503e-03,  1.50717934e-01, -2.03119953e-01,\n",
       "        2.62321089e-01,  2.04339438e-01,  1.66130942e-01,  1.48543106e-01,\n",
       "        1.43233690e-01, -8.98994754e-02,  5.16979774e-01, -2.06885044e-01,\n",
       "       -5.41509464e-02,  6.08619507e-03, -8.22546090e-02, -1.04955591e-01,\n",
       "        5.58409965e-02, -5.25798515e-02, -3.77568149e-01, -9.94972889e-02,\n",
       "       -7.61347282e-02,  5.59921390e-02, -3.76652270e-01,  1.14895716e-02,\n",
       "       -2.02462606e-02,  1.25689270e-01,  1.06416501e-01,  2.26685218e-01,\n",
       "       -1.79701630e-01, -1.22994279e+00, -1.90479301e-01,  8.07922916e-02,\n",
       "        8.26279628e-02,  4.63310777e-02, -7.98042993e-02, -4.32157349e-02,\n",
       "       -9.56150122e-02,  8.97586403e-02, -2.08833315e-01,  8.18729261e-03,\n",
       "       -3.27354445e-01, -3.31709386e-01, -4.46685757e-01, -1.90772612e-01,\n",
       "       -4.20162977e-01, -1.23130309e-01, -5.34255137e-01, -2.33896742e-01,\n",
       "       -4.50768280e-01, -3.25818758e-01, -2.13147219e-01, -9.26921216e-02,\n",
       "       -2.48522767e-01, -2.28041910e-02, -1.48773807e-01, -1.17713662e+00,\n",
       "       -1.92856035e-01, -2.48433048e-01, -3.06434336e-01, -7.26334266e-03,\n",
       "       -6.76917751e-02,  1.04458883e-01, -1.08763545e-01, -1.62676871e-01,\n",
       "       -8.08779890e-02,  1.46141547e-01, -8.14636346e-01,  5.37942941e-02,\n",
       "        9.75951593e-02,  2.29471832e-01,  3.98748109e-01,  2.01401872e-02,\n",
       "       -4.65297789e-02,  2.41777610e-01,  1.39709270e-01,  7.03693973e-02,\n",
       "       -3.24633694e-02,  5.30445796e-02,  4.18416406e-01,  7.48887489e-02,\n",
       "        2.60828178e-01, -1.92627294e-02,  2.40258306e-01,  4.51328896e-02,\n",
       "        9.84254696e-02,  2.73014989e-02,  1.40867022e-01, -1.78902233e-01,\n",
       "        2.65217195e-01,  3.43257292e-01,  2.25281730e-02, -4.23007154e-02,\n",
       "        7.04674216e-02,  1.06471976e-01, -1.27866907e-02,  1.71439240e-02,\n",
       "       -1.85587383e-01, -4.04564635e-02, -1.28636398e-02, -4.82630564e-02,\n",
       "       -1.19257938e-01,  3.86019145e-02,  3.63286397e-02,  3.56328385e-01,\n",
       "        8.92885986e-02,  2.84555389e-02,  5.08413062e-03, -5.78149020e-02,\n",
       "        1.80625289e-01, -1.26379370e-01, -2.15567345e-01,  3.06995843e-02,\n",
       "        6.39348546e-02,  2.78009885e-02, -1.25993571e-01, -3.68738341e-02,\n",
       "       -2.56130975e-01,  8.17954085e-02, -5.10536764e-03,  3.23665764e-01,\n",
       "        2.61428534e-01, -1.20378844e-01,  1.08551066e-01, -9.07062674e-02,\n",
       "       -1.18621014e-01, -2.04075283e-01, -8.39708753e-02,  1.36246193e-01,\n",
       "       -3.06619673e-02, -6.34313834e-02, -6.09366802e-02, -9.18347911e-02,\n",
       "       -1.08164894e-01, -2.78127917e-01, -1.65444395e-01, -1.24692643e-01,\n",
       "        2.78175594e-01, -1.14753194e-02,  1.68674630e-01,  2.10288894e-01,\n",
       "        5.43192840e-02, -5.82056181e-02,  1.60984975e-01, -2.01571397e-03,\n",
       "        6.08151269e-02,  3.94579404e-01, -2.71637106e-01, -9.02486344e-01,\n",
       "       -3.99177877e-02,  2.18616571e-02,  1.63876588e-02,  2.32428257e-01,\n",
       "        1.18803178e-01, -6.29406127e-02, -1.47093485e-01, -1.85448475e-01,\n",
       "        5.62636167e-02,  4.73094763e-02, -1.75123515e-01, -7.11803296e-02,\n",
       "       -3.30745776e-02,  1.02438567e-01,  2.07928684e-01,  2.15191550e-01,\n",
       "       -1.15140923e-01, -4.28050887e-02, -8.36505137e-02, -1.19647367e-02,\n",
       "        1.55951896e-01,  1.02223932e-02, -4.58287977e-02,  2.95996635e-01,\n",
       "       -2.75912763e-01,  7.66203040e-02,  3.11155953e-01, -1.12634079e-01,\n",
       "        1.04253253e-01, -1.16646803e-01, -6.71399906e-02, -1.21803595e-01,\n",
       "       -8.36317456e-02, -1.13169304e-01, -4.43962250e-02,  6.21455726e-01,\n",
       "       -7.40423789e-02, -1.48046532e-01,  7.77606139e-03,  8.25322266e-02,\n",
       "        3.79823843e-01, -3.37636102e-02,  5.34120191e-01, -6.66037475e-02,\n",
       "        7.24086972e-02, -1.46396844e-01, -4.21815649e-02, -7.85529102e-02,\n",
       "       -1.40182953e-01,  5.42488887e-02,  1.43404279e-02,  5.73057065e-01,\n",
       "       -3.16983043e-01, -1.45088821e-02, -3.17153869e-02, -1.29167195e-01,\n",
       "        1.03110371e-01, -1.49189662e-02,  3.58765342e-01,  2.26959294e-01,\n",
       "       -1.13813498e-01,  9.24439515e-03, -3.50177185e-02,  2.49429937e-01,\n",
       "       -2.10367626e-01,  3.66050154e-01, -2.98996704e-02,  8.48091310e-01,\n",
       "       -3.90506765e-02, -9.70967502e-02,  6.10434048e-02,  6.91947550e-01,\n",
       "        2.67351294e-01, -6.60612765e-02,  3.92810533e-01, -6.25892065e-01,\n",
       "        2.33777353e-01,  1.20895333e-01, -7.72596198e-01,  4.31151711e-02,\n",
       "       -3.69530811e-03, -2.23307285e-02, -1.35115122e-01,  9.43673294e-02,\n",
       "       -6.91010724e-01,  2.73755582e-01, -4.26572054e-01, -3.85174188e-01,\n",
       "        2.39548307e-01,  1.91762142e-01,  1.94325074e-01,  4.41064167e-01,\n",
       "        5.10169908e-01, -1.64683783e-01, -6.50560810e-01,  3.58083568e-01,\n",
       "       -3.88300996e-01, -9.49292731e-01, -1.64801277e-01, -8.42751214e-02,\n",
       "        3.89868932e-01, -1.20617534e+00, -4.42396535e-02, -8.42210821e-03,\n",
       "       -5.41728615e-01,  3.94661397e-01, -3.15301456e-01, -2.30930299e-01,\n",
       "       -4.63800759e-01, -1.72779205e-01, -1.78681866e-01,  6.00062228e-01,\n",
       "        5.05540836e-02,  1.14993890e-01,  3.26153605e-02,  4.71043833e-02,\n",
       "        5.44713896e-01,  4.45096530e-01, -1.23460151e-01, -3.44790459e-01,\n",
       "        2.41795203e-01,  3.60983249e-01,  6.36828891e-02,  7.66203040e-02,\n",
       "        2.59982875e-01,  2.89818888e-02, -3.44861072e-01,  3.61082510e-01,\n",
       "        1.34841950e-02, -4.82486900e-01,  2.61428534e-01,  5.73057065e-01,\n",
       "       -3.03334091e-02, -4.00836137e-01, -6.69478797e-01, -9.33965665e-03,\n",
       "        1.42205305e-01,  8.46252795e-01,  1.90386261e-01,  1.65988486e-01,\n",
       "       -2.35785411e-01, -6.97828707e-01,  1.36189521e-01, -3.00979693e-01,\n",
       "        1.28624629e+00, -3.69512662e-01, -1.71024994e+00,  3.24714102e-01,\n",
       "        5.40834656e-01, -6.88341469e-01,  4.16721256e-01,  5.09720966e-03,\n",
       "       -1.20378844e-01,  2.49950483e-01, -1.96156189e-01, -1.45088821e-02,\n",
       "        4.20179316e-01,  2.95534235e+00])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Polislava\n",
    "weights_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "6RG3aeRSPtLX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted valence for pseudowords (unigrams):\n",
      "[2.86320485 2.94319716 2.8664392  ... 2.90404121 2.81630509 2.89130198]\n",
      "\n",
      "Predicted valence for pseudowords (bigrams):\n",
      "[2.90384868 2.61101178 2.90378339 ... 3.03607404 3.13920897 3.02480308]\n",
      "\n",
      "Predicted valence for training data (unigrams):\n",
      "[2.89677953 2.95453454 2.98260769 ... 2.8534857  2.85580625 2.85822289]\n",
      "\n",
      "Predicted valence for training data (bigrams):\n",
      "[3.11962299 2.51046414 3.46135382 ... 2.52543824 2.58113107 2.67811751]\n"
     ]
    }
   ],
   "source": [
    "# apply trained models to predict the valence of pseudowords from Gatti et al (2024).\n",
    "# Then apply the same models back onto the training set to see how well they predict the valence of words in Speed and Brysbaert (2024).\n",
    "# 2 points for correctly applied models\n",
    "\n",
    "# pseudo_words = gatti_filtered['rownames'].tolist()\n",
    "\n",
    "# X_test_uni, _ = extract_ngrams_fixed(pseudo_words, 1, vocab=uni_vocab)\n",
    "# X_test_bi, _ = extract_ngrams_fixed(pseudo_words, 2, vocab=bi_vocab)\n",
    "\n",
    "# # Predict valence\n",
    "# train_preds_uni = predict(X_train_uni, weights_uni)\n",
    "# train_preds_bi = predict(X_train_bi, weights_bi)\n",
    "\n",
    "# test_preds_uni = predict(X_test_uni, weights_uni)\n",
    "# test_preds_bi = predict(X_test_bi, weights_bi)\n",
    "\n",
    "\n",
    "#Polislava\n",
    "import numpy as np\n",
    "\n",
    "# Linear regression fitting function\n",
    "def fit(X, y):\n",
    "    # Add intercept (bias term)\n",
    "    X_aug = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "    # Compute weights using the normal equation\n",
    "    weights = np.linalg.pinv(X_aug) @ y\n",
    "    return weights\n",
    "\n",
    "# Predict function\n",
    "def predict(X, weights):\n",
    "    X_aug = np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "    return X_aug @ weights\n",
    "\n",
    "# Training phase (use your actual data)\n",
    "X_train_uni, uni_vocab = extract_ngrams_fixed(valence_df['Word'].tolist(), 1)\n",
    "X_train_bi, bi_vocab = extract_ngrams_fixed(valence_df['Word'].tolist(), 2)\n",
    "\n",
    "weights_uni = fit(X_train_uni, valence_df['Valence'].values)\n",
    "weights_bi = fit(X_train_bi, valence_df['Valence'].values)\n",
    "\n",
    "# Prediction phase\n",
    "pseudo_words = gatti_filtered['rownames'].tolist()\n",
    "\n",
    "X_test_uni, _ = extract_ngrams_fixed(pseudo_words, 1, vocab=uni_vocab)\n",
    "X_test_bi, _ = extract_ngrams_fixed(pseudo_words, 2, vocab=bi_vocab)\n",
    "\n",
    "# Predict on pseudowords\n",
    "test_preds_uni = predict(X_test_uni, weights_uni)\n",
    "test_preds_bi = predict(X_test_bi, weights_bi)\n",
    "\n",
    "# Predict back on training set\n",
    "train_preds_uni = predict(X_train_uni, weights_uni)\n",
    "train_preds_bi = predict(X_train_bi, weights_bi)\n",
    "print(\"\\nPredicted valence for pseudowords (unigrams):\")\n",
    "print(test_preds_uni)\n",
    "\n",
    "print(\"\\nPredicted valence for pseudowords (bigrams):\")\n",
    "print(test_preds_bi)\n",
    "\n",
    "print(\"\\nPredicted valence for training data (unigrams):\")\n",
    "print(train_preds_uni)\n",
    "\n",
    "print(\"\\nPredicted valence for training data (bigrams):\")\n",
    "print(train_preds_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "9aDwbajtPxze"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation (training set, unigram model): 0.089\n",
      "Spearman correlation (training set, bigram model): 0.321\n",
      "Spearman correlation (pseudowords, unigram model): 0.045\n",
      "Spearman correlation (pseudowords, bigram model): 0.055\n"
     ]
    }
   ],
   "source": [
    "# compute the Spearman correlation coefficients between true valence and predicted valence under both uni- and bi-gram models for\n",
    "# - words from Speed and Brysbaert (2024)\n",
    "# - pseudowords from Gatti and colleagues (2024)\n",
    "# show both correlation coefficients.\n",
    "# 2 points for the correct Spearman correlation coefficients (rounded to the third decimal place)\n",
    "\n",
    "\n",
    "#Polislava\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# True valence scores for the training data (Speed & Brysbaert, 2024)\n",
    "y_train_true = valence_df['Valence'].values\n",
    "\n",
    "# Compute Spearman correlation for training set\n",
    "corr_train_uni, _ = spearmanr(y_train_true, train_preds_uni)\n",
    "corr_train_bi, _ = spearmanr(y_train_true, train_preds_bi)\n",
    "\n",
    "# For pseudowords, we don't have true valence (since they are pseudowords),\n",
    "# so correlation is computed between the predictions from the two models\n",
    "# OR you can skip the correlation between models and just show predictions.\n",
    "\n",
    "# But assuming Gatti et al. provided some valence ratings (e.g., for analysis), then:\n",
    "if 'Valence' in gatti_filtered.columns:\n",
    "    y_test_true = gatti_filtered['Valence'].values\n",
    "\n",
    "    # Compute Spearman correlation for pseudowords\n",
    "    corr_test_uni, _ = spearmanr(y_test_true, test_preds_uni)\n",
    "    corr_test_bi, _ = spearmanr(y_test_true, test_preds_bi)\n",
    "\n",
    "    print(f\"Spearman correlation (training set, unigram model): {corr_train_uni:.3f}\")\n",
    "    print(f\"Spearman correlation (training set, bigram model): {corr_train_bi:.3f}\")\n",
    "\n",
    "    print(f\"Spearman correlation (pseudowords, unigram model): {corr_test_uni:.3f}\")\n",
    "    print(f\"Spearman correlation (pseudowords, bigram model): {corr_test_bi:.3f}\")\n",
    "else:\n",
    "    print(f\"Spearman correlation (training set, unigram model): {corr_train_uni:.3f}\")\n",
    "    print(f\"Spearman correlation (training set, bigram model): {corr_train_bi:.3f}\")\n",
    "    print(\"\\nNote: No 'Valence' ratings found in Gatti pseudoword data, so test set correlations cannot be computed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0P1rNg5QoDn"
   },
   "source": [
    "**Task 2** (*8 points available, see breakdown below*)\n",
    "\n",
    "Again following Gatti and colleagues, you should encode the target strings (pseudowords and Dutch words from Speed and Brysbaert) as fastText embeddings, train a multiple regression model on Dutch words and apply it to the pseudowords in Gatti et al. You should finally report the Spearman correlation coefficient between observed and predicted valence for both words and pseudowords.\n",
    "\n",
    "You should use the pre-trained fastText model for Dutch, available at this page: https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "\n",
    "Finally, you should answer two questions about the fastText model (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "with gzip.open(\"cc.nl.300.bin.gz\", \"rb\") as f_in:\n",
    "    with open(\"cc.nl.300.bin\", \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pDeWgWUNAckd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# load the fastText model\n",
    "# 1 point for correctly loading the appropriate fastText model\n",
    "#step1\n",
    "import fasttext\n",
    "#fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('cc.nl.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of Dutch fastText embeddings: 300\n",
      "Minimum n-gram: 5\n",
      "Maximum n-gram: 5\n"
     ]
    }
   ],
   "source": [
    "#step2\n",
    "#What is the dimensionality of the embeddings?\n",
    "embedding_dim = ft.get_dimension()\n",
    "print(\"Dimensionality of Dutch fastText embeddings:\", embedding_dim)\n",
    "\n",
    "# What were the minimum and maximum n-gram sizes?\n",
    "args = ft.f.getArgs()\n",
    "print(\"Minimum n-gram:\", args.minn)\n",
    "print(\"Maximum n-gram:\", args.maxn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUD0VUJeRhr3"
   },
   "source": [
    "What is the dimensionality of the pre-trained Dutch fastText embeddings? (*1 point for the correct answer*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "300 Standard fastText embedding size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgej4BPNRoUE"
   },
   "source": [
    "What minimum and maximum n-gram size was specified for training this fastText model? (*1 point for the correct answer*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5 and 5 Dutch model trained with only 5-character subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
     ]
    }
   ],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "aW-XEksGR28U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding of 'speelplaats': [ 0.0253247  -0.00634261  0.02746305 -0.04024595  0.04888906  0.00660965\n",
      " -0.04152017 -0.01824508 -0.00645641  0.00093806  0.0708492  -0.03291791\n",
      "  0.00263817 -0.02825846 -0.02188046 -0.03188037 -0.01846142 -0.02203094\n",
      " -0.01883078 -0.00259199]\n",
      "Embedding of 'danchunk': [-0.00592199  0.00097547  0.05925412  0.00053251 -0.00386978 -0.02089076\n",
      " -0.02829577  0.00972911 -0.02510111 -0.11454885 -0.02695064  0.01551034\n",
      "  0.02384409  0.01009528  0.04545438  0.00997385 -0.00474529  0.02524533\n",
      "  0.02430548 -0.02851078]\n"
     ]
    }
   ],
   "source": [
    "# encode Dutch words and pseudowords as fastText embeddings\n",
    "# show the first 20 values of the embedding of the word 'speelplaats' and of the pseudoword 'danchunk'\n",
    "# 2 points for correctly encoding words and pseudowords with fastText\n",
    "\n",
    "valence_df = pd.read_excel(\"All_Valence.xlsx\")\n",
    "gatti_df = pd.read_csv(\"gatti_new_converted_data_pseudovalenceRData.csv\")\n",
    "\n",
    "\n",
    "def encode(word):\n",
    "    return ft.get_word_vector(word)\n",
    "\n",
    "# Show first 20 values of embeddings\n",
    "print(\"Embedding of 'speelplaats':\", encode(\"speelplaats\")[:20])\n",
    "print(\"Embedding of 'danchunk':\", encode(\"danchunk\")[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rownames               1\n",
       "Valence                0\n",
       "predicted_val          0\n",
       "predicted_valL         0\n",
       "predicted_valL_BI      0\n",
       "predicted_valDIM       0\n",
       "predicted_valL_DIM     0\n",
       "predicted_valBI        0\n",
       "predicted_valBI_DIM    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gatti_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatti_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Word', 'Valence', 'N_Unknown', 'N_Valence', 'ProportionUnknown',\n",
       "       'RemoveUnknown'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valence_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rownames', 'Valence', 'predicted_val', 'predicted_valL',\n",
       "       'predicted_valL_BI', 'predicted_valDIM', 'predicted_valL_DIM',\n",
       "       'predicted_valBI', 'predicted_valBI_DIM'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gatti_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3ePBth7cSAJU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                        | 0/24037 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████████████████████████████████▌                                       | 10871/24037 [00:00<00:00, 101108.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████| 24037/24037 [00:00<00:00, 105302.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                                                        | 0/13786 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████| 13786/13786 [00:00<00:00, 111835.99it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Regression model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# train regression model on word valence\n",
    "# 1 point for correctly training the regression model\n",
    "from tqdm import tqdm\n",
    "# Create embedding matrices\n",
    "X_words = np.vstack([encode(w) for w in tqdm(valence_df[\"Word\"])])\n",
    "y_words = valence_df[\"Valence\"].values\n",
    "\n",
    "X_pseudo = np.vstack([encode(w) for w in tqdm(gatti_df[\"rownames\"])])\n",
    "y_pseudo = gatti_df[\"Valence\"].values\n",
    "\n",
    "# Add bias term (intercept) to X\n",
    "X_words_bias = np.hstack((X_words, np.ones((X_words.shape[0], 1))))\n",
    "\n",
    "# Normal equation: w = (X^T X)^-1 X^T y\n",
    "weights = np.linalg.pinv(X_words_bias.T @ X_words_bias) @ X_words_bias.T @ y_words\n",
    "\n",
    "print(\"✅ Regression model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "REJnnM2mSHHK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predictions done for both pseudowords and training words!\n"
     ]
    }
   ],
   "source": [
    "# apply the trained model to predict the valence of pseudowords from Gatti et al (2024).\n",
    "# Then apply the same model back onto the training set to see how well it predicts the valence of words in Speed and Brysbaert (2024).\n",
    "# 1 point for correctly applied model\n",
    "\n",
    "# Add bias term to pseudoword embeddings\n",
    "X_pseudo_bias = np.hstack((X_pseudo, np.ones((X_pseudo.shape[0], 1))))\n",
    "\n",
    "# Predict valence scores for pseudowords\n",
    "pred_pseudo = X_pseudo_bias @ weights\n",
    "\n",
    "# Predict valence scores for training words (for model evaluation)\n",
    "pred_words = X_words_bias @ weights\n",
    "\n",
    "print(\"✅ Predictions done for both pseudowords and training words!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "JIyyTyfHSKh5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation (words): 0.724\n",
      "Spearman correlation (pseudowords): 0.498\n"
     ]
    }
   ],
   "source": [
    "# compute the Spearman correlation coefficients between true valence and predicted valence for\n",
    "# - words from Speed and Brysbaert (2024)\n",
    "# - pseudowords from Gatti and colleagues (2024)\n",
    "# show the correlation coefficient.\n",
    "# 1 point for the correct Spearman correlation coefficients (rounded to the third decimal place)\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Correlation for Dutch words\n",
    "r_words, _ = spearmanr(y_words, pred_words)\n",
    "\n",
    "# Correlation for pseudowords\n",
    "r_pseudo, _ = spearmanr(y_pseudo, pred_pseudo)\n",
    "\n",
    "# Round to 3 decimal places\n",
    "print(f\"Spearman correlation (words): {r_words:.3f}\")\n",
    "print(f\"Spearman correlation (pseudowords): {r_pseudo:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnqKJ5XOSTbM"
   },
   "source": [
    "**Task 3** (*6 points available, see breakdown below*)\n",
    "\n",
    "Now you are asked to extend the work by Gatti et al by also considering the representations learned by a transformer-based models, in detail *RobBERT v2* (https://huggingface.co/pdelobelle/robbert-v2-dutch-base). You should follow the same pipeline as for the previous models, encoding both Dutch words from Speed and Brysbaert (2024) and the pseudowords from Gatti et al using the embedding of each string at layer 0, before positional information is factored in. If a string consists of multiple tokens, average the embeddings of all tokens to produce the embedding of the whole string. Then train a multiple regression model on the valence of Dutch words, apply it to the pseudowords, and compute the Spearman correlation between observed and predicted ratings.\n",
    "\n",
    "Use the HuggingFace model card for RobBERT v2 to check how to access it.\n",
    "\n",
    "I recommend saving the embeddings to file once you have generated them and you know they are correct: embedding thousands of strings takes some time, and you don't want to have to do it again. For the same reason, develop your code by considering only a small fractions of the words and pseudowords, in order to quickly see if something is wrong. Only when you are positive it works, embed all strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp39-cp39-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\python3\\lib\\site-packages (from torch) (3.16.1)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Using cached typing_extensions-4.13.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\python3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\python3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Downloading torch-2.6.0-cp39-cp39-win_amd64.whl (204.1 MB)\n",
      "   ---------------------------------------- 0.0/204.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/204.1 MB 16.7 MB/s eta 0:00:13\n",
      "   ---------------------------------------- 1.8/204.1 MB 4.4 MB/s eta 0:00:47\n",
      "   - -------------------------------------- 7.6/204.1 MB 12.7 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 18.1/204.1 MB 22.4 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 30.4/204.1 MB 30.1 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 38.8/204.1 MB 31.6 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 48.5/204.1 MB 33.6 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 55.8/204.1 MB 33.6 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 63.2/204.1 MB 33.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 69.2/204.1 MB 33.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 74.4/204.1 MB 32.5 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 80.5/204.1 MB 32.1 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 87.0/204.1 MB 31.9 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 92.3/204.1 MB 31.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 96.5/204.1 MB 30.6 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 101.2/204.1 MB 30.0 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 106.2/204.1 MB 29.7 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 111.4/204.1 MB 29.4 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 116.4/204.1 MB 29.1 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 121.6/204.1 MB 28.9 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 125.6/204.1 MB 28.4 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 129.5/204.1 MB 27.9 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 133.4/204.1 MB 27.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 137.4/204.1 MB 27.2 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 141.3/204.1 MB 26.8 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 145.5/204.1 MB 26.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 149.9/204.1 MB 26.2 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 154.4/204.1 MB 26.1 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 158.3/204.1 MB 25.9 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 161.5/204.1 MB 25.5 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 164.9/204.1 MB 25.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 168.3/204.1 MB 24.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 170.9/204.1 MB 24.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 173.8/204.1 MB 24.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 176.4/204.1 MB 23.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 179.3/204.1 MB 23.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 182.7/204.1 MB 23.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 185.9/204.1 MB 23.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 189.0/204.1 MB 23.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 192.9/204.1 MB 22.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 196.6/204.1 MB 22.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  200.0/204.1 MB 22.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.2/204.1 MB 22.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 22.3 MB/s eta 0:00:01\n",
      "   --------------------------------------- 204.1/204.1 MB 21.5 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 3.9/6.2 MB 19.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 17.3 MB/s eta 0:00:00\n",
      "Using cached typing_extensions-4.13.1-py3-none-any.whl (45 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 17.8 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 8.6 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, networkx, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "Successfully installed mpmath-1.3.0 networkx-3.2.1 sympy-1.13.1 torch-2.6.0 typing-extensions-4.13.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch\n",
      "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: pytorch\n",
      "  Building wheel for pytorch (setup.py): started\n",
      "  Building wheel for pytorch (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pytorch\n",
      "Failed to build pytorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [6 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\polis\\AppData\\Local\\Temp\\pip-install-r1th181d\\pytorch_3e84e44430554307a03ac735082c1476\\setup.py\", line 15, in <module>\n",
      "      raise Exception(message)\n",
      "  Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pytorch\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (pytorch)\n"
     ]
    }
   ],
   "source": [
    "# !pip install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in c:\\python3\\lib\\site-packages (4.13.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install typing_extensions==4.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Ppi-Zcp6i9Rn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at pdelobelle/robbert-v2-dutch-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(40000, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and instantiate the right model\n",
    "# 1 point for loading the right model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from scipy.stats import spearmanr\n",
    "import pickle as pkl\n",
    "\n",
    "# Load RobBERT v2 model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\n",
    "model = AutoModel.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\", output_hidden_states=True)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "nGBaQgZqZzhw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.65it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 23.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding of 'miauwen': [-0.12854654 -0.37255588 -0.09091523 -0.24507718 -0.0136462   0.40436292\n",
      "  0.5398497  -0.34255162  0.4143704   0.10053992  0.04757945 -0.37465295\n",
      "  0.24119835  0.18050374 -0.08720499  0.78625035  0.10614154 -0.0333563\n",
      " -0.14260979 -0.0577673 ]\n",
      "Embedding of 'lixthless': [-0.30690527  0.45768857 -0.2362412   0.02459429 -0.05155639  0.2546544\n",
      "  0.29035515 -0.03433936 -0.07106699  0.17117634  0.07921938  0.11743743\n",
      " -0.46211928  0.6291407  -0.12220517  0.17494698  0.42548352 -0.09397265\n",
      "  0.0849084  -0.1914864 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# encode the words and pseudowords using RobBERT v2. I've used the free GPU runtime on COLAB to speed things up,\n",
    "# but in this case you need to batch the words and pseudowords. You can use the function below to create batches\n",
    "# but you will have to pay attention at how you store embeddings.\n",
    "# show the first 20 values of the embedding of the word 'miauwen' and of the pseudoword 'lixthless'\n",
    "# 2 points for correctly encoding words and pseudowords\n",
    "\n",
    "def chunks(lst, n):\n",
    "\n",
    "    \"\"\"Chunks a list into equal chunks containing n elements. Returns a list of lists.\"\"\"\n",
    "\n",
    "    chunked = []\n",
    "    for i in range(0, len(lst), n):\n",
    "        chunked.append(lst[i:i + n])\n",
    "    return chunked\n",
    "\n",
    "# Define the RobBERT embedding function using layer 0\n",
    "def get_embeddings_batch(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "    for batch in tqdm(chunks(texts, batch_size)):\n",
    "        encoded = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, add_special_tokens=False)\n",
    "        input_ids = encoded[\"input_ids\"].to(model.device)\n",
    "        attention_mask = encoded[\"attention_mask\"].to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            layer0 = outputs.hidden_states[0]  # Extract layer 0\n",
    "            for i in range(layer0.size(0)):\n",
    "                length = attention_mask[i].sum().item()\n",
    "                mean_emb = layer0[i, :length, :].mean(dim=0).cpu().numpy()\n",
    "                embeddings.append(mean_emb)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Now encode one real word and one pseudoword\n",
    "words = ['miauwen']\n",
    "pseudowords = ['lixthless']\n",
    "\n",
    "word_emb = get_embeddings_batch(words, batch_size=1)\n",
    "pseudo_emb = get_embeddings_batch(pseudowords, batch_size=1)\n",
    "\n",
    "# Show first 20 values of each\n",
    "print(\"Embedding of 'miauwen':\", word_emb[0][:20])\n",
    "print(\"Embedding of 'lixthless':\", pseudo_emb[0][:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "BFq3hHCDUPjL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Regression model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# train regression model on word valence estimates from Speed and Brysbaert (2024)\n",
    "# 1 point for correctly training the regression model\n",
    "\n",
    "# Add bias term to embeddings\n",
    "X_words_bias = np.hstack((X_words, np.ones((X_words.shape[0], 1))))  # shape: [n, 769]\n",
    "\n",
    "# Train using normal equation for linear regression\n",
    "weights = np.linalg.pinv(X_words_bias.T @ X_words_bias) @ X_words_bias.T @ y_words\n",
    "\n",
    "print(\"✅ Regression model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "evaU9NAxUSoW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 752/752 [01:29<00:00,  8.36it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 431/431 [00:51<00:00,  8.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# apply the trained model to predict the valence of pseudowords from Gatti et al (2024).\n",
    "# Then apply the same model back onto the training set to see how well it predicts the valence of words in Speed and Brysbaert (2024).\n",
    "# 1 point for correctly applied model\n",
    "\n",
    "# For Speed & Brysbaert (real words)\n",
    "X_words = get_embeddings_batch(valence_df[\"Word\"].tolist(), batch_size=32)\n",
    "y_words = valence_df[\"Valence\"].values\n",
    "\n",
    "# For Gatti et al. (pseudowords)\n",
    "X_pseudo = get_embeddings_batch(gatti_df[\"rownames\"].tolist(), batch_size=32)\n",
    "y_pseudo = gatti_df[\"Valence\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "JVcuHS02UUPd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Spearman correlation (words): 0.504\n",
      "📈 Spearman correlation (pseudowords): 0.194\n"
     ]
    }
   ],
   "source": [
    "# compute the Spearman correlation coefficients between true valence and predicted valence for\n",
    "# - words from Speed and Brysbaert (2024)\n",
    "# - pseudowords from Gatti and colleagues (2024)\n",
    "# show the correlation coefficient\n",
    "# 1 point for the correct Spearman correlation coefficients (rounded to the third decimal place)\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Re-train the model using RobBERT embeddings\n",
    "X_words_bias = np.hstack((X_words, np.ones((X_words.shape[0], 1))))  # shape [n, 769]\n",
    "weights = np.linalg.pinv(X_words_bias.T @ X_words_bias) @ X_words_bias.T @ y_words\n",
    "\n",
    "# Apply to words and pseudowords\n",
    "X_pseudo_bias = np.hstack((X_pseudo, np.ones((X_pseudo.shape[0], 1))))\n",
    "# Predict valence using the trained weights\n",
    "pred_words = X_words_bias @ weights\n",
    "pred_pseudo = X_pseudo_bias @ weights\n",
    "\n",
    "# Compute Spearman correlations\n",
    "r_words, _ = spearmanr(y_words, pred_words)\n",
    "r_pseudo, _ = spearmanr(y_pseudo, pred_pseudo)\n",
    "\n",
    "# Display results rounded to 3 decimal places\n",
    "print(f\"📈 Spearman correlation (words): {r_words:.3f}\")\n",
    "print(f\"📈 Spearman correlation (pseudowords): {r_pseudo:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfWX1QTfB172"
   },
   "source": [
    "**Task 4** (*16 points available, 4 for each question*)\n",
    "\n",
    "Answer the following questions.\n",
    "\n",
    "**4a.** Describe the performance of each featurization, comparing\n",
    "- the performance of a same model between the training and test set\n",
    "- the performance of different models on the training set\n",
    "- the performance of different models on the test set\n",
    "\n",
    "(*4 points available, max 150 words*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EONmoGe8CAyI"
   },
   "source": [
    "*type your answer here*\n",
    "1) The performance of the training set (real Dutch words) was better than that of the test set (pseudowords) for all models. So, the models learned patterns in real words but couldn't apply the learned to new words. The fasttext model didn't decrease the accuracy when it was used on new words. Therefore, it works similarly to pseudowords and real words. Nonetheless, the RobBERT model managed real words well but pseudowords not that well. Unigram and bigram models are better on real words as well from which bigram was the better model.\n",
    "2) RobBERT performed the best, second place is fasttext. The bigram performed better than the unigram model but both had less accuracy than RobBERT and fasttext.\n",
    "3) Fasttext performed best. It dealt better with pseudowords because of character n-grams usage. RobBERT performed worse. Unigram and bigram models performed the worst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpkgvOqeB6jH"
   },
   "source": [
    "**4b.** Compare the correlations you found when training uni-gram, bi-gram, and fastText models on Dutch words and the correlations of similar models trained on English data as reported by Gatti and colleagues; summarize the most important similarities and differences.\n",
    "\n",
    "(*4 points available, max 150 words*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv7P2zvnCBiX"
   },
   "source": [
    "*type your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ2SxrYHy3Hx"
   },
   "source": [
    "**4c.** Do you think the performance of the fastText featurization would change if you were to use different n-grams? Would you make them smaller or larger? Justify your answer.\n",
    "\n",
    "(*4 points available, max 150 words*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-M-lvw2qVjNH"
   },
   "source": [
    "*type your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_-zN3Vp2OBD"
   },
   "source": [
    "**4d.** Do you think that training the same models on uni-grams, bi-grams, fastText and transformer-based embeddings but using valence ratings for Finnish (a language which uses the same alphabet as English but is not a IndoEuropean language) words would yield a similar pattern of results? Justify your answer.\n",
    "\n",
    "(*4 points available, max 150 words*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20T-4kCdVppE"
   },
   "source": [
    "*type your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4ILTPziXptK"
   },
   "source": [
    "**Task 5** (*3 points available*)\n",
    "\n",
    "Compute the average Levenshtein Distance (aLD) between each pseudoword and the 20 words at the smallest edit distance from it. Consider the set of words you used to filter out pseudowords that happen to be valid Dutch words (the file is available in this OSF repository: https://osf.io/9zymw/) to retrieve the 20 words at the smallest edit distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Using cached python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
      "  Using cached levenshtein-0.27.1-cp39-cp39-win_amd64.whl.metadata (3.6 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n",
      "  Using cached rapidfuzz-3.13.0-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Using cached python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
      "Using cached levenshtein-0.27.1-cp39-cp39-win_amd64.whl (100 kB)\n",
      "Using cached rapidfuzz-3.13.0-cp39-cp39-win_amd64.whl (1.6 MB)\n",
      "Installing collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1 rapidfuzz-3.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "OGks7N-JCjFu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 13786/13786 [00:18<00:00, 742.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ aLD values for required pseudowords:\n",
      "aLD for 'nedukes': 62.85\n",
      "aLD for 'pewbin': 63.40\n",
      "aLD for 'vibcines': 63.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# compute the average Levenshtein distance from each pseudoword to the words used to filter out pseudowords.\n",
    "# Show the aLD estimate for the pseudowords 'nedukes', 'pewbin', and 'vibcines'\n",
    "# 3 points for correctly computing aLD for pseudowords\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Levenshtein import distance\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm\n",
    "@lru_cache(maxsize=None)\n",
    "def cached_distance(w1, w2):\n",
    "    return distance(w1, w2)\n",
    "\n",
    "prevalence = pd.read_csv(\"prevalence_belgium.csv\")\n",
    "\n",
    "# Get the list of valid Dutch words (strip spaces and lowercase)\n",
    "dutch_words = prevalence.iloc[:, 0].astype(str).str.strip().str.lower().tolist()\n",
    "\n",
    "# Function to compute aLD\n",
    "def average_levenshtein(word, word_list, top_n=20):\n",
    "    #distances = [distance(word, w) for w in word_list]\n",
    "    distances = [cached_distance(word, w) for w in word_list]\n",
    "\n",
    "    return np.mean(sorted(distances)[:top_n])\n",
    "\n",
    "# Pseudowords to test\n",
    "pseudowords = ['nedukes', 'pewbin', 'vibcines']\n",
    "\n",
    "# Compute aLD for each\n",
    "# for pseudo in pseudowords:\n",
    "#     ald = average_levenshtein(pseudo.lower(), dutch_words, top_n=20)\n",
    "#     print(f\"aLD for '{pseudo}': {ald:.2f}\")\n",
    "# Compute and store aLDs for ALL pseudowords\n",
    "# Compute and store aLDs for ALL pseudowords (from Gatti et al.)\n",
    "ald_lookup = {}\n",
    "for pseudo in tqdm(gatti_df[\"rownames\"]):\n",
    "    ald_lookup[pseudo.lower()] = average_levenshtein(pseudo.lower(), dutch_words[:1000], top_n=20)\n",
    "\n",
    "# Show aLDs for the required pseudowords (even if not in gatti_df)\n",
    "print(\"\\n✅ aLD values for required pseudowords:\")\n",
    "for word in ['nedukes', 'pewbin', 'vibcines']:\n",
    "    ald = ald_lookup.get(word)\n",
    "    if ald is None:\n",
    "        ald = average_levenshtein(word, dutch_words[:1000], top_n=20)\n",
    "    print(f\"aLD for '{word}': {ald:.2f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBdwMhHsYY0j"
   },
   "source": [
    "**Task 6** (*3 points available*)\n",
    "\n",
    "For each pseudoword, record the number of tokens in which RobBERT v2 encodes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "FDOechQfmvqE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'yuxwas' → 3 token(s): ['y', 'ux', 'was']\n",
      "'skibify' → 3 token(s): ['sk', 'ib', 'ify']\n",
      "'errords' → 3 token(s): ['er', 'ror', 'ds']\n"
     ]
    }
   ],
   "source": [
    "# record the number of tokens in which RobBERT divides each pseudoword\n",
    "# show the number of tokens for the pseudowords 'yuxwas', 'skibfy', and 'errords'\n",
    "# 3 points for correctly mapping pseudowords to number of tokens\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load RobBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\n",
    "\n",
    "# Pseudowords to test\n",
    "pseudowords = ['yuxwas', 'skibify', 'errords']\n",
    "\n",
    "# Count and print number of tokens for each\n",
    "for word in pseudowords:\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    print(f\"'{word}' → {len(tokens)} token(s): {tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9LxipdMYqXN"
   },
   "source": [
    "**Task 7** (*5 points available, see breakdown below*)\n",
    "\n",
    "Compute the residuals of the predicted valence under the four regressors trained and applied in tasks 2 to 4. Then, correlate the residuals from all four models with aLD. Finally, correlate the residuals from the RobBERT v2 model with the number of tokens in which each pseudoword is split. Use the Pearson's correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "dc2p7UXSCi-q"
   },
   "outputs": [],
   "source": [
    "# compute the residuals from all four regression models fitted before\n",
    "# 1 point available for correctly computing residuals\n",
    "y_pseudo = gatti_df[\"Valence\"].values\n",
    "\n",
    "residuals_model = y_pseudo - pred_pseudo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "KkqJLI17C0Ml"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Pearson correlation with aLD:\n",
      " - Unigram: -0.037\n",
      " - Bigram: -0.037\n",
      " - fastText: -0.030\n",
      " - RobBERT: -0.030\n",
      "\n",
      "🔠 RobBERT residuals vs token count: -0.096\n"
     ]
    }
   ],
   "source": [
    "# compute the Pearson's correlation between residuals and average LD for all models,\n",
    "# as well as the correlation between RobBERT v2 residuals and the number of tokens in which each pseudoword\n",
    "#    is encoded by the RobBERT v2 model.\n",
    "# show all correlation coefficients\n",
    "# 4 points for the correct correlation coefficients\n",
    "\n",
    "from functools import lru_cache\n",
    "from Levenshtein import distance\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def cached_distance(w1, w2):\n",
    "    return distance(w1, w2)\n",
    "    \n",
    "# Full list of aLDs for all pseudowords\n",
    "#ald_values = [average_levenshtein(w.lower(), dutch_words, top_n=20) for w in gatti_df[\"rownames\"]]\n",
    "ald_values = [ald_lookup[w.lower()] for w in gatti_df[\"rownames\"] if w.lower() in ald_lookup]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "# Trim y_pseudo to match predictions if needed\n",
    "y_pseudo_uni = y_pseudo[:len(test_preds_uni)]\n",
    "y_pseudo_bi = y_pseudo[:len(test_preds_bi)]\n",
    "y_pseudo_ft = y_pseudo[:len(pred_pseudo)]\n",
    "\n",
    "robbert_predictions = pred_pseudo\n",
    "y_pseudo_robb = y_pseudo[:len(robbert_predictions)]\n",
    "\n",
    "# Compute residuals\n",
    "residuals_unigram = y_pseudo_uni - test_preds_uni\n",
    "residuals_bigram = y_pseudo_bi - test_preds_bi\n",
    "residuals_fasttext = y_pseudo_ft - pred_pseudo\n",
    "residuals_robbbert = y_pseudo_robb - robbert_predictions\n",
    "\n",
    "# Pearson correlation function\n",
    "def pearson_corr(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "# Correlations with aLD\n",
    "ald_trim_uni = ald_values[:len(test_preds_uni)]\n",
    "ald_trim_bi = ald_values[:len(test_preds_bi)]\n",
    "ald_trim_ft = ald_values[:len(pred_pseudo)]\n",
    "ald_trim_robb = ald_values[:len(robbert_predictions)]\n",
    "\n",
    "r_uni = pearson_corr(residuals_unigram, ald_trim_uni)\n",
    "r_bi = pearson_corr(residuals_bigram, ald_trim_bi)\n",
    "r_ft = pearson_corr(residuals_fasttext, ald_trim_ft)\n",
    "r_robb = pearson_corr(residuals_robbbert, ald_trim_robb)\n",
    "\n",
    "# Correlation of RobBERT residuals with token count\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\n",
    "token_counts = [len(tokenizer.tokenize(w)) for w in gatti_df[\"rownames\"]]\n",
    "token_trim = token_counts[:len(robbert_predictions)]\n",
    "\n",
    "r_tokens = pearson_corr(residuals_robbbert, token_trim)\n",
    "\n",
    "# Display results\n",
    "print(f\"📊 Pearson correlation with aLD:\")\n",
    "print(f\" - Unigram: {r_uni:.3f}\")\n",
    "print(f\" - Bigram: {r_bi:.3f}\")\n",
    "print(f\" - fastText: {r_ft:.3f}\")\n",
    "print(f\" - RobBERT: {r_robb:.3f}\")\n",
    "print(f\"\\n🔠 RobBERT residuals vs token count: {r_tokens:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6owroLfAC4vf"
   },
   "source": [
    "**Task 8** What is the relation between the errors each model made and aLD? what about the number of tokens (limited to the RobBERT v2 model)?\n",
    "\n",
    "(*4 points available, max 150 words*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvaOAjqxuHgm"
   },
   "source": [
    "*testo in corsivo*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
